{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640d5c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import (\n",
    "    BertModel, BertTokenizer,\n",
    "    RobertaModel, RobertaTokenizer,\n",
    "    ElectraModel, ElectraTokenizer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from river import drift\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from typing import List, Tuple, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class LoRATransformerWrapper:\n",
    "    \"\"\"Wrapper para modelos Transformer com LoRA aplicado\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str, num_labels: int = 2, rank: int = 8, alpha: int = 16):\n",
    "        self.model_name = model_name\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # Carrega modelo e tokenizer\n",
    "        if 'bert-base' in model_name:\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "            self.base_model = BertModel.from_pretrained(model_name)\n",
    "        elif 'roberta' in model_name:\n",
    "            self.tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "            self.base_model = RobertaModel.from_pretrained(model_name)\n",
    "        elif 'electra' in model_name:\n",
    "            self.tokenizer = ElectraTokenizer.from_pretrained(model_name)\n",
    "            self.base_model = ElectraModel.from_pretrained(model_name)\n",
    "\n",
    "        # Configura√ß√£o LoRA\n",
    "        lora_config = LoraConfig(\n",
    "            task_type=TaskType.FEATURE_EXTRACTION,\n",
    "            r=rank,\n",
    "            lora_alpha=alpha,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"query\", \"value\"]  # Aplica LoRA nas camadas Q e V\n",
    "        )\n",
    "\n",
    "        # Aplica LoRA ao modelo\n",
    "        self.model = get_peft_model(self.base_model, lora_config)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Classifier head\n",
    "        hidden_size = self.base_model.config.hidden_size\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels).to(self.device)\n",
    "\n",
    "        print(f\"‚úì {model_name} carregado com LoRA (params trein√°veis: {self.model.print_trainable_parameters()})\")\n",
    "\n",
    "    def encode(self, texts: List[str]) -> torch.Tensor:\n",
    "        \"\"\"Gera embeddings para uma lista de textos\"\"\"\n",
    "        self.model.eval()\n",
    "\n",
    "        # Tokeniza\n",
    "        encoded = self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors='pt'\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Gera embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**encoded)\n",
    "            # Usa [CLS] token embedding\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def fine_tune(self, buffer: List[Tuple[str, int]], epochs: int = 3, lr: float = 3e-4):\n",
    "        \"\"\"Fine-tune incremental com LoRA\"\"\"\n",
    "        self.model.train()\n",
    "        self.classifier.train()\n",
    "\n",
    "        # Prepara dados\n",
    "        texts = [x for x, y in buffer]\n",
    "        labels = torch.tensor([y for x, y in buffer], dtype=torch.long).to(self.device)\n",
    "\n",
    "        encoded = self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors='pt'\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Dataset e DataLoader\n",
    "        dataset = TensorDataset(\n",
    "            encoded['input_ids'],\n",
    "            encoded['attention_mask'],\n",
    "            labels\n",
    "        )\n",
    "        dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "        # Otimizador (apenas params LoRA + classifier)\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            list(self.model.parameters()) + list(self.classifier.parameters()),\n",
    "            lr=lr\n",
    "        )\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Treinamento\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for batch in dataloader:\n",
    "                input_ids, attention_mask, batch_labels = batch\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "                logits = self.classifier(embeddings)\n",
    "\n",
    "                loss = criterion(logits, batch_labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            print(f\"  Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "class MLPMetaLearner(nn.Module):\n",
    "    \"\"\"Meta-learner MLP que combina embeddings dos 3 modelos\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int = 768*3, hidden_dim: int = 256, num_labels: int = 2):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim // 2, num_labels)\n",
    "        )\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "    def train_mlp(self, embeddings: torch.Tensor, labels: torch.Tensor,\n",
    "                  epochs: int = 5, lr: float = 1e-3):\n",
    "        \"\"\"Treina o meta-learner\"\"\"\n",
    "        self.train()\n",
    "\n",
    "        dataset = TensorDataset(embeddings, labels)\n",
    "        dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for batch_emb, batch_labels in dataloader:\n",
    "                batch_emb = batch_emb.to(self.device)\n",
    "                batch_labels = batch_labels.to(self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                logits = self(batch_emb)\n",
    "                loss = criterion(logits, batch_labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            print(f\"  MLP Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "\n",
    "class IncrementalMetaLearnerWithLoRA:\n",
    "    \"\"\"Sistema completo de aprendizado incremental com LoRA e detec√ß√£o de drift\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size: int = 100, drift_threshold: float = 0.001):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "        # Detectores de drift (um por modelo para melhor detec√ß√£o)\n",
    "        self.drift_detectors = {\n",
    "            'bert': drift.ADWIN(delta=drift_threshold),\n",
    "            'roberta': drift.ADWIN(delta=drift_threshold),\n",
    "            'electra': drift.ADWIN(delta=drift_threshold)\n",
    "        }\n",
    "\n",
    "        # Modelos base com LoRA\n",
    "        print(\"Inicializando modelos base com LoRA...\")\n",
    "        self.models = {\n",
    "            'bert': LoRATransformerWrapper('bert-base-uncased'),\n",
    "            'roberta': LoRATransformerWrapper('roberta-base'),\n",
    "            'electra': LoRATransformerWrapper('google/electra-base-discriminator')\n",
    "        }\n",
    "\n",
    "        # Meta-learner\n",
    "        self.meta_learner = MLPMetaLearner(input_dim=768*3, num_labels=2)\n",
    "\n",
    "        self.is_initialized = False\n",
    "        self.total_samples = 0\n",
    "        self.drift_count = 0\n",
    "\n",
    "    def _extract_combined_embeddings(self, texts: List[str]) -> torch.Tensor:\n",
    "        \"\"\"Extrai e concatena embeddings dos 3 modelos\"\"\"\n",
    "        emb_bert = self.models['bert'].encode(texts)\n",
    "        emb_roberta = self.models['roberta'].encode(texts)\n",
    "        emb_electra = self.models['electra'].encode(texts)\n",
    "\n",
    "        # Concatena embeddings\n",
    "        combined = torch.cat([emb_bert, emb_roberta, emb_electra], dim=1)\n",
    "        return combined\n",
    "\n",
    "    def _detect_drift(self) -> bool:\n",
    "        \"\"\"Detecta concept drift usando m√∫ltiplos detectores\"\"\"\n",
    "        if len(self.buffer) < 30:  # M√≠nimo de amostras\n",
    "            return False\n",
    "\n",
    "        # Calcula acur√°cia dos √∫ltimos exemplos\n",
    "        recent_texts = [x for x, y in list(self.buffer)[-30:]]\n",
    "        recent_labels = torch.tensor([y for x, y in list(self.buffer)[-30:]])\n",
    "\n",
    "        # Predi√ß√£o\n",
    "        embeddings = self._extract_combined_embeddings(recent_texts)\n",
    "        self.meta_learner.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = self.meta_learner(embeddings)\n",
    "            predictions = torch.argmax(logits, dim=1).cpu()\n",
    "\n",
    "        # Calcula erro\n",
    "        errors = (predictions != recent_labels).float()\n",
    "\n",
    "        # Atualiza detectores\n",
    "        drift_detected = False\n",
    "        for i, error in enumerate(errors):\n",
    "            for detector in self.drift_detectors.values():\n",
    "                detector.update(error.item())\n",
    "                if detector.drift_detected:\n",
    "                    drift_detected = True\n",
    "\n",
    "        return drift_detected\n",
    "\n",
    "    def learn_one(self, text: str, label: int):\n",
    "        \"\"\"Aprende incrementalmente com uma √∫nica inst√¢ncia\"\"\"\n",
    "        self.buffer.append((text, label))\n",
    "        self.total_samples += 1\n",
    "\n",
    "        # Inicializa√ß√£o: aguarda buffer cheio pela primeira vez\n",
    "        if not self.is_initialized and len(self.buffer) == self.buffer_size:\n",
    "            print(f\"\\n=== Inicializa√ß√£o com {self.buffer_size} amostras ===\")\n",
    "            self._initial_training()\n",
    "            self.is_initialized = True\n",
    "            return\n",
    "\n",
    "        # Ap√≥s inicializa√ß√£o: verifica drift quando buffer enche\n",
    "        if self.is_initialized and len(self.buffer) == self.buffer_size:\n",
    "            drift_detected = self._detect_drift()\n",
    "\n",
    "            if drift_detected:\n",
    "                self.drift_count += 1\n",
    "                print(f\"\\nüîÑ DRIFT DETECTADO (#{self.drift_count}) - Atualizando modelos...\")\n",
    "                self._incremental_update()\n",
    "\n",
    "    def _initial_training(self):\n",
    "        \"\"\"Treinamento inicial com o primeiro buffer\"\"\"\n",
    "        buffer_list = list(self.buffer)\n",
    "\n",
    "        # Fine-tune modelos base com LoRA\n",
    "        print(\"\\n1. Fine-tuning modelos base com LoRA...\")\n",
    "        for name, model in self.models.items():\n",
    "            print(f\"\\n  Fine-tuning {name.upper()}...\")\n",
    "            model.fine_tune(buffer_list, epochs=3)\n",
    "\n",
    "        # Extrai embeddings\n",
    "        print(\"\\n2. Extraindo embeddings combinados...\")\n",
    "        texts = [x for x, y in buffer_list]\n",
    "        labels = torch.tensor([y for x, y in buffer_list], dtype=torch.long)\n",
    "\n",
    "        embeddings = self._extract_combined_embeddings(texts)\n",
    "\n",
    "        # Treina meta-learner\n",
    "        print(\"\\n3. Treinando Meta-learner MLP...\")\n",
    "        self.meta_learner.train_mlp(embeddings, labels, epochs=5)\n",
    "\n",
    "        print(\"\\n‚úì Inicializa√ß√£o completa!\")\n",
    "\n",
    "    def _incremental_update(self):\n",
    "        \"\"\"Atualiza√ß√£o incremental ap√≥s detec√ß√£o de drift\"\"\"\n",
    "        buffer_list = list(self.buffer)\n",
    "\n",
    "        # Fine-tune incremental com LoRA (r√°pido!)\n",
    "        print(\"  Atualizando modelos com LoRA...\")\n",
    "        for name, model in self.models.items():\n",
    "            print(f\"    {name.upper()}...\")\n",
    "            model.fine_tune(buffer_list, epochs=2, lr=1e-4)  # Menos √©pocas, LR menor\n",
    "\n",
    "        # Atualiza meta-learner\n",
    "        print(\"  Atualizando Meta-learner...\")\n",
    "        texts = [x for x, y in buffer_list]\n",
    "        labels = torch.tensor([y for x, y in buffer_list], dtype=torch.long)\n",
    "        embeddings = self._extract_combined_embeddings(texts)\n",
    "\n",
    "        self.meta_learner.train_mlp(embeddings, labels, epochs=3, lr=5e-4)\n",
    "\n",
    "        print(\"  ‚úì Atualiza√ß√£o completa!\")\n",
    "\n",
    "        # Limpa buffer ap√≥s update\n",
    "        self.buffer.clear()\n",
    "\n",
    "    def predict_one(self, text: str) -> int:\n",
    "        \"\"\"Prediz a classe de uma √∫nica inst√¢ncia\"\"\"\n",
    "        if not self.is_initialized:\n",
    "            return None\n",
    "\n",
    "        self.meta_learner.eval()\n",
    "        with torch.no_grad():\n",
    "            embeddings = self._extract_combined_embeddings([text])\n",
    "            logits = self.meta_learner(embeddings)\n",
    "            prediction = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "        return prediction\n",
    "\n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Retorna estat√≠sticas do sistema\"\"\"\n",
    "        return {\n",
    "            'total_samples': self.total_samples,\n",
    "            'drift_count': self.drift_count,\n",
    "            'buffer_size': len(self.buffer),\n",
    "            'is_initialized': self.is_initialized\n",
    "        }\n",
    "\n",
    "\n",
    "# ========================\n",
    "# EXEMPLO DE USO COM STREAM\n",
    "# ========================\n",
    "\n",
    "from river import datasets\n",
    "\n",
    "dataset = datasets.SMSSpam()\n",
    "\n",
    "system = IncrementalMetaLearnerWithLoRA(buffer_size=100, drift_threshold=0.001)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "print(\"Iniciando aprendizado incremental com SMS Spam (River)...\\n\")\n",
    "\n",
    "for i, (x, y) in enumerate(dataset):\n",
    "    text = x['body']\n",
    "    label = int(y)  # False=0, True=1\n",
    "\n",
    "    # Predi√ß√£o\n",
    "    prediction = system.predict_one(text)\n",
    "\n",
    "    if prediction is not None:\n",
    "        total += 1\n",
    "        if prediction == label:\n",
    "            correct += 1\n",
    "\n",
    "    # Aprendizado incremental\n",
    "    system.learn_one(text, label)\n",
    "\n",
    "    # Log peri√≥dico\n",
    "    if (i + 1) % 100 == 0:\n",
    "        stats = system.get_stats()\n",
    "        acc = (correct / total * 100) if total > 0 else 0\n",
    "        print(f\"\\nüìä Progresso: {i+1} mensagens\")\n",
    "        print(f\"   Acur√°cia online: {acc:.2f}%\")\n",
    "        print(f\"   Drifts detectados: {stats['drift_count']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ESTAT√çSTICAS FINAIS\")\n",
    "print(\"=\"*50)\n",
    "stats = system.get_stats()\n",
    "print(f\"Total de amostras: {stats['total_samples']}\")\n",
    "print(f\"Drifts detectados: {stats['drift_count']}\")\n",
    "print(f\"Acur√°cia final: {correct/total*100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
