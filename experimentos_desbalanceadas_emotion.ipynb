{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0778f19a-abc8-4031-9dd2-4a6df32cfa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "import itertools\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7a85655-b5eb-40b7-9ca9-3708ead8f88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in ./venv/lib/python3.11/site-packages (3.0.5)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.11/site-packages (from xgboost) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in ./venv/lib/python3.11/site-packages (from xgboost) (2.26.2)\n",
      "Requirement already satisfied: scipy in ./venv/lib/python3.11/site-packages (from xgboost) (1.15.3)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9c37bdd-d3d4-41d1-9335-6e514594ca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def create_model(input_dim, hidden_dim1, dropout, num_classes=2):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, hidden_dim1),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(hidden_dim1, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, num_classes)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e994828-70ab-4314-afea-b15bcc45c469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "def trainXGBoostOnly(\n",
    "    bertTrainLogits, robertaTrainLogits, electraTrainLogits,\n",
    "    bertTestLogits, robertaTestLogits, electraTestLogits,\n",
    "    trainLabels, testLabels,\n",
    "    num_classes,\n",
    "    val_size=0.2,\n",
    "    random_state=42\n",
    "):\n",
    "    # Inicia contagem de tempo\n",
    "    start_time = time.time()\n",
    "    print(\"üïê Iniciando experimento XGBoost...\")\n",
    "    \n",
    "    # Concatena os logits das tr√™s redes\n",
    "    concatenated_logits_train = np.concatenate(\n",
    "        [bertTrainLogits['logits'], robertaTrainLogits['logits'], electraTrainLogits['logits']], axis=1\n",
    "    )\n",
    "    concatenated_logits_test = np.concatenate(\n",
    "        [bertTestLogits['logits'], robertaTestLogits['logits'], electraTestLogits['logits']], axis=1\n",
    "    )\n",
    "    \n",
    "    train_labels = np.array(trainLabels)\n",
    "    test_labels = np.array(testLabels)\n",
    "    \n",
    "    print(f\"üìä Shape dos dados concatenados: {concatenated_logits_train.shape}\")\n",
    "    print(f\"üìä N√∫mero de classes: {num_classes}\")\n",
    "    \n",
    "    # Dividir treino em treino e valida√ß√£o (estratificado)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        concatenated_logits_train,\n",
    "        train_labels,\n",
    "        test_size=val_size,\n",
    "        stratify=train_labels,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Define o classificador XGBoost (ajuste o objective conforme n√∫mero de classes)\n",
    "    if num_classes == 2:\n",
    "        objective = 'binary:logistic'\n",
    "        eval_metric = 'logloss'\n",
    "        scoring_metric = 'f1'\n",
    "    else:\n",
    "        objective = 'multi:softprob'\n",
    "        eval_metric = 'mlogloss'\n",
    "        scoring_metric = 'f1_weighted'  # Corrigido para multiclasse\n",
    "    \n",
    "    xgb_clf = xgb.XGBClassifier(\n",
    "        objective=objective,\n",
    "        eval_metric=eval_metric,\n",
    "        use_label_encoder=False,\n",
    "        num_class=num_classes if num_classes > 2 else None,\n",
    "        random_state=random_state,\n",
    "        verbosity=0  # Reduzido para menos verbose\n",
    "    )\n",
    "    \n",
    "    # Grade de hiperpar√¢metros para busca\n",
    "    param_grid_xgb = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 4, 6],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0]\n",
    "    }\n",
    "    \n",
    "    total_combinations = np.prod([len(v) for v in param_grid_xgb.values()])\n",
    "    print(f\"üîç Testando {total_combinations} combina√ß√µes de hiperpar√¢metros com CV=3...\")\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=xgb_clf,\n",
    "        param_grid=param_grid_xgb,\n",
    "        scoring=scoring_metric,\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    print(\"‚öôÔ∏è  Rodando GridSearch para XGBoost...\")\n",
    "    grid_start_time = time.time()\n",
    "    \n",
    "    # Usa apenas treino para GridSearch (valida√ß√£o fica separada para avalia√ß√£o)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    grid_end_time = time.time()\n",
    "    print(f\"‚úÖ GridSearch conclu√≠do em {grid_end_time - grid_start_time:.2f} segundos\")\n",
    "    \n",
    "    best_xgb_model = grid_search.best_estimator_\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üèÜ MELHORES HIPERPAR√ÇMETROS:\")\n",
    "    print(grid_search.best_params_)\n",
    "    print(f\"üéØ Melhor score no CV: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Avalia√ß√£o no conjunto de valida√ß√£o\n",
    "    print(\"\\nüìä AVALIA√á√ÉO NO CONJUNTO DE VALIDA√á√ÉO:\")\n",
    "    val_pred = best_xgb_model.predict(X_val)\n",
    "    val_accuracy = accuracy_score(y_val, val_pred)\n",
    "    val_f1_weighted = f1_score(y_val, val_pred, average='weighted')\n",
    "    val_f1_macro = f1_score(y_val, val_pred, average='macro')\n",
    "    \n",
    "    print(f\"Valida√ß√£o - Acur√°cia: {val_accuracy:.4f}\")\n",
    "    print(f\"Valida√ß√£o - F1 Weighted: {val_f1_weighted:.4f}\")\n",
    "    print(f\"Valida√ß√£o - F1 Macro: {val_f1_macro:.4f}\")\n",
    "    \n",
    "    # Retreina o modelo com treino + valida√ß√£o para avalia√ß√£o final\n",
    "    print(\"\\nüîÑ Retreinando modelo final com treino + valida√ß√£o...\")\n",
    "    X_trainval = np.concatenate([X_train, X_val], axis=0)\n",
    "    y_trainval = np.concatenate([y_train, y_val], axis=0)\n",
    "    \n",
    "    final_model = xgb.XGBClassifier(**grid_search.best_params_,\n",
    "                                   objective=objective,\n",
    "                                   eval_metric=eval_metric,\n",
    "                                   use_label_encoder=False,\n",
    "                                   num_class=num_classes if num_classes > 2 else None,\n",
    "                                   random_state=random_state,\n",
    "                                   verbosity=0)\n",
    "    \n",
    "    final_model.fit(X_trainval, y_trainval)\n",
    "    \n",
    "    # Avalia√ß√£o final no conjunto de teste\n",
    "    print(\"\\nüìà AVALIA√á√ÉO FINAL NO TESTE:\")\n",
    "    y_pred = final_model.predict(concatenated_logits_test)\n",
    "    \n",
    "    # Calcula m√©tricas finais\n",
    "    test_accuracy = accuracy_score(test_labels, y_pred)\n",
    "    test_f1_weighted = f1_score(test_labels, y_pred, average='weighted')\n",
    "    test_f1_macro = f1_score(test_labels, y_pred, average='macro')\n",
    "    \n",
    "    # Tempo total\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"üìà RESULTADOS FINAIS:\")\n",
    "    print(f\"Acur√°cia no teste: {test_accuracy:.4f}\")\n",
    "    print(f\"F1-Score (weighted): {test_f1_weighted:.4f}\")\n",
    "    print(f\"F1-Score (macro): {test_f1_macro:.4f}\")\n",
    "    print(f\"‚è±Ô∏è  Tempo total: {total_time:.2f} segundos ({total_time/60:.2f} minutos)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nüìã RELAT√ìRIO DETALHADO:\")\n",
    "    print(classification_report(test_labels, y_pred))\n",
    "    \n",
    "    # Informa√ß√µes adicionais do modelo\n",
    "    if hasattr(final_model, 'feature_importances_'):\n",
    "        print(f\"\\nüîç Top 5 features mais importantes:\")\n",
    "        feature_importance = final_model.feature_importances_\n",
    "        top_features = np.argsort(feature_importance)[-5:][::-1]\n",
    "        for i, feat_idx in enumerate(top_features, 1):\n",
    "            print(f\"  {i}. Feature {feat_idx}: {feature_importance[feat_idx]:.4f}\")\n",
    "    \n",
    "    # Retorna as m√©tricas principais\n",
    "    return {\n",
    "        'model': final_model,\n",
    "        'accuracy': test_accuracy,\n",
    "        'f1_weighted': test_f1_weighted,\n",
    "        'f1_macro': test_f1_macro,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_cv_score': grid_search.best_score_,\n",
    "        'total_time_seconds': total_time,\n",
    "        'total_time_minutes': total_time/60,\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'val_f1_weighted': val_f1_weighted,\n",
    "        'val_f1_macro': val_f1_macro\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1dc8305e-157c-48ab-bad3-5ea938de0dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "def trainNNLogits(\n",
    "    bertTrainLogits, robertaTrainLogits, electraTrainLogits,\n",
    "    bertTestLogits, robertaTestLogits, electraTestLogits,\n",
    "    trainLabels, testLabels,\n",
    "    num_classes,\n",
    "    val_size=0.2,\n",
    "    batch_size=32,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "):\n",
    "    # Inicia contagem de tempo\n",
    "    start_time = time.time()\n",
    "    print(\"üïê Iniciando experimento NN com Logits...\")\n",
    "    \n",
    "    # Concatena os logits das tr√™s redes\n",
    "    concatenated_logits = np.concatenate(\n",
    "        [bertTrainLogits['logits'], robertaTrainLogits['logits'], electraTrainLogits['logits']], axis=1\n",
    "    )\n",
    "    concatenated_test_logits = np.concatenate(\n",
    "        [bertTestLogits['logits'], robertaTestLogits['logits'], electraTestLogits['logits']], axis=1\n",
    "    )\n",
    "\n",
    "    train_labels = np.array(trainLabels)\n",
    "    test_labels = np.array(testLabels)\n",
    "    \n",
    "    print(f\"üìä Shape dos logits concatenados: {concatenated_logits.shape}\")\n",
    "    print(f\"üìä N√∫mero de classes: {num_classes}\")\n",
    "\n",
    "    # Split treino/val a partir do conjunto de treino\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        concatenated_logits,\n",
    "        train_labels,\n",
    "        test_size=val_size,\n",
    "        stratify=train_labels,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Cria DataLoaders\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "    val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
    "    test_dataset = TensorDataset(torch.tensor(concatenated_test_logits, dtype=torch.float32), torch.tensor(test_labels, dtype=torch.long))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    param_grid = {\n",
    "        'lr': [1e-3, 5e-4],\n",
    "        'hidden_dim1': [64, 128],\n",
    "        'dropout': [0.3, 0.5]\n",
    "    }\n",
    "\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "    best_f1 = 0.0  # Mudan√ßa: agora otimiza pelo F1 ao inv√©s da loss\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    print(f\"üîç Testando {len(combinations)} combina√ß√µes de hiperpar√¢metros...\")\n",
    "\n",
    "    for i, params in enumerate(combinations, 1):\n",
    "        print(f\"\\n[{i}/{len(combinations)}] Testando: {params}\")\n",
    "        \n",
    "        model = create_model(\n",
    "            input_dim=X_train.shape[1],\n",
    "            hidden_dim1=params['hidden_dim1'],\n",
    "            dropout=params['dropout'],\n",
    "            num_classes=num_classes\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Treina por 5 epochs\n",
    "        for epoch in range(5):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * X_batch.size(0)\n",
    "            epoch_loss = running_loss / len(train_loader.dataset)\n",
    "            print(f\"  Epoch {epoch+1} - Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Avalia√ß√£o no conjunto de valida√ß√£o (agora com F1)\n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        val_true_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                val_predictions.extend(predicted.cpu().numpy())\n",
    "                val_true_labels.extend(y_batch.cpu().numpy())\n",
    "        \n",
    "        # Calcula F1 na valida√ß√£o\n",
    "        val_f1 = f1_score(val_true_labels, val_predictions, average='weighted')\n",
    "        val_acc = accuracy_score(val_true_labels, val_predictions)\n",
    "        \n",
    "        print(f\"  Val Accuracy: {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_params = params\n",
    "            best_model = model.state_dict().copy()  # Salva o estado do modelo\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üèÜ MELHORES RESULTADOS NA VALIDA√á√ÉO:\")\n",
    "    print(f\"Melhores par√¢metros: {best_params}\")\n",
    "    print(f\"Melhor F1 na valida√ß√£o: {best_f1:.4f}\")\n",
    "\n",
    "    # Recria o melhor modelo para avalia√ß√£o final\n",
    "    final_model = create_model(\n",
    "        input_dim=X_train.shape[1],\n",
    "        hidden_dim1=best_params['hidden_dim1'],\n",
    "        dropout=best_params['dropout'],\n",
    "        num_classes=num_classes\n",
    "    ).to(device)\n",
    "    final_model.load_state_dict(best_model)\n",
    "\n",
    "    # Avalia√ß√£o final no conjunto de teste\n",
    "    print(\"\\nüìä AVALIA√á√ÉO FINAL NO TESTE:\")\n",
    "    final_model.eval()\n",
    "    test_predictions = []\n",
    "    test_true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = final_model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            test_predictions.extend(predicted.cpu().numpy())\n",
    "            test_true_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    # Calcula m√©tricas finais\n",
    "    test_accuracy = accuracy_score(test_true_labels, test_predictions)\n",
    "    test_f1_weighted = f1_score(test_true_labels, test_predictions, average='weighted')\n",
    "    test_f1_macro = f1_score(test_true_labels, test_predictions, average='macro')\n",
    "    \n",
    "    # Tempo total\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"üìà RESULTADOS FINAIS:\")\n",
    "    print(f\"Acur√°cia no teste: {test_accuracy:.4f}\")\n",
    "    print(f\"F1-Score (weighted): {test_f1_weighted:.4f}\")\n",
    "    print(f\"F1-Score (macro): {test_f1_macro:.4f}\")\n",
    "    print(f\"‚è±Ô∏è  Tempo total: {total_time:.2f} segundos ({total_time/60:.2f} minutos)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nüìã RELAT√ìRIO DETALHADO:\")\n",
    "    print(classification_report(test_true_labels, test_predictions))\n",
    "    \n",
    "    # Retorna as m√©tricas principais\n",
    "    return {\n",
    "        'accuracy': test_accuracy,\n",
    "        'f1_weighted': test_f1_weighted,\n",
    "        'f1_macro': test_f1_macro,\n",
    "        'best_params': best_params,\n",
    "        'total_time_seconds': total_time,\n",
    "        'total_time_minutes': total_time/60\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec6f940-fc60-4ca4-b5c9-5b7cc7bd7834",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c2bac7e2-4f1e-46ca-9c03-4c18784f2c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "def trainNNEmb(\n",
    "    bertTrainEmbeddings, robertaTrainEmbeddings, electraTrainEmbeddings,\n",
    "    bertTestEmbeddings, robertaTestEmbeddings, electraTestEmbeddings,\n",
    "    trainLabels, testLabels,\n",
    "    num_classes,\n",
    "    val_size=0.2,\n",
    "    batch_size=32,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "):\n",
    "    # Inicia contagem de tempo\n",
    "    start_time = time.time()\n",
    "    print(\"üïê Iniciando experimento...\")\n",
    "    \n",
    "    # Concatena os logits das tr√™s redes\n",
    "    concatenated_logits = np.concatenate(\n",
    "        [bertTrainEmbeddings, robertaTrainEmbeddings, electraTrainEmbeddings], axis=1\n",
    "    )\n",
    "    concatenated_test_logits = np.concatenate(\n",
    "        [bertTestEmbeddings, robertaTestEmbeddings, electraTestEmbeddings], axis=1\n",
    "    )\n",
    "\n",
    "    train_labels = np.array(trainLabels)\n",
    "    test_labels = np.array(testLabels)\n",
    "\n",
    "    # Split treino/val a partir do conjunto de treino\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        concatenated_logits,\n",
    "        train_labels,\n",
    "        test_size=val_size,\n",
    "        stratify=train_labels,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Cria DataLoaders\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "    val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
    "    test_dataset = TensorDataset(torch.tensor(concatenated_test_logits, dtype=torch.float32), torch.tensor(test_labels, dtype=torch.long))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    param_grid = {\n",
    "        'lr': [1e-3, 5e-4],\n",
    "        'hidden_dim1': [64, 128],\n",
    "        'dropout': [0.3, 0.5]\n",
    "    }\n",
    "\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "    best_f1 = 0.0  # Mudan√ßa: agora otimiza pelo F1 ao inv√©s da loss\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    print(f\"üîç Testando {len(combinations)} combina√ß√µes de hiperpar√¢metros...\")\n",
    "\n",
    "    for i, params in enumerate(combinations, 1):\n",
    "        print(f\"\\n[{i}/{len(combinations)}] Testando: {params}\")\n",
    "        \n",
    "        model = create_model(\n",
    "            input_dim=X_train.shape[1],\n",
    "            hidden_dim1=params['hidden_dim1'],\n",
    "            dropout=params['dropout'],\n",
    "            num_classes=num_classes\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Treina por 5 epochs\n",
    "        for epoch in range(5):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * X_batch.size(0)\n",
    "            epoch_loss = running_loss / len(train_loader.dataset)\n",
    "            print(f\"  Epoch {epoch+1} - Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Avalia√ß√£o no conjunto de valida√ß√£o (agora com F1)\n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        val_true_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                val_predictions.extend(predicted.cpu().numpy())\n",
    "                val_true_labels.extend(y_batch.cpu().numpy())\n",
    "        \n",
    "        # Calcula F1 na valida√ß√£o\n",
    "        val_f1 = f1_score(val_true_labels, val_predictions, average='weighted')\n",
    "        val_acc = accuracy_score(val_true_labels, val_predictions)\n",
    "        \n",
    "        print(f\"  Val Accuracy: {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_params = params\n",
    "            best_model = model.state_dict().copy()  # Salva o estado do modelo\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üèÜ MELHORES RESULTADOS NA VALIDA√á√ÉO:\")\n",
    "    print(f\"Melhores par√¢metros: {best_params}\")\n",
    "    print(f\"Melhor F1 na valida√ß√£o: {best_f1:.4f}\")\n",
    "\n",
    "    # Recria o melhor modelo para avalia√ß√£o final\n",
    "    final_model = create_model(\n",
    "        input_dim=X_train.shape[1],\n",
    "        hidden_dim1=best_params['hidden_dim1'],\n",
    "        dropout=best_params['dropout'],\n",
    "        num_classes=num_classes\n",
    "    ).to(device)\n",
    "    final_model.load_state_dict(best_model)\n",
    "\n",
    "    # Avalia√ß√£o final no conjunto de teste\n",
    "    print(\"\\nüìä AVALIA√á√ÉO FINAL NO TESTE:\")\n",
    "    final_model.eval()\n",
    "    test_predictions = []\n",
    "    test_true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = final_model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            test_predictions.extend(predicted.cpu().numpy())\n",
    "            test_true_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    # Calcula m√©tricas finais\n",
    "    test_accuracy = accuracy_score(test_true_labels, test_predictions)\n",
    "    test_f1_weighted = f1_score(test_true_labels, test_predictions, average='weighted')\n",
    "    test_f1_macro = f1_score(test_true_labels, test_predictions, average='macro')\n",
    "    \n",
    "    # Tempo total\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"üìà RESULTADOS FINAIS:\")\n",
    "    print(f\"Acur√°cia no teste: {test_accuracy:.4f}\")\n",
    "    print(f\"F1-Score (weighted): {test_f1_weighted:.4f}\")\n",
    "    print(f\"F1-Score (macro): {test_f1_macro:.4f}\")\n",
    "    print(f\"‚è±Ô∏è  Tempo total: {total_time:.2f} segundos ({total_time/60:.2f} minutos)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nüìã RELAT√ìRIO DETALHADO:\")\n",
    "    print(classification_report(test_true_labels, test_predictions))\n",
    "    \n",
    "    # Retorna as m√©tricas principais\n",
    "    return {\n",
    "        'accuracy': test_accuracy,\n",
    "        'f1_weighted': test_f1_weighted,\n",
    "        'f1_macro': test_f1_macro,\n",
    "        'best_params': best_params,\n",
    "        'total_time_seconds': total_time,\n",
    "        'total_time_minutes': total_time/60\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca1a22be-8670-4cfb-be0c-7a6f524895a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def trainNNEmbL2(\n",
    "    bertTrainEmbeddings, robertaTrainEmbeddings, electraTrainEmbeddings,\n",
    "    bertTestEmbeddings, robertaTestEmbeddings, electraTestEmbeddings,\n",
    "    trainLabels, testLabels,\n",
    "    num_classes,\n",
    "    val_size=0.2,\n",
    "    batch_size=32,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "):\n",
    "    # Inicia contagem de tempo\n",
    "    start_time = time.time()\n",
    "    print(\"üïê Iniciando experimento...\")\n",
    "    bertTrainL2 = normalize(bertTrainEmbeddings, norm='l2', axis=1)\n",
    "    robertaTrainL2 = normalize(robertaTrainEmbeddings, norm='l2', axis=1)\n",
    "    electraTrainL2 = normalize(electraTrainEmbeddings, norm='l2', axis=1)\n",
    "\n",
    "    bertTestL2 = normalize(bertTestEmbeddings, norm='l2', axis=1)\n",
    "    robertaTestL2 = normalize(robertaTestEmbeddings, norm='l2', axis=1)\n",
    "    electraTestL2 = normalize(electraTestEmbeddings, norm='l2', axis=1)\n",
    "    # Concatena os logits das tr√™s redes\n",
    "    concatenated_logits = np.concatenate(\n",
    "        [bertTrainL2, robertaTrainL2, electraTrainL2], axis=1\n",
    "    )\n",
    "    concatenated_test_logits = np.concatenate(\n",
    "        [bertTestL2, robertaTestL2, electraTestL2], axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "    train_labels = np.array(trainLabels)\n",
    "    test_labels = np.array(testLabels)\n",
    "\n",
    "    # Split treino/val a partir do conjunto de treino\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        concatenated_logits,\n",
    "        train_labels,\n",
    "        test_size=val_size,\n",
    "        stratify=train_labels,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Cria DataLoaders\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "    val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
    "    test_dataset = TensorDataset(torch.tensor(concatenated_test_logits, dtype=torch.float32), torch.tensor(test_labels, dtype=torch.long))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    param_grid = {\n",
    "        'lr': [1e-3, 5e-4],\n",
    "        'hidden_dim1': [64, 128],\n",
    "        'dropout': [0.3, 0.5]\n",
    "    }\n",
    "\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "    best_f1 = 0.0  # Mudan√ßa: agora otimiza pelo F1 ao inv√©s da loss\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    print(f\"üîç Testando {len(combinations)} combina√ß√µes de hiperpar√¢metros...\")\n",
    "\n",
    "    for i, params in enumerate(combinations, 1):\n",
    "        print(f\"\\n[{i}/{len(combinations)}] Testando: {params}\")\n",
    "        \n",
    "        model = create_model(\n",
    "            input_dim=X_train.shape[1],\n",
    "            hidden_dim1=params['hidden_dim1'],\n",
    "            dropout=params['dropout'],\n",
    "            num_classes=num_classes\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Treina por 5 epochs\n",
    "        for epoch in range(5):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * X_batch.size(0)\n",
    "            epoch_loss = running_loss / len(train_loader.dataset)\n",
    "            print(f\"  Epoch {epoch+1} - Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Avalia√ß√£o no conjunto de valida√ß√£o (agora com F1)\n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        val_true_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                val_predictions.extend(predicted.cpu().numpy())\n",
    "                val_true_labels.extend(y_batch.cpu().numpy())\n",
    "        \n",
    "        # Calcula F1 na valida√ß√£o\n",
    "        val_f1 = f1_score(val_true_labels, val_predictions, average='weighted')\n",
    "        val_acc = accuracy_score(val_true_labels, val_predictions)\n",
    "        \n",
    "        print(f\"  Val Accuracy: {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_params = params\n",
    "            best_model = model.state_dict().copy()  # Salva o estado do modelo\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üèÜ MELHORES RESULTADOS NA VALIDA√á√ÉO:\")\n",
    "    print(f\"Melhores par√¢metros: {best_params}\")\n",
    "    print(f\"Melhor F1 na valida√ß√£o: {best_f1:.4f}\")\n",
    "\n",
    "    # Recria o melhor modelo para avalia√ß√£o final\n",
    "    final_model = create_model(\n",
    "        input_dim=X_train.shape[1],\n",
    "        hidden_dim1=best_params['hidden_dim1'],\n",
    "        dropout=best_params['dropout'],\n",
    "        num_classes=num_classes\n",
    "    ).to(device)\n",
    "    final_model.load_state_dict(best_model)\n",
    "\n",
    "    # Avalia√ß√£o final no conjunto de teste\n",
    "    print(\"\\nüìä AVALIA√á√ÉO FINAL NO TESTE:\")\n",
    "    final_model.eval()\n",
    "    test_predictions = []\n",
    "    test_true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = final_model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            test_predictions.extend(predicted.cpu().numpy())\n",
    "            test_true_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    # Calcula m√©tricas finais\n",
    "    test_accuracy = accuracy_score(test_true_labels, test_predictions)\n",
    "    test_f1_weighted = f1_score(test_true_labels, test_predictions, average='weighted')\n",
    "    test_f1_macro = f1_score(test_true_labels, test_predictions, average='macro')\n",
    "    \n",
    "    # Tempo total\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"üìà RESULTADOS FINAIS:\")\n",
    "    print(f\"Acur√°cia no teste: {test_accuracy:.4f}\")\n",
    "    print(f\"F1-Score (weighted): {test_f1_weighted:.4f}\")\n",
    "    print(f\"F1-Score (macro): {test_f1_macro:.4f}\")\n",
    "    print(f\"‚è±Ô∏è  Tempo total: {total_time:.2f} segundos ({total_time/60:.2f} minutos)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nüìã RELAT√ìRIO DETALHADO:\")\n",
    "    print(classification_report(test_true_labels, test_predictions))\n",
    "    \n",
    "    # Retorna as m√©tricas principais\n",
    "    return {\n",
    "        'accuracy': test_accuracy,\n",
    "        'f1_weighted': test_f1_weighted,\n",
    "        'f1_macro': test_f1_macro,\n",
    "        'best_params': best_params,\n",
    "        'total_time_seconds': total_time,\n",
    "        'total_time_minutes': total_time/60\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17a0890c-b05d-4af5-9018-9e678f9d3a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def create_model_2(input_dim, hidden_dim1, dropout, num_classes=2):\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.bn = nn.BatchNorm1d(input_dim)\n",
    "            self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.fc2 = nn.Linear(hidden_dim1, 32)\n",
    "            self.fc3 = nn.Linear(32, num_classes)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.bn(x)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.dropout(x)\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "\n",
    "    return Net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0096c96c-2887-48ed-bf3a-0f9c1dd29807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "def trainNNEmbOutra(\n",
    "    bertTrainEmbeddings, robertaTrainEmbeddings, electraTrainEmbeddings,\n",
    "    bertTestEmbeddings, robertaTestEmbeddings, electraTestEmbeddings,\n",
    "    trainLabels, testLabels,\n",
    "    num_classes,\n",
    "    val_size=0.2,\n",
    "    batch_size=32,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "):\n",
    "    # Inicia contagem de tempo\n",
    "    start_time = time.time()\n",
    "    print(\"üïê Iniciando experimento...\")\n",
    "    \n",
    "    # Concatena os logits das tr√™s redes\n",
    "    concatenated_logits = np.concatenate(\n",
    "        [bertTrainEmbeddings, robertaTrainEmbeddings, electraTrainEmbeddings], axis=1\n",
    "    )\n",
    "    concatenated_test_logits = np.concatenate(\n",
    "        [bertTestEmbeddings, robertaTestEmbeddings, electraTestEmbeddings], axis=1\n",
    "    )\n",
    "\n",
    "    train_labels = np.array(trainLabels)\n",
    "    test_labels = np.array(testLabels)\n",
    "\n",
    "    # Split treino/val a partir do conjunto de treino\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        concatenated_logits,\n",
    "        train_labels,\n",
    "        test_size=val_size,\n",
    "        stratify=train_labels,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Cria DataLoaders\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "    val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
    "    test_dataset = TensorDataset(torch.tensor(concatenated_test_logits, dtype=torch.float32), torch.tensor(test_labels, dtype=torch.long))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    param_grid = {\n",
    "        'lr': [1e-3, 5e-4],\n",
    "        'hidden_dim1': [64, 128],\n",
    "        'dropout': [0.3, 0.5]\n",
    "    }\n",
    "\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "    best_f1 = 0.0  # Mudan√ßa: agora otimiza pelo F1 ao inv√©s da loss\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    print(f\"üîç Testando {len(combinations)} combina√ß√µes de hiperpar√¢metros...\")\n",
    "\n",
    "    for i, params in enumerate(combinations, 1):\n",
    "        print(f\"\\n[{i}/{len(combinations)}] Testando: {params}\")\n",
    "        \n",
    "        model = create_model_2(\n",
    "            input_dim=X_train.shape[1],\n",
    "            hidden_dim1=params['hidden_dim1'],\n",
    "            dropout=params['dropout'],\n",
    "            num_classes=num_classes\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Treina por 5 epochs\n",
    "        for epoch in range(5):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * X_batch.size(0)\n",
    "            epoch_loss = running_loss / len(train_loader.dataset)\n",
    "            print(f\"  Epoch {epoch+1} - Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Avalia√ß√£o no conjunto de valida√ß√£o (agora com F1)\n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        val_true_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                val_predictions.extend(predicted.cpu().numpy())\n",
    "                val_true_labels.extend(y_batch.cpu().numpy())\n",
    "        \n",
    "        # Calcula F1 na valida√ß√£o\n",
    "        val_f1 = f1_score(val_true_labels, val_predictions, average='weighted')\n",
    "        val_acc = accuracy_score(val_true_labels, val_predictions)\n",
    "        \n",
    "        print(f\"  Val Accuracy: {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_params = params\n",
    "            best_model = model.state_dict().copy()  # Salva o estado do modelo\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üèÜ MELHORES RESULTADOS NA VALIDA√á√ÉO:\")\n",
    "    print(f\"Melhores par√¢metros: {best_params}\")\n",
    "    print(f\"Melhor F1 na valida√ß√£o: {best_f1:.4f}\")\n",
    "\n",
    "    # Recria o melhor modelo para avalia√ß√£o final\n",
    "    final_model = create_model_2(\n",
    "        input_dim=X_train.shape[1],\n",
    "        hidden_dim1=best_params['hidden_dim1'],\n",
    "        dropout=best_params['dropout'],\n",
    "        num_classes=num_classes\n",
    "    ).to(device)\n",
    "    final_model.load_state_dict(best_model)\n",
    "\n",
    "    # Avalia√ß√£o final no conjunto de teste\n",
    "    print(\"\\nüìä AVALIA√á√ÉO FINAL NO TESTE:\")\n",
    "    final_model.eval()\n",
    "    test_predictions = []\n",
    "    test_true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = final_model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            test_predictions.extend(predicted.cpu().numpy())\n",
    "            test_true_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    # Calcula m√©tricas finais\n",
    "    test_accuracy = accuracy_score(test_true_labels, test_predictions)\n",
    "    test_f1_weighted = f1_score(test_true_labels, test_predictions, average='weighted')\n",
    "    test_f1_macro = f1_score(test_true_labels, test_predictions, average='macro')\n",
    "    \n",
    "    # Tempo total\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"üìà RESULTADOS FINAIS:\")\n",
    "    print(f\"Acur√°cia no teste: {test_accuracy:.4f}\")\n",
    "    print(f\"F1-Score (weighted): {test_f1_weighted:.4f}\")\n",
    "    print(f\"F1-Score (macro): {test_f1_macro:.4f}\")\n",
    "    print(f\"‚è±Ô∏è  Tempo total: {total_time:.2f} segundos ({total_time/60:.2f} minutos)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nüìã RELAT√ìRIO DETALHADO:\")\n",
    "    print(classification_report(test_true_labels, test_predictions))\n",
    "    \n",
    "    # Retorna as m√©tricas principais\n",
    "    return {\n",
    "        'accuracy': test_accuracy,\n",
    "        'f1_weighted': test_f1_weighted,\n",
    "        'f1_macro': test_f1_macro,\n",
    "        'best_params': best_params,\n",
    "        'total_time_seconds': total_time,\n",
    "        'total_time_minutes': total_time/60\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4070d447-1b28-42cd-89e5-e72250288624",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_logits_file = np.load('logits_google-bert/bert-base-uncased_emotion_train_bert-base-uncased.npz')\n",
    "roberta_logits_file = np.load('logits_roberta-base_emotion_train_roberta-base.npz')\n",
    "electra_logits_file = np.load('logits_google/electra-base-discriminator_emotion_train_electra-base-discriminator.npz')\n",
    "\n",
    "\n",
    "bert_logits_test_file = np.load('logits_google-bert/bert-base-uncased_emotion_test_bert-base-uncased.npz')\n",
    "roberta_logits_test_file = np.load('logits_roberta-base_emotion_test_roberta-base.npz')\n",
    "electra_logits_test_file = np.load('logits_google/electra-base-discriminator_emotion_test_electra-base-discriminator.npz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd5ca1ef-2c92-488a-87b1-cb55d6e8fca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTADOS INDIVIDUAIS:\n",
      "BERT     - Acc: 0.9245 | F1: 0.9242\n",
      "RoBERTa  - Acc: 0.9295 | F1: 0.9290\n",
      "ELECTRA  - Acc: 0.9320 | F1: 0.9314\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# BERT\n",
    "bert_preds = np.argmax(bertTestLogits['logits'], axis=1)\n",
    "bert_acc = accuracy_score(testLabels, bert_preds)\n",
    "bert_f1 = f1_score(testLabels, bert_preds, average='weighted')\n",
    "\n",
    "# RoBERTa  \n",
    "roberta_preds = np.argmax(robertaTestLogits['logits'], axis=1)\n",
    "roberta_acc = accuracy_score(testLabels, roberta_preds)\n",
    "roberta_f1 = f1_score(testLabels, roberta_preds, average='weighted')\n",
    "\n",
    "# ELECTRA\n",
    "electra_preds = np.argmax(electraTestLogits['logits'], axis=1)\n",
    "electra_acc = accuracy_score(testLabels, electra_preds)\n",
    "electra_f1 = f1_score(testLabels, electra_preds, average='weighted')\n",
    "\n",
    "# Resultados\n",
    "print(\"RESULTADOS INDIVIDUAIS:\")\n",
    "print(f\"BERT     - Acc: {bert_acc:.4f} | F1: {bert_f1:.4f}\")\n",
    "print(f\"RoBERTa  - Acc: {roberta_acc:.4f} | F1: {roberta_f1:.4f}\")\n",
    "print(f\"ELECTRA  - Acc: {electra_acc:.4f} | F1: {electra_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "12267608-78ad-4e18-8b1e-febb13e0d908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score Results:\n",
      "BERT:           0.9242\n",
      "RoBERTa:        0.9290\n",
      "ELECTRA:        0.9314\n",
      "Voto Majorit√°rio: 0.9298\n",
      "M√©dia Logits:     0.9304\n",
      "Or√°culo:          0.9561\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bert': 0.924232012996299,\n",
       " 'roberta': 0.9289537024600699,\n",
       " 'electra': 0.9314334572387593,\n",
       " 'majority': 0.9298389504679578,\n",
       " 'avg_logits': 0.9303517155707239,\n",
       " 'oracle': 0.9561256834863818}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "def avaliar_ensemble_logits(logits_bert, logits_roberta, logits_electra, true_labels):\n",
    "    \"\"\"\n",
    "    Avalia ensemble de logits com diferentes estrat√©gias\n",
    "    \"\"\"\n",
    "    # Predi√ß√µes individuais\n",
    "    pred_bert = np.argmax(logits_bert, axis=1)\n",
    "    pred_roberta = np.argmax(logits_roberta, axis=1)\n",
    "    pred_electra = np.argmax(logits_electra, axis=1)\n",
    "    \n",
    "    # 1. F1 individual\n",
    "    f1_bert = f1_score(true_labels, pred_bert, average='weighted')\n",
    "    f1_roberta = f1_score(true_labels, pred_roberta, average='weighted')\n",
    "    f1_electra = f1_score(true_labels, pred_electra, average='weighted')\n",
    "    \n",
    "    # 2. Voto majorit√°rio\n",
    "    votes = np.column_stack([pred_bert, pred_roberta, pred_electra])\n",
    "    pred_majority = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=1, arr=votes)\n",
    "    f1_majority = f1_score(true_labels, pred_majority, average='weighted')\n",
    "    \n",
    "    # 3. M√©dia dos logits\n",
    "    logits_avg = (logits_bert + logits_roberta + logits_electra) / 3\n",
    "    pred_avg = np.argmax(logits_avg, axis=1)\n",
    "    f1_avg = f1_score(true_labels, pred_avg, average='weighted')\n",
    "    \n",
    "    # 4. Or√°culo (melhor predi√ß√£o para cada amostra)\n",
    "    all_preds = np.column_stack([pred_bert, pred_roberta, pred_electra])\n",
    "    pred_oracle = []\n",
    "    for i in range(len(true_labels)):\n",
    "        # Para cada amostra, pega a predi√ß√£o que est√° certa (se houver)\n",
    "        correct_preds = all_preds[i][all_preds[i] == true_labels[i]]\n",
    "        if len(correct_preds) > 0:\n",
    "            pred_oracle.append(correct_preds[0])\n",
    "        else:\n",
    "            # Se nenhuma est√° certa, usa voto majorit√°rio\n",
    "            pred_oracle.append(pred_majority[i])\n",
    "    \n",
    "    f1_oracle = f1_score(true_labels, pred_oracle, average='weighted')\n",
    "    \n",
    "    print(\"F1-Score Results:\")\n",
    "    print(f\"BERT:           {f1_bert:.4f}\")\n",
    "    print(f\"RoBERTa:        {f1_roberta:.4f}\")\n",
    "    print(f\"ELECTRA:        {f1_electra:.4f}\")\n",
    "    print(f\"Voto Majorit√°rio: {f1_majority:.4f}\")\n",
    "    print(f\"M√©dia Logits:     {f1_avg:.4f}\")\n",
    "    print(f\"Or√°culo:          {f1_oracle:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'bert': f1_bert,\n",
    "        'roberta': f1_roberta, \n",
    "        'electra': f1_electra,\n",
    "        'majority': f1_majority,\n",
    "        'avg_logits': f1_avg,\n",
    "        'oracle': f1_oracle\n",
    "    }\n",
    "\n",
    "# Exemplo de uso:\n",
    "avaliar_ensemble_logits(bertTestLogits['logits'], robertaTestLogits['logits'], electraTestLogits['logits'], bertTestLogits['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "528c2a2e-4a1e-4f5e-96f7-f0e723c75b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïê Iniciando experimento NN com Logits...\n",
      "üìä Shape dos logits concatenados: (16000, 18)\n",
      "üìä N√∫mero de classes: 6\n",
      "üîç Testando 8 combina√ß√µes de hiperpar√¢metros...\n",
      "\n",
      "[1/8] Testando: {'lr': 0.001, 'hidden_dim1': 64, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 0.1573\n",
      "  Epoch 2 - Loss: 0.0472\n",
      "  Epoch 3 - Loss: 0.0464\n",
      "  Epoch 4 - Loss: 0.0443\n",
      "  Epoch 5 - Loss: 0.0413\n",
      "  Val Accuracy: 0.9825 | Val F1: 0.9824\n",
      "\n",
      "[2/8] Testando: {'lr': 0.001, 'hidden_dim1': 64, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 0.2044\n",
      "  Epoch 2 - Loss: 0.0580\n",
      "  Epoch 3 - Loss: 0.0551\n",
      "  Epoch 4 - Loss: 0.0503\n",
      "  Epoch 5 - Loss: 0.0459\n",
      "  Val Accuracy: 0.9806 | Val F1: 0.9804\n",
      "\n",
      "[3/8] Testando: {'lr': 0.001, 'hidden_dim1': 128, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 0.1270\n",
      "  Epoch 2 - Loss: 0.0457\n",
      "  Epoch 3 - Loss: 0.0397\n",
      "  Epoch 4 - Loss: 0.0418\n",
      "  Epoch 5 - Loss: 0.0389\n",
      "  Val Accuracy: 0.9809 | Val F1: 0.9807\n",
      "\n",
      "[4/8] Testando: {'lr': 0.001, 'hidden_dim1': 128, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 0.1227\n",
      "  Epoch 2 - Loss: 0.0504\n",
      "  Epoch 3 - Loss: 0.0480\n",
      "  Epoch 4 - Loss: 0.0450\n",
      "  Epoch 5 - Loss: 0.0415\n",
      "  Val Accuracy: 0.9822 | Val F1: 0.9821\n",
      "\n",
      "[5/8] Testando: {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 0.2468\n",
      "  Epoch 2 - Loss: 0.0523\n",
      "  Epoch 3 - Loss: 0.0476\n",
      "  Epoch 4 - Loss: 0.0469\n",
      "  Epoch 5 - Loss: 0.0432\n",
      "  Val Accuracy: 0.9806 | Val F1: 0.9804\n",
      "\n",
      "[6/8] Testando: {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 0.3102\n",
      "  Epoch 2 - Loss: 0.0624\n",
      "  Epoch 3 - Loss: 0.0530\n",
      "  Epoch 4 - Loss: 0.0523\n",
      "  Epoch 5 - Loss: 0.0473\n",
      "  Val Accuracy: 0.9831 | Val F1: 0.9831\n",
      "\n",
      "[7/8] Testando: {'lr': 0.0005, 'hidden_dim1': 128, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 0.1708\n",
      "  Epoch 2 - Loss: 0.0481\n",
      "  Epoch 3 - Loss: 0.0447\n",
      "  Epoch 4 - Loss: 0.0441\n",
      "  Epoch 5 - Loss: 0.0408\n",
      "  Val Accuracy: 0.9822 | Val F1: 0.9821\n",
      "\n",
      "[8/8] Testando: {'lr': 0.0005, 'hidden_dim1': 128, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 0.2071\n",
      "  Epoch 2 - Loss: 0.0524\n",
      "  Epoch 3 - Loss: 0.0475\n",
      "  Epoch 4 - Loss: 0.0452\n",
      "  Epoch 5 - Loss: 0.0437\n",
      "  Val Accuracy: 0.9803 | Val F1: 0.9801\n",
      "\n",
      "============================================================\n",
      "üèÜ MELHORES RESULTADOS NA VALIDA√á√ÉO:\n",
      "Melhores par√¢metros: {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.5}\n",
      "Melhor F1 na valida√ß√£o: 0.9831\n",
      "\n",
      "üìä AVALIA√á√ÉO FINAL NO TESTE:\n",
      "============================================================\n",
      "üìà RESULTADOS FINAIS:\n",
      "Acur√°cia no teste: 0.9270\n",
      "F1-Score (weighted): 0.9272\n",
      "F1-Score (macro): 0.8813\n",
      "‚è±Ô∏è  Tempo total: 8.04 segundos (0.13 minutos)\n",
      "============================================================\n",
      "\n",
      "üìã RELAT√ìRIO DETALHADO:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97       581\n",
      "           1       0.95      0.94      0.95       695\n",
      "           2       0.80      0.83      0.82       159\n",
      "           3       0.94      0.93      0.93       275\n",
      "           4       0.90      0.88      0.89       224\n",
      "           5       0.71      0.76      0.74        66\n",
      "\n",
      "    accuracy                           0.93      2000\n",
      "   macro avg       0.88      0.88      0.88      2000\n",
      "weighted avg       0.93      0.93      0.93      2000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.927,\n",
       " 'f1_weighted': 0.9272270668257073,\n",
       " 'f1_macro': 0.8812588180373443,\n",
       " 'best_params': {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.5},\n",
       " 'total_time_seconds': 8.037059545516968,\n",
       " 'total_time_minutes': 0.1339509924252828}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=1.0; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=200, subsample=1.0; total time=   1.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=4, n_estimators=200, subsample=1.0; total time=   1.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=1.0; total time=   1.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=4, n_estimators=100, subsample=1.0; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=4, n_estimators=200, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=4, n_estimators=100, subsample=1.0; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=4, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=4, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=200, subsample=1.0; total time=   1.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=4, n_estimators=200, subsample=1.0; total time=   1.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=1.0; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=0.8; total time=   2.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.4s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=200, subsample=1.0; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.9s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=200, subsample=1.0; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=1.0; total time=   1.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=4, n_estimators=100, subsample=1.0; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=200, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=100, subsample=1.0; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=4, n_estimators=100, subsample=1.0; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=4, n_estimators=200, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=0.8; total time=   1.0s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=1.0; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=4, n_estimators=200, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=1.0; total time=   0.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=4, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=4, n_estimators=100, subsample=1.0; total time=   0.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=1.0; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=200, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=100, subsample=1.0; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=1.0; total time=   1.9s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=4, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=1.0; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=1.0; total time=   1.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=4, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=1.0; total time=   1.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=100, subsample=1.0; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=4, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=1.0; total time=   2.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=200, subsample=0.8; total time=   2.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=1.0; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=4, n_estimators=200, subsample=1.0; total time=   1.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=1.0; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=1.0; total time=   1.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=4, n_estimators=100, subsample=1.0; total time=   0.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=1.0; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=1.0; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=1.0; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=200, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=0.8; total time=   1.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=4, n_estimators=200, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=1.0; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   1.0s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=4, n_estimators=200, subsample=1.0; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=4, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=1.0; total time=   2.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=4, n_estimators=200, subsample=1.0; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=1.0; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=1.0; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=200, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=4, n_estimators=200, subsample=1.0; total time=   1.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=1.0; total time=   1.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=4, n_estimators=200, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=1.0; total time=   1.4s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=100, subsample=0.8; total time=   0.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=4, n_estimators=200, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=4, n_estimators=200, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=4, n_estimators=200, subsample=1.0; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=100, subsample=1.0; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=1.0; total time=   1.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=4, n_estimators=200, subsample=1.0; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=0.8; total time=   2.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   1.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=1.0; total time=   1.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=4, n_estimators=100, subsample=1.0; total time=   0.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=4, n_estimators=200, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=100, subsample=0.8; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=4, n_estimators=200, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=0.8; total time=   2.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=4, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=4, n_estimators=200, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=200, subsample=1.0; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=1.0; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=100, subsample=1.0; total time=   0.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=0.8; total time=   1.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=4, n_estimators=200, subsample=1.0; total time=   1.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=100, subsample=1.0; total time=   1.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=4, n_estimators=100, subsample=1.0; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=4, n_estimators=200, subsample=1.0; total time=   1.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=1.0; total time=   1.4s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=4, n_estimators=200, subsample=1.0; total time=   1.6s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=0.8; total time=   1.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=6, n_estimators=100, subsample=0.8; total time=   1.5s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=4, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=6, n_estimators=200, subsample=1.0; total time=   1.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=4, n_estimators=100, subsample=1.0; total time=   0.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=0.8; total time=   2.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=4, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=1.0; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.7s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=4, n_estimators=200, subsample=1.0; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=1.0; total time=   0.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=4, n_estimators=100, subsample=1.0; total time=   0.9s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=4, n_estimators=200, subsample=0.8; total time=   1.4s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=4, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=1.0; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.05, max_depth=4, n_estimators=200, subsample=1.0; total time=   1.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=1.0; total time=   0.9s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=4, n_estimators=200, subsample=0.8; total time=   1.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.05, max_depth=6, n_estimators=100, subsample=1.0; total time=   1.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=4, n_estimators=100, subsample=0.8; total time=   0.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=4, n_estimators=200, subsample=0.8; total time=   2.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.6s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=6, n_estimators=100, subsample=0.8; total time=   0.9s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=4, n_estimators=100, subsample=1.0; total time=   0.8s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=6, n_estimators=200, subsample=1.0; total time=   2.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=4, n_estimators=100, subsample=1.0; total time=   0.7s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=6, n_estimators=200, subsample=1.0; total time=   0.9s\n"
     ]
    }
   ],
   "source": [
    "bertTrainLogits = bert_logits_file\n",
    "robertaTrainLogits = roberta_logits_file\n",
    "electraTrainLogits = electra_logits_file\n",
    "\n",
    "bertTestLogits = bert_logits_test_file\n",
    "robertaTestLogits = roberta_logits_test_file\n",
    "electraTestLogits = electra_logits_test_file\n",
    "\n",
    "trainLabels = bert_logits_file['labels']\n",
    "testLabels = bert_logits_test_file['labels']\n",
    "\n",
    "trainNNLogits(\n",
    "    bertTrainLogits, robertaTrainLogits, electraTrainLogits,\n",
    "    bertTestLogits, robertaTestLogits, electraTestLogits,\n",
    "    trainLabels, testLabels,\n",
    "    num_classes=6,\n",
    "    val_size=0.2,\n",
    "    batch_size=32,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e2b9acce-6ece-4ea0-8aee-d957d3b8a5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïê Iniciando experimento XGBoost...\n",
      "üìä Shape dos dados concatenados: (16000, 18)\n",
      "üìä N√∫mero de classes: 6\n",
      "üîç Testando 72 combina√ß√µes de hiperpar√¢metros com CV=3...\n",
      "‚öôÔ∏è  Rodando GridSearch para XGBoost...\n",
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
      "‚úÖ GridSearch conclu√≠do em 9.85 segundos\n",
      "\n",
      "============================================================\n",
      "üèÜ MELHORES HIPERPAR√ÇMETROS:\n",
      "{'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}\n",
      "üéØ Melhor score no CV: 0.9876\n",
      "\n",
      "üìä AVALIA√á√ÉO NO CONJUNTO DE VALIDA√á√ÉO:\n",
      "Valida√ß√£o - Acur√°cia: 0.9847\n",
      "Valida√ß√£o - F1 Weighted: 0.9846\n",
      "Valida√ß√£o - F1 Macro: 0.9717\n",
      "\n",
      "üîÑ Retreinando modelo final com treino + valida√ß√£o...\n",
      "\n",
      "üìà AVALIA√á√ÉO FINAL NO TESTE:\n",
      "============================================================\n",
      "üìà RESULTADOS FINAIS:\n",
      "Acur√°cia no teste: 0.9220\n",
      "F1-Score (weighted): 0.9218\n",
      "F1-Score (macro): 0.8789\n",
      "‚è±Ô∏è  Tempo total: 10.02 segundos (0.17 minutos)\n",
      "============================================================\n",
      "\n",
      "üìã RELAT√ìRIO DETALHADO:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96       581\n",
      "           1       0.94      0.94      0.94       695\n",
      "           2       0.82      0.81      0.81       159\n",
      "           3       0.93      0.92      0.92       275\n",
      "           4       0.88      0.89      0.88       224\n",
      "           5       0.77      0.74      0.75        66\n",
      "\n",
      "    accuracy                           0.92      2000\n",
      "   macro avg       0.88      0.88      0.88      2000\n",
      "weighted avg       0.92      0.92      0.92      2000\n",
      "\n",
      "\n",
      "üîç Top 5 features mais importantes:\n",
      "  1. Feature 0: 0.1597\n",
      "  2. Feature 13: 0.1501\n",
      "  3. Feature 3: 0.1191\n",
      "  4. Feature 1: 0.1121\n",
      "  5. Feature 4: 0.0862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "               colsample_bylevel=None, colsample_bynode=None,\n",
       "               colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "               enable_categorical=False, eval_metric='mlogloss',\n",
       "               feature_types=None, feature_weights=None, gamma=None,\n",
       "               grow_policy=None, importance_type=None,\n",
       "               interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
       "               max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "               max_delta_step=None, max_depth=3, max_leaves=None,\n",
       "               min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "               multi_strategy=None, n_estimators=100, n_jobs=None, num_class=6, ...),\n",
       " 'accuracy': 0.922,\n",
       " 'f1_weighted': 0.9218290067075109,\n",
       " 'f1_macro': 0.8789487015221361,\n",
       " 'best_params': {'colsample_bytree': 0.8,\n",
       "  'learning_rate': 0.05,\n",
       "  'max_depth': 3,\n",
       "  'n_estimators': 100,\n",
       "  'subsample': 0.8},\n",
       " 'best_cv_score': 0.9876464123023941,\n",
       " 'total_time_seconds': 10.017797231674194,\n",
       " 'total_time_minutes': 0.1669632871945699,\n",
       " 'val_accuracy': 0.9846875,\n",
       " 'val_f1_weighted': 0.9845703544593013,\n",
       " 'val_f1_macro': 0.9716750572956636}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainXGBoostOnly( bertTrainLogits, robertaTrainLogits, electraTrainLogits,\n",
    "    bertTestLogits, robertaTestLogits, electraTestLogits,\n",
    "    trainLabels, testLabels,\n",
    "    num_classes=6,\n",
    "    val_size=0.2,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0e40badf-75b0-42a0-b752-6a8c11030f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['embeddings_roberta-base_imdb_test_roberta-base.npz', 'embeddings_google_electra-base-discriminator_emotion_train_electra-base-discriminator.npz', 'embeddings_roberta-base_emotion_train_roberta-base.npz', 'embeddings_google-bert_bert-base-uncased_clincoos_test_bert-base-uncased.npz', 'embeddings_google-bert_bert-base-uncased_agnews_train_bert-base-uncased.npz', 'logits_google_electra-base-discriminator_train_imdb_2_2.npz', 'embeddings_roberta-base_imdb_train_roberta-base.npz', 'logits_roberta-base_clincoos_train_roberta-base.npz', 'logits_roberta-base_train_agnews.npz', 'embeddings_google-bert_bert-base-uncased_amazonpolarity_val_bert-base-uncased.npz', 'embeddings_google_electra-base-discriminator_banking77_train_electra-base-discriminator.npz', 'logits_roberta-base_amazonpolarity_val_roberta-base.npz', 'logits_roberta-base_emotion_val_roberta-base.npz', 'logits_google-bert_bert-base-uncased_test_agnews.npz', 'embeddings_google-bert_bert-base-uncased_snli_train_bert-base-uncased.npz', 'logits_google_electra-base-discriminator_test_agnews.npz', 'logits_google-bert_bert-base-uncased_test_snli.npz', 'logits_google-bert_bert-base-uncased_train_snli.npz', 'embeddings_google-bert_bert-base-uncased_clincoos_val_bert-base-uncased.npz', 'logits_roberta-base_test_yelp.npz', 'embeddings_google_electra-base-discriminator_imdb_train_electra-base-discriminator.npz', 'embeddings_google-bert_bert-base-uncased_imdb_test_bert-base-uncased.npz', 'logits_google-bert_bert-base-uncased_train_imdb.npz', 'embeddings_roberta-base_banking77_train_roberta-base.npz', 'embeddings_roberta-base_clincoos_val_roberta-base.npz', 'embeddings_google-bert_bert-base-uncased_emotion_train_bert-base-uncased.npz', 'embeddings_google_electra-base-discriminator_banking77_test_electra-base-discriminator.npz', 'logits_google-bert_bert-base-uncased_train_agnews.npz', 'logits_google_electra-base-discriminator_train_yelp.npz', 'embeddings_roberta-base_amazonpolarity_test_roberta-base.npz', 'embeddings_roberta-base_yelp_train_roberta-base.npz', 'embeddings_google_electra-base-discriminator_yelp_train_electra-base-discriminator.npz', 'embeddings_google_electra-base-discriminator_amazonpolarity_val_electra-base-discriminator.npz', 'embeddings_google-bert_bert-base-uncased_emotion_test_bert-base-uncased.npz', 'logits_roberta-base_clincoos_test_roberta-base.npz', 'embeddings_google-bert_bert-base-uncased_clincoos_train_bert-base-uncased.npz', 'logits_google_electra-base-discriminator_train_imdb.npz', 'embeddings_google_electra-base-discriminator_amazonpolarity_train_electra-base-discriminator.npz', 'embeddings_roberta-base_emotion_val_roberta-base.npz', 'embeddings_roberta-base_snli_train_roberta-base.npz', 'embeddings_google_electra-base-discriminator_imdb_test_electra-base-discriminator.npz', 'embeddings_google_electra-base-discriminator_emotion_val_electra-base-discriminator.npz', 'embeddings_google-bert_bert-base-uncased_banking77_test_bert-base-uncased.npz', 'logits_roberta-base_emotion_test_roberta-base.npz', 'logits_google_electra-base-discriminator_test_yelp.npz', 'embeddings_roberta-base_snli_test_roberta-base.npz', 'logits_roberta-base_test_imdb.npz', 'embeddings_google_electra-base-discriminator_clincoos_train_electra-base-discriminator.npz', 'logits_roberta-base_test_agnews.npz', 'logits_roberta-base_banking77_val_roberta-base.npz', 'embeddings_roberta-base_banking77_val_roberta-base.npz', 'logits_roberta-base_train_snli.npz', 'logits_roberta-base_amazonpolarity_test_roberta-base.npz', 'logits_roberta-base_train_yelp.npz', 'embeddings_google_electra-base-discriminator_emotion_test_electra-base-discriminator.npz', 'embeddings_google-bert_bert-base-uncased_imdb_train_bert-base-uncased.npz', 'embeddings_roberta-base_agnews_train_roberta-base.npz', 'embeddings_google_electra-base-discriminator_snli_test_electra-base-discriminator.npz', 'logits_google-bert_bert-base-uncased_test_yelp.npz', 'embeddings_google-bert_bert-base-uncased_amazonpolarity_test_bert-base-uncased.npz', 'embeddings_google_electra-base-discriminator_agnews_train_electra-base-discriminator.npz', 'embeddings_roberta-base_yelp_test_roberta-base.npz', 'embeddings_google_electra-base-discriminator_agnews_test_electra-base-discriminator.npz', 'logits_roberta-base_banking77_train_roberta-base.npz', 'embeddings_google-bert_bert-base-uncased_yelp_train_bert-base-uncased.npz', 'embeddings_google_electra-base-discriminator_clincoos_val_electra-base-discriminator.npz', 'embeddings_roberta-base_amazonpolarity_val_roberta-base.npz', 'embeddings_google-bert_bert-base-uncased_agnews_test_bert-base-uncased.npz', 'logits_roberta-base_train_imdb.npz', 'embeddings_roberta-base_agnews_test_roberta-base.npz', 'embeddings_google-bert_bert-base-uncased_banking77_train_bert-base-uncased.npz', 'embeddings_google-bert_bert-base-uncased_emotion_val_bert-base-uncased.npz', 'embeddings_google-bert_bert-base-uncased_banking77_val_bert-base-uncased.npz', 'logits_roberta-base_emotion_train_roberta-base.npz', 'logits_google_electra-base-discriminator_train_agnews.npz', 'embeddings_google_electra-base-discriminator_snli_train_electra-base-discriminator.npz', 'embeddings_google_electra-base-discriminator_banking77_val_electra-base-discriminator.npz', 'logits_roberta-base_test_snli.npz', 'embeddings_roberta-base_amazonpolarity_train_roberta-base.npz', 'logits_roberta-base_amazonpolarity_train_roberta-base.npz', 'logits_google_electra-base-discriminator_test_imdb.npz', 'embeddings_roberta-base_banking77_test_roberta-base.npz', 'logits_google_electra-base-discriminator_train_snli.npz', 'embeddings_google_electra-base-discriminator_clincoos_test_electra-base-discriminator.npz', 'logits_google_electra-base-discriminator_test_snli.npz', 'logits_google_electra-base-discriminator_test_imdb_2_2.npz', 'embeddings_google-bert_bert-base-uncased_snli_test_bert-base-uncased.npz', 'embeddings_google-bert_bert-base-uncased_amazonpolarity_train_bert-base-uncased.npz', 'embeddings_google_electra-base-discriminator_yelp_test_electra-base-discriminator.npz', 'logits_roberta-base_clincoos_val_roberta-base.npz', 'embeddings_google_electra-base-discriminator_amazonpolarity_test_electra-base-discriminator.npz', 'embeddings_roberta-base_clincoos_test_roberta-base.npz', 'logits_google-bert_bert-base-uncased_train_yelp.npz', 'embeddings_google-bert_bert-base-uncased_yelp_test_bert-base-uncased.npz', 'embeddings_roberta-base_emotion_test_roberta-base.npz', 'logits_roberta-base_banking77_test_roberta-base.npz', 'embeddings_roberta-base_clincoos_train_roberta-base.npz', 'logits_google-bert_bert-base-uncased_test_imdb.npz']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Lista todos os arquivos .npz no diret√≥rio atual\n",
    "arquivos_npz = [f for f in os.listdir('.') if f.endswith('.npz')]\n",
    "print(arquivos_npz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ed51c9b-973d-4391-8585-dedeac90eabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMOTION - Shapes:\n",
      "Roberta - Train: (16000, 768), Test: (2000, 768)\n",
      "BERT - Train: (16000, 768), Test: (2000, 768)\n",
      "Electra - Train: (16000, 768), Test: (2000, 768)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# EMOTION - ROBERTA\n",
    "train_data = np.load('embeddings_roberta-base_emotion_train_roberta-base.npz')\n",
    "test_data = np.load('embeddings_roberta-base_emotion_test_roberta-base.npz')\n",
    "X_train_emotion_roberta = train_data['embeddings']\n",
    "y_train_emotion_roberta = train_data['labels']\n",
    "X_test_emotion_roberta = test_data['embeddings'] \n",
    "y_test_emotion_roberta = test_data['labels']\n",
    "\n",
    "# EMOTION - BERT\n",
    "train_data = np.load('embeddings_google-bert_bert-base-uncased_emotion_train_bert-base-uncased.npz')\n",
    "test_data = np.load('embeddings_google-bert_bert-base-uncased_emotion_test_bert-base-uncased.npz')\n",
    "X_train_emotion_bert = train_data['embeddings']\n",
    "y_train_emotion_bert = train_data['labels']\n",
    "X_test_emotion_bert = test_data['embeddings'] \n",
    "y_test_emotion_bert = test_data['labels']\n",
    "\n",
    "# EMOTION - ELECTRA\n",
    "train_data = np.load('embeddings_google_electra-base-discriminator_emotion_train_electra-base-discriminator.npz')\n",
    "test_data = np.load('embeddings_google_electra-base-discriminator_emotion_test_electra-base-discriminator.npz')\n",
    "X_train_emotion_electra = train_data['embeddings']\n",
    "y_train_emotion_electra = train_data['labels']\n",
    "X_test_emotion_electra = test_data['embeddings'] \n",
    "y_test_emotion_electra = test_data['labels']\n",
    "\n",
    "print(\"EMOTION - Shapes:\")\n",
    "print(f\"Roberta - Train: {X_train_emotion_roberta.shape}, Test: {X_test_emotion_roberta.shape}\")\n",
    "print(f\"BERT - Train: {X_train_emotion_bert.shape}, Test: {X_test_emotion_bert.shape}\")\n",
    "print(f\"Electra - Train: {X_train_emotion_electra.shape}, Test: {X_test_emotion_electra.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "041e0275-f622-4558-96b9-e212cd904f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïê Iniciando experimento...\n",
      "üîç Testando 8 combina√ß√µes de hiperpar√¢metros...\n",
      "\n",
      "[1/8] Testando: {'lr': 0.001, 'hidden_dim1': 64, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 0.0862\n",
      "  Epoch 2 - Loss: 0.0476\n",
      "  Epoch 3 - Loss: 0.0415\n",
      "  Epoch 4 - Loss: 0.0413\n",
      "  Epoch 5 - Loss: 0.0388\n",
      "  Val Accuracy: 0.9828 | Val F1: 0.9830\n",
      "\n",
      "[2/8] Testando: {'lr': 0.001, 'hidden_dim1': 64, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 0.0997\n",
      "  Epoch 2 - Loss: 0.0581\n",
      "  Epoch 3 - Loss: 0.0492\n",
      "  Epoch 4 - Loss: 0.0481\n",
      "  Epoch 5 - Loss: 0.0472\n",
      "  Val Accuracy: 0.9819 | Val F1: 0.9819\n",
      "\n",
      "[3/8] Testando: {'lr': 0.001, 'hidden_dim1': 128, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 0.0753\n",
      "  Epoch 2 - Loss: 0.0458\n",
      "  Epoch 3 - Loss: 0.0413\n",
      "  Epoch 4 - Loss: 0.0393\n",
      "  Epoch 5 - Loss: 0.0391\n",
      "  Val Accuracy: 0.9844 | Val F1: 0.9843\n",
      "\n",
      "[4/8] Testando: {'lr': 0.001, 'hidden_dim1': 128, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 0.0867\n",
      "  Epoch 2 - Loss: 0.0528\n",
      "  Epoch 3 - Loss: 0.0463\n",
      "  Epoch 4 - Loss: 0.0439\n",
      "  Epoch 5 - Loss: 0.0432\n",
      "  Val Accuracy: 0.9797 | Val F1: 0.9792\n",
      "\n",
      "[5/8] Testando: {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 0.1013\n",
      "  Epoch 2 - Loss: 0.0449\n",
      "  Epoch 3 - Loss: 0.0395\n",
      "  Epoch 4 - Loss: 0.0387\n",
      "  Epoch 5 - Loss: 0.0372\n",
      "  Val Accuracy: 0.9853 | Val F1: 0.9852\n",
      "\n",
      "[6/8] Testando: {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 0.1142\n",
      "  Epoch 2 - Loss: 0.0523\n",
      "  Epoch 3 - Loss: 0.0502\n",
      "  Epoch 4 - Loss: 0.0466\n",
      "  Epoch 5 - Loss: 0.0445\n",
      "  Val Accuracy: 0.9806 | Val F1: 0.9803\n",
      "\n",
      "[7/8] Testando: {'lr': 0.0005, 'hidden_dim1': 128, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 0.0845\n",
      "  Epoch 2 - Loss: 0.0440\n",
      "  Epoch 3 - Loss: 0.0376\n",
      "  Epoch 4 - Loss: 0.0395\n",
      "  Epoch 5 - Loss: 0.0360\n",
      "  Val Accuracy: 0.9809 | Val F1: 0.9806\n",
      "\n",
      "[8/8] Testando: {'lr': 0.0005, 'hidden_dim1': 128, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 0.0937\n",
      "  Epoch 2 - Loss: 0.0467\n",
      "  Epoch 3 - Loss: 0.0429\n",
      "  Epoch 4 - Loss: 0.0406\n",
      "  Epoch 5 - Loss: 0.0408\n",
      "  Val Accuracy: 0.9847 | Val F1: 0.9847\n",
      "\n",
      "============================================================\n",
      "üèÜ MELHORES RESULTADOS NA VALIDA√á√ÉO:\n",
      "Melhores par√¢metros: {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.3}\n",
      "Melhor F1 na valida√ß√£o: 0.9852\n",
      "\n",
      "üìä AVALIA√á√ÉO FINAL NO TESTE:\n",
      "============================================================\n",
      "üìà RESULTADOS FINAIS:\n",
      "Acur√°cia no teste: 0.9270\n",
      "F1-Score (weighted): 0.9267\n",
      "F1-Score (macro): 0.8832\n",
      "‚è±Ô∏è  Tempo total: 9.51 segundos (0.16 minutos)\n",
      "============================================================\n",
      "\n",
      "üìã RELAT√ìRIO DETALHADO:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.97       581\n",
      "           1       0.95      0.95      0.95       695\n",
      "           2       0.82      0.81      0.82       159\n",
      "           3       0.92      0.92      0.92       275\n",
      "           4       0.90      0.88      0.89       224\n",
      "           5       0.77      0.74      0.75        66\n",
      "\n",
      "    accuracy                           0.93      2000\n",
      "   macro avg       0.89      0.88      0.88      2000\n",
      "weighted avg       0.93      0.93      0.93      2000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.927,\n",
       " 'f1_weighted': 0.926716493134764,\n",
       " 'f1_macro': 0.8832381260830179,\n",
       " 'best_params': {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.3},\n",
       " 'total_time_seconds': 9.51409387588501,\n",
       " 'total_time_minutes': 0.15856823126475017}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainNNEmb(\n",
    "    X_train_emotion_bert, X_train_emotion_roberta, X_train_emotion_electra,\n",
    "    X_test_emotion_bert, X_test_emotion_roberta, X_test_emotion_electra,\n",
    "    y_train_emotion_roberta, y_test_emotion_roberta,\n",
    "    num_classes=6,  # emotion tem 6 classes\n",
    "    val_size=0.2,\n",
    "    batch_size=32,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ddad2971-edfc-44d3-9e94-99a7f1c5b4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïê Iniciando experimento...\n",
      "üîç Testando 8 combina√ß√µes de hiperpar√¢metros...\n",
      "\n",
      "[1/8] Testando: {'lr': 0.001, 'hidden_dim1': 64, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 0.1863\n",
      "  Epoch 2 - Loss: 0.0452\n",
      "  Epoch 3 - Loss: 0.0401\n",
      "  Epoch 4 - Loss: 0.0386\n",
      "  Epoch 5 - Loss: 0.0346\n",
      "  Val Accuracy: 0.9856 | Val F1: 0.9855\n",
      "\n",
      "[2/8] Testando: {'lr': 0.001, 'hidden_dim1': 64, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 0.2432\n",
      "  Epoch 2 - Loss: 0.0586\n",
      "  Epoch 3 - Loss: 0.0483\n",
      "  Epoch 4 - Loss: 0.0435\n",
      "  Epoch 5 - Loss: 0.0436\n",
      "  Val Accuracy: 0.9853 | Val F1: 0.9852\n",
      "\n",
      "[3/8] Testando: {'lr': 0.001, 'hidden_dim1': 128, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 0.1500\n",
      "  Epoch 2 - Loss: 0.0420\n",
      "  Epoch 3 - Loss: 0.0376\n",
      "  Epoch 4 - Loss: 0.0352\n",
      "  Epoch 5 - Loss: 0.0353\n",
      "  Val Accuracy: 0.9819 | Val F1: 0.9816\n",
      "\n",
      "[4/8] Testando: {'lr': 0.001, 'hidden_dim1': 128, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 0.1632\n",
      "  Epoch 2 - Loss: 0.0458\n",
      "  Epoch 3 - Loss: 0.0409\n",
      "  Epoch 4 - Loss: 0.0394\n",
      "  Epoch 5 - Loss: 0.0374\n",
      "  Val Accuracy: 0.9809 | Val F1: 0.9807\n",
      "\n",
      "[5/8] Testando: {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 0.2913\n",
      "  Epoch 2 - Loss: 0.0489\n",
      "  Epoch 3 - Loss: 0.0416\n",
      "  Epoch 4 - Loss: 0.0384\n",
      "  Epoch 5 - Loss: 0.0366\n",
      "  Val Accuracy: 0.9841 | Val F1: 0.9839\n",
      "\n",
      "[6/8] Testando: {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 0.3316\n",
      "  Epoch 2 - Loss: 0.0593\n",
      "  Epoch 3 - Loss: 0.0545\n",
      "  Epoch 4 - Loss: 0.0486\n",
      "  Epoch 5 - Loss: 0.0428\n",
      "  Val Accuracy: 0.9841 | Val F1: 0.9840\n",
      "\n",
      "[7/8] Testando: {'lr': 0.0005, 'hidden_dim1': 128, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 0.2349\n",
      "  Epoch 2 - Loss: 0.0426\n",
      "  Epoch 3 - Loss: 0.0386\n",
      "  Epoch 4 - Loss: 0.0371\n",
      "  Epoch 5 - Loss: 0.0331\n",
      "  Val Accuracy: 0.9834 | Val F1: 0.9833\n",
      "\n",
      "[8/8] Testando: {'lr': 0.0005, 'hidden_dim1': 128, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 0.2493\n",
      "  Epoch 2 - Loss: 0.0486\n",
      "  Epoch 3 - Loss: 0.0444\n",
      "  Epoch 4 - Loss: 0.0416\n",
      "  Epoch 5 - Loss: 0.0374\n",
      "  Val Accuracy: 0.9844 | Val F1: 0.9843\n",
      "\n",
      "============================================================\n",
      "üèÜ MELHORES RESULTADOS NA VALIDA√á√ÉO:\n",
      "Melhores par√¢metros: {'lr': 0.001, 'hidden_dim1': 64, 'dropout': 0.3}\n",
      "Melhor F1 na valida√ß√£o: 0.9855\n",
      "\n",
      "üìä AVALIA√á√ÉO FINAL NO TESTE:\n",
      "============================================================\n",
      "üìà RESULTADOS FINAIS:\n",
      "Acur√°cia no teste: 0.9265\n",
      "F1-Score (weighted): 0.9260\n",
      "F1-Score (macro): 0.8796\n",
      "‚è±Ô∏è  Tempo total: 9.38 segundos (0.16 minutos)\n",
      "============================================================\n",
      "\n",
      "üìã RELAT√ìRIO DETALHADO:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97       581\n",
      "           1       0.94      0.95      0.95       695\n",
      "           2       0.83      0.79      0.81       159\n",
      "           3       0.95      0.91      0.93       275\n",
      "           4       0.89      0.88      0.89       224\n",
      "           5       0.75      0.73      0.74        66\n",
      "\n",
      "    accuracy                           0.93      2000\n",
      "   macro avg       0.89      0.87      0.88      2000\n",
      "weighted avg       0.93      0.93      0.93      2000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9265,\n",
       " 'f1_weighted': 0.9259760525262535,\n",
       " 'f1_macro': 0.8795698926316072,\n",
       " 'best_params': {'lr': 0.001, 'hidden_dim1': 64, 'dropout': 0.3},\n",
       " 'total_time_seconds': 9.378422498703003,\n",
       " 'total_time_minutes': 0.15630704164505005}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainNNEmbL2(\n",
    "    X_train_emotion_bert, X_train_emotion_roberta, X_train_emotion_electra,\n",
    "    X_test_emotion_bert, X_test_emotion_roberta, X_test_emotion_electra,\n",
    "    y_train_emotion_roberta, y_test_emotion_roberta,\n",
    "    num_classes=6,  # emotion tem 6 classes\n",
    "    val_size=0.2,\n",
    "    batch_size=32,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c7a74ee4-cb03-4c21-9e8b-ccafffe1f06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïê Iniciando experimento...\n",
      "üîç Testando 8 combina√ß√µes de hiperpar√¢metros...\n",
      "\n",
      "[1/8] Testando: {'lr': 0.001, 'hidden_dim1': 64, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 0.0880\n",
      "  Epoch 2 - Loss: 0.0577\n",
      "  Epoch 3 - Loss: 0.0514\n",
      "  Epoch 4 - Loss: 0.0463\n",
      "  Epoch 5 - Loss: 0.0453\n",
      "  Val Accuracy: 0.9841 | Val F1: 0.9840\n",
      "\n",
      "[2/8] Testando: {'lr': 0.001, 'hidden_dim1': 64, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 0.1109\n",
      "  Epoch 2 - Loss: 0.0675\n",
      "  Epoch 3 - Loss: 0.0600\n",
      "  Epoch 4 - Loss: 0.0569\n",
      "  Epoch 5 - Loss: 0.0540\n",
      "  Val Accuracy: 0.9831 | Val F1: 0.9831\n",
      "\n",
      "[3/8] Testando: {'lr': 0.001, 'hidden_dim1': 128, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 0.0906\n",
      "  Epoch 2 - Loss: 0.0540\n",
      "  Epoch 3 - Loss: 0.0496\n",
      "  Epoch 4 - Loss: 0.0467\n",
      "  Epoch 5 - Loss: 0.0451\n",
      "  Val Accuracy: 0.9841 | Val F1: 0.9840\n",
      "\n",
      "[4/8] Testando: {'lr': 0.001, 'hidden_dim1': 128, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 0.1015\n",
      "  Epoch 2 - Loss: 0.0636\n",
      "  Epoch 3 - Loss: 0.0572\n",
      "  Epoch 4 - Loss: 0.0531\n",
      "  Epoch 5 - Loss: 0.0509\n",
      "  Val Accuracy: 0.9794 | Val F1: 0.9792\n",
      "\n",
      "[5/8] Testando: {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 0.0935\n",
      "  Epoch 2 - Loss: 0.0534\n",
      "  Epoch 3 - Loss: 0.0475\n",
      "  Epoch 4 - Loss: 0.0450\n",
      "  Epoch 5 - Loss: 0.0437\n",
      "  Val Accuracy: 0.9841 | Val F1: 0.9840\n",
      "\n",
      "[6/8] Testando: {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 0.1147\n",
      "  Epoch 2 - Loss: 0.0626\n",
      "  Epoch 3 - Loss: 0.0571\n",
      "  Epoch 4 - Loss: 0.0551\n",
      "  Epoch 5 - Loss: 0.0552\n",
      "  Val Accuracy: 0.9803 | Val F1: 0.9803\n",
      "\n",
      "[7/8] Testando: {'lr': 0.0005, 'hidden_dim1': 128, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 0.0918\n",
      "  Epoch 2 - Loss: 0.0524\n",
      "  Epoch 3 - Loss: 0.0462\n",
      "  Epoch 4 - Loss: 0.0457\n",
      "  Epoch 5 - Loss: 0.0425\n",
      "  Val Accuracy: 0.9834 | Val F1: 0.9834\n",
      "\n",
      "[8/8] Testando: {'lr': 0.0005, 'hidden_dim1': 128, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 0.1038\n",
      "  Epoch 2 - Loss: 0.0597\n",
      "  Epoch 3 - Loss: 0.0525\n",
      "  Epoch 4 - Loss: 0.0511\n",
      "  Epoch 5 - Loss: 0.0500\n",
      "  Val Accuracy: 0.9812 | Val F1: 0.9809\n",
      "\n",
      "============================================================\n",
      "üèÜ MELHORES RESULTADOS NA VALIDA√á√ÉO:\n",
      "Melhores par√¢metros: {'lr': 0.001, 'hidden_dim1': 64, 'dropout': 0.3}\n",
      "Melhor F1 na valida√ß√£o: 0.9840\n",
      "\n",
      "üìä AVALIA√á√ÉO FINAL NO TESTE:\n",
      "============================================================\n",
      "üìà RESULTADOS FINAIS:\n",
      "Acur√°cia no teste: 0.9240\n",
      "F1-Score (weighted): 0.9242\n",
      "F1-Score (macro): 0.8786\n",
      "‚è±Ô∏è  Tempo total: 10.20 segundos (0.17 minutos)\n",
      "============================================================\n",
      "\n",
      "üìã RELAT√ìRIO DETALHADO:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.97       581\n",
      "           1       0.95      0.94      0.95       695\n",
      "           2       0.82      0.81      0.81       159\n",
      "           3       0.95      0.91      0.93       275\n",
      "           4       0.89      0.86      0.87       224\n",
      "           5       0.70      0.80      0.75        66\n",
      "\n",
      "    accuracy                           0.92      2000\n",
      "   macro avg       0.88      0.88      0.88      2000\n",
      "weighted avg       0.92      0.92      0.92      2000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.924,\n",
       " 'f1_weighted': 0.9241632318428544,\n",
       " 'f1_macro': 0.878649402653746,\n",
       " 'best_params': {'lr': 0.001, 'hidden_dim1': 64, 'dropout': 0.3},\n",
       " 'total_time_seconds': 10.195956945419312,\n",
       " 'total_time_minutes': 0.16993261575698854}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainNNEmbOutra(\n",
    "    X_train_emotion_bert, X_train_emotion_roberta, X_train_emotion_electra,\n",
    "    X_test_emotion_bert, X_test_emotion_roberta, X_test_emotion_electra,\n",
    "    y_train_emotion_roberta, y_test_emotion_roberta,\n",
    "    num_classes=6,  # emotion tem 6 classes\n",
    "    val_size=0.2,\n",
    "    batch_size=32,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aa610c-6613-4503-9efa-6d3340347ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
