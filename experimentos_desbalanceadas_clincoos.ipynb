{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0778f19a-abc8-4031-9dd2-4a6df32cfa9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lua/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "import itertools\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f42b94c-5d5d-4db5-a68e-a1e9e408f1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "def trainNNEmbWithVal(\n",
    "    bertTrainEmbeddings, robertaTrainEmbeddings, electraTrainEmbeddings,\n",
    "    bertValEmbeddings, robertaValEmbeddings, electraValEmbeddings,\n",
    "    bertTestEmbeddings, robertaTestEmbeddings, electraTestEmbeddings,\n",
    "    trainLabels, valLabels, testLabels,\n",
    "    num_classes,\n",
    "    batch_size=32,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "):\n",
    "    # Inicia contagem de tempo\n",
    "    start_time = time.time()\n",
    "    print(\" Iniciando experimento...\")\n",
    "    \n",
    "    # Concatena os embeddings das tr√™s redes\n",
    "    X_train = np.concatenate([bertTrainEmbeddings, robertaTrainEmbeddings, electraTrainEmbeddings], axis=1)\n",
    "    X_val = np.concatenate([bertValEmbeddings, robertaValEmbeddings, electraValEmbeddings], axis=1)\n",
    "    X_test = np.concatenate([bertTestEmbeddings, robertaTestEmbeddings, electraTestEmbeddings], axis=1)\n",
    "\n",
    "    y_train = np.array(trainLabels)\n",
    "    y_val = np.array(valLabels)\n",
    "    y_test = np.array(testLabels)\n",
    "\n",
    "    # Cria DataLoaders\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "    val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
    "    test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    param_grid = {\n",
    "        'lr': [1e-3, 5e-4],\n",
    "        'hidden_dim1': [64, 128],\n",
    "        'dropout': [0.3, 0.5]\n",
    "    }\n",
    "\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "    best_f1 = 0.0\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    print(f\" Testando {len(combinations)} combina√ß√µes de hiperpar√¢metros...\")\n",
    "\n",
    "    for i, params in enumerate(combinations, 1):\n",
    "        print(f\"\\n[{i}/{len(combinations)}] Testando: {params}\")\n",
    "        \n",
    "        model = create_model(  # Usando create_model_2\n",
    "            input_dim=X_train.shape[1],\n",
    "            hidden_dim1=params['hidden_dim1'],\n",
    "            dropout=params['dropout'],\n",
    "            num_classes=num_classes\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Treina por 5 epochs\n",
    "        for epoch in range(5):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * X_batch.size(0)\n",
    "            epoch_loss = running_loss / len(train_loader.dataset)\n",
    "            print(f\"  Epoch {epoch+1} - Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Avalia√ß√£o no conjunto de valida√ß√£o\n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        val_true_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                val_predictions.extend(predicted.cpu().numpy())\n",
    "                val_true_labels.extend(y_batch.cpu().numpy())\n",
    "        \n",
    "        # Calcula F1 na valida√ß√£o\n",
    "        val_f1 = f1_score(val_true_labels, val_predictions, average='weighted')\n",
    "        val_acc = accuracy_score(val_true_labels, val_predictions)\n",
    "        \n",
    "        print(f\"  Val Accuracy: {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_params = params\n",
    "            best_model = model.state_dict().copy()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" MELHORES RESULTADOS NA VALIDA√á√ÉO:\")\n",
    "    print(f\"Melhores par√¢metros: {best_params}\")\n",
    "    print(f\"Melhor F1 na valida√ß√£o: {best_f1:.4f}\")\n",
    "\n",
    "    # Recria o melhor modelo para avalia√ß√£o final\n",
    "    final_model = create_model(\n",
    "        input_dim=X_train.shape[1],\n",
    "        hidden_dim1=best_params['hidden_dim1'],\n",
    "        dropout=best_params['dropout'],\n",
    "        num_classes=num_classes\n",
    "    ).to(device)\n",
    "    final_model.load_state_dict(best_model)\n",
    "\n",
    "    # Avalia√ß√£o final no conjunto de teste\n",
    "    print(\"\\n AVALIA√á√ÉO FINAL NO TESTE:\")\n",
    "    final_model.eval()\n",
    "    test_predictions = []\n",
    "    test_true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = final_model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            test_predictions.extend(predicted.cpu().numpy())\n",
    "            test_true_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    # Calcula m√©tricas finais\n",
    "    test_accuracy = accuracy_score(test_true_labels, test_predictions)\n",
    "    test_f1_weighted = f1_score(test_true_labels, test_predictions, average='weighted')\n",
    "    test_f1_macro = f1_score(test_true_labels, test_predictions, average='macro')\n",
    "    \n",
    "    # Tempo total\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\" RESULTADOS FINAIS:\")\n",
    "    print(f\"Acur√°cia no teste: {test_accuracy:.4f}\")\n",
    "    print(f\"F1-Score (weighted): {test_f1_weighted:.4f}\")\n",
    "    print(f\"F1-Score (macro): {test_f1_macro:.4f}\")\n",
    "    print(f\"‚è±Ô∏è  Tempo total: {total_time:.2f} segundos ({total_time/60:.2f} minutos)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\n RELAT√ìRIO DETALHADO:\")\n",
    "    print(classification_report(test_true_labels, test_predictions))\n",
    "    \n",
    "    return {\n",
    "        'accuracy': test_accuracy,\n",
    "        'f1_weighted': test_f1_weighted,\n",
    "        'f1_macro': test_f1_macro,\n",
    "        'best_params': best_params,\n",
    "        'total_time_seconds': total_time,\n",
    "        'total_time_minutes': total_time/60\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e20f9a3-86fd-4a69-846c-8d91c776a75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "def trainNNEmbOutraWithVal(\n",
    "    bertTrainEmbeddings, robertaTrainEmbeddings, electraTrainEmbeddings,\n",
    "    bertValEmbeddings, robertaValEmbeddings, electraValEmbeddings,\n",
    "    bertTestEmbeddings, robertaTestEmbeddings, electraTestEmbeddings,\n",
    "    trainLabels, valLabels, testLabels,\n",
    "    num_classes,\n",
    "    batch_size=32,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "):\n",
    "    # Inicia contagem de tempo\n",
    "    start_time = time.time()\n",
    "    print(\" Iniciando experimento...\")\n",
    "    \n",
    "    # Concatena os embeddings das tr√™s redes\n",
    "    X_train = np.concatenate([bertTrainEmbeddings, robertaTrainEmbeddings, electraTrainEmbeddings], axis=1)\n",
    "    X_val = np.concatenate([bertValEmbeddings, robertaValEmbeddings, electraValEmbeddings], axis=1)\n",
    "    X_test = np.concatenate([bertTestEmbeddings, robertaTestEmbeddings, electraTestEmbeddings], axis=1)\n",
    "\n",
    "    y_train = np.array(trainLabels)\n",
    "    y_val = np.array(valLabels)\n",
    "    y_test = np.array(testLabels)\n",
    "\n",
    "    # Cria DataLoaders\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "    val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
    "    test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    param_grid = {\n",
    "        'lr': [1e-3, 5e-4],\n",
    "        'hidden_dim1': [64, 128],\n",
    "        'dropout': [0.3, 0.5]\n",
    "    }\n",
    "\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "    best_f1 = 0.0\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    print(f\" Testando {len(combinations)} combina√ß√µes de hiperpar√¢metros...\")\n",
    "\n",
    "    for i, params in enumerate(combinations, 1):\n",
    "        print(f\"\\n[{i}/{len(combinations)}] Testando: {params}\")\n",
    "        \n",
    "        model = create_model_2(  # Usando create_model_2\n",
    "            input_dim=X_train.shape[1],\n",
    "            hidden_dim1=params['hidden_dim1'],\n",
    "            dropout=params['dropout'],\n",
    "            num_classes=num_classes\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Treina por 5 epochs\n",
    "        for epoch in range(5):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * X_batch.size(0)\n",
    "            epoch_loss = running_loss / len(train_loader.dataset)\n",
    "            print(f\"  Epoch {epoch+1} - Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Avalia√ß√£o no conjunto de valida√ß√£o\n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        val_true_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                val_predictions.extend(predicted.cpu().numpy())\n",
    "                val_true_labels.extend(y_batch.cpu().numpy())\n",
    "        \n",
    "        # Calcula F1 na valida√ß√£o\n",
    "        val_f1 = f1_score(val_true_labels, val_predictions, average='weighted')\n",
    "        val_acc = accuracy_score(val_true_labels, val_predictions)\n",
    "        \n",
    "        print(f\"  Val Accuracy: {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_params = params\n",
    "            best_model = model.state_dict().copy()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" MELHORES RESULTADOS NA VALIDA√á√ÉO:\")\n",
    "    print(f\"Melhores par√¢metros: {best_params}\")\n",
    "    print(f\"Melhor F1 na valida√ß√£o: {best_f1:.4f}\")\n",
    "\n",
    "    # Recria o melhor modelo para avalia√ß√£o final\n",
    "    final_model = create_model_2(\n",
    "        input_dim=X_train.shape[1],\n",
    "        hidden_dim1=best_params['hidden_dim1'],\n",
    "        dropout=best_params['dropout'],\n",
    "        num_classes=num_classes\n",
    "    ).to(device)\n",
    "    final_model.load_state_dict(best_model)\n",
    "\n",
    "    # Avalia√ß√£o final no conjunto de teste\n",
    "    print(\"\\n AVALIA√á√ÉO FINAL NO TESTE:\")\n",
    "    final_model.eval()\n",
    "    test_predictions = []\n",
    "    test_true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = final_model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            test_predictions.extend(predicted.cpu().numpy())\n",
    "            test_true_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    # Calcula m√©tricas finais\n",
    "    test_accuracy = accuracy_score(test_true_labels, test_predictions)\n",
    "    test_f1_weighted = f1_score(test_true_labels, test_predictions, average='weighted')\n",
    "    test_f1_macro = f1_score(test_true_labels, test_predictions, average='macro')\n",
    "    \n",
    "    # Tempo total\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\" RESULTADOS FINAIS:\")\n",
    "    print(f\"Acur√°cia no teste: {test_accuracy:.4f}\")\n",
    "    print(f\"F1-Score (weighted): {test_f1_weighted:.4f}\")\n",
    "    print(f\"F1-Score (macro): {test_f1_macro:.4f}\")\n",
    "    print(f\"‚è±Ô∏è  Tempo total: {total_time:.2f} segundos ({total_time/60:.2f} minutos)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\n RELAT√ìRIO DETALHADO:\")\n",
    "    print(classification_report(test_true_labels, test_predictions))\n",
    "    \n",
    "    return {\n",
    "        'accuracy': test_accuracy,\n",
    "        'f1_weighted': test_f1_weighted,\n",
    "        'f1_macro': test_f1_macro,\n",
    "        'best_params': best_params,\n",
    "        'total_time_seconds': total_time,\n",
    "        'total_time_minutes': total_time/60\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7a85655-b5eb-40b7-9ca9-3708ead8f88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in ./venv/lib/python3.11/site-packages (3.0.5)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.11/site-packages (from xgboost) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in ./venv/lib/python3.11/site-packages (from xgboost) (2.26.2)\n",
      "Requirement already satisfied: scipy in ./venv/lib/python3.11/site-packages (from xgboost) (1.15.3)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9c37bdd-d3d4-41d1-9335-6e514594ca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def create_model(input_dim, hidden_dim1, dropout, num_classes=2):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, hidden_dim1),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(hidden_dim1, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, num_classes)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e994828-70ab-4314-afea-b15bcc45c469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "def trainXGBoostOnly(\n",
    "    bertTrainLogits, robertaTrainLogits, electraTrainLogits,\n",
    "    bertTestLogits, robertaTestLogits, electraTestLogits,\n",
    "    trainLabels, testLabels,\n",
    "    num_classes,\n",
    "    val_size=0.2,\n",
    "    random_state=42\n",
    "):\n",
    "    # Inicia contagem de tempo\n",
    "    start_time = time.time()\n",
    "    print(\"üïê Iniciando experimento XGBoost...\")\n",
    "    \n",
    "    # Concatena os logits das tr√™s redes\n",
    "    concatenated_logits_train = np.concatenate(\n",
    "        [bertTrainLogits['logits'], robertaTrainLogits['logits'], electraTrainLogits['logits']], axis=1\n",
    "    )\n",
    "    concatenated_logits_test = np.concatenate(\n",
    "        [bertTestLogits['logits'], robertaTestLogits['logits'], electraTestLogits['logits']], axis=1\n",
    "    )\n",
    "    \n",
    "    train_labels = np.array(trainLabels)\n",
    "    test_labels = np.array(testLabels)\n",
    "    \n",
    "    print(f\"üìä Shape dos dados concatenados: {concatenated_logits_train.shape}\")\n",
    "    print(f\"üìä N√∫mero de classes: {num_classes}\")\n",
    "    \n",
    "    # Dividir treino em treino e valida√ß√£o (estratificado)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        concatenated_logits_train,\n",
    "        train_labels,\n",
    "        test_size=val_size,\n",
    "        stratify=train_labels,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Define o classificador XGBoost (ajuste o objective conforme n√∫mero de classes)\n",
    "    if num_classes == 2:\n",
    "        objective = 'binary:logistic'\n",
    "        eval_metric = 'logloss'\n",
    "        scoring_metric = 'f1'\n",
    "    else:\n",
    "        objective = 'multi:softprob'\n",
    "        eval_metric = 'mlogloss'\n",
    "        scoring_metric = 'f1_weighted'  # Corrigido para multiclasse\n",
    "    \n",
    "    xgb_clf = xgb.XGBClassifier(\n",
    "        tree_method='gpu_hist',\n",
    "        gpu_id=0,\n",
    "        objective=objective,\n",
    "        eval_metric=eval_metric,\n",
    "        use_label_encoder=False,\n",
    "        num_class=num_classes if num_classes > 2 else None,\n",
    "        random_state=random_state,\n",
    "        verbosity=0  # Reduzido para menos verbose\n",
    "    )\n",
    "    \n",
    "    # Grade de hiperpar√¢metros para busca\n",
    "    param_grid_xgb = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 4, 6],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0]\n",
    "    }\n",
    "    \n",
    "    total_combinations = np.prod([len(v) for v in param_grid_xgb.values()])\n",
    "    print(f\"üîç Testando {total_combinations} combina√ß√µes de hiperpar√¢metros com CV=3...\")\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=xgb_clf,\n",
    "        param_grid=param_grid_xgb,\n",
    "        scoring=scoring_metric,\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    print(\"‚öôÔ∏è  Rodando GridSearch para XGBoost...\")\n",
    "    grid_start_time = time.time()\n",
    "    \n",
    "    # Usa apenas treino para GridSearch (valida√ß√£o fica separada para avalia√ß√£o)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    grid_end_time = time.time()\n",
    "    print(f\"‚úÖ GridSearch conclu√≠do em {grid_end_time - grid_start_time:.2f} segundos\")\n",
    "    \n",
    "    best_xgb_model = grid_search.best_estimator_\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üèÜ MELHORES HIPERPAR√ÇMETROS:\")\n",
    "    print(grid_search.best_params_)\n",
    "    print(f\"üéØ Melhor score no CV: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Avalia√ß√£o no conjunto de valida√ß√£o\n",
    "    print(\"\\nüìä AVALIA√á√ÉO NO CONJUNTO DE VALIDA√á√ÉO:\")\n",
    "    val_pred = best_xgb_model.predict(X_val)\n",
    "    val_accuracy = accuracy_score(y_val, val_pred)\n",
    "    val_f1_weighted = f1_score(y_val, val_pred, average='weighted')\n",
    "    val_f1_macro = f1_score(y_val, val_pred, average='macro')\n",
    "    \n",
    "    print(f\"Valida√ß√£o - Acur√°cia: {val_accuracy:.4f}\")\n",
    "    print(f\"Valida√ß√£o - F1 Weighted: {val_f1_weighted:.4f}\")\n",
    "    print(f\"Valida√ß√£o - F1 Macro: {val_f1_macro:.4f}\")\n",
    "    \n",
    "    # Retreina o modelo com treino + valida√ß√£o para avalia√ß√£o final\n",
    "    print(\"\\nüîÑ Retreinando modelo final com treino + valida√ß√£o...\")\n",
    "    X_trainval = np.concatenate([X_train, X_val], axis=0)\n",
    "    y_trainval = np.concatenate([y_train, y_val], axis=0)\n",
    "    \n",
    "    final_model = xgb.XGBClassifier(**grid_search.best_params_,\n",
    "                                   objective=objective,\n",
    "                                   eval_metric=eval_metric,\n",
    "                                   use_label_encoder=False,\n",
    "                                   num_class=num_classes if num_classes > 2 else None,\n",
    "                                   random_state=random_state,\n",
    "                                   verbosity=0)\n",
    "    \n",
    "    final_model.fit(X_trainval, y_trainval)\n",
    "    \n",
    "    # Avalia√ß√£o final no conjunto de teste\n",
    "    print(\"\\nüìà AVALIA√á√ÉO FINAL NO TESTE:\")\n",
    "    y_pred = final_model.predict(concatenated_logits_test)\n",
    "    \n",
    "    # Calcula m√©tricas finais\n",
    "    test_accuracy = accuracy_score(test_labels, y_pred)\n",
    "    test_f1_weighted = f1_score(test_labels, y_pred, average='weighted')\n",
    "    test_f1_macro = f1_score(test_labels, y_pred, average='macro')\n",
    "    \n",
    "    # Tempo total\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"üìà RESULTADOS FINAIS:\")\n",
    "    print(f\"Acur√°cia no teste: {test_accuracy:.4f}\")\n",
    "    print(f\"F1-Score (weighted): {test_f1_weighted:.4f}\")\n",
    "    print(f\"F1-Score (macro): {test_f1_macro:.4f}\")\n",
    "    print(f\"‚è±Ô∏è  Tempo total: {total_time:.2f} segundos ({total_time/60:.2f} minutos)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nüìã RELAT√ìRIO DETALHADO:\")\n",
    "    print(classification_report(test_labels, y_pred))\n",
    "    \n",
    "    # Informa√ß√µes adicionais do modelo\n",
    "    if hasattr(final_model, 'feature_importances_'):\n",
    "        print(f\"\\nüîç Top 5 features mais importantes:\")\n",
    "        feature_importance = final_model.feature_importances_\n",
    "        top_features = np.argsort(feature_importance)[-5:][::-1]\n",
    "        for i, feat_idx in enumerate(top_features, 1):\n",
    "            print(f\"  {i}. Feature {feat_idx}: {feature_importance[feat_idx]:.4f}\")\n",
    "    \n",
    "    # Retorna as m√©tricas principais\n",
    "    return {\n",
    "        'model': final_model,\n",
    "        'accuracy': test_accuracy,\n",
    "        'f1_weighted': test_f1_weighted,\n",
    "        'f1_macro': test_f1_macro,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_cv_score': grid_search.best_score_,\n",
    "        'total_time_seconds': total_time,\n",
    "        'total_time_minutes': total_time/60,\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'val_f1_weighted': val_f1_weighted,\n",
    "        'val_f1_macro': val_f1_macro\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dc8305e-157c-48ab-bad3-5ea938de0dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "def trainNNLogits(\n",
    "    bertTrainLogits, robertaTrainLogits, electraTrainLogits,\n",
    "    bertTestLogits, robertaTestLogits, electraTestLogits,\n",
    "    trainLabels, testLabels,\n",
    "    num_classes,\n",
    "    val_size=0.2,\n",
    "    batch_size=32,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "):\n",
    "    # Inicia contagem de tempo\n",
    "    start_time = time.time()\n",
    "    print(\"üïê Iniciando experimento NN com Logits...\")\n",
    "    \n",
    "    # Concatena os logits das tr√™s redes\n",
    "    concatenated_logits = np.concatenate(\n",
    "        [bertTrainLogits['logits'], robertaTrainLogits['logits'], electraTrainLogits['logits']], axis=1\n",
    "    )\n",
    "    concatenated_test_logits = np.concatenate(\n",
    "        [bertTestLogits['logits'], robertaTestLogits['logits'], electraTestLogits['logits']], axis=1\n",
    "    )\n",
    "\n",
    "    train_labels = np.array(trainLabels)\n",
    "    test_labels = np.array(testLabels)\n",
    "    \n",
    "    print(f\"üìä Shape dos logits concatenados: {concatenated_logits.shape}\")\n",
    "    print(f\"üìä N√∫mero de classes: {num_classes}\")\n",
    "\n",
    "    # Split treino/val a partir do conjunto de treino\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        concatenated_logits,\n",
    "        train_labels,\n",
    "        test_size=val_size,\n",
    "        stratify=train_labels,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Cria DataLoaders\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "    val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
    "    test_dataset = TensorDataset(torch.tensor(concatenated_test_logits, dtype=torch.float32), torch.tensor(test_labels, dtype=torch.long))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    param_grid = {\n",
    "        'lr': [1e-3, 5e-4],\n",
    "        'hidden_dim1': [64, 128],\n",
    "        'dropout': [0.3, 0.5]\n",
    "    }\n",
    "\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "    best_f1 = 0.0  # Mudan√ßa: agora otimiza pelo F1 ao inv√©s da loss\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    print(f\"üîç Testando {len(combinations)} combina√ß√µes de hiperpar√¢metros...\")\n",
    "\n",
    "    for i, params in enumerate(combinations, 1):\n",
    "        print(f\"\\n[{i}/{len(combinations)}] Testando: {params}\")\n",
    "        \n",
    "        model = create_model(\n",
    "            input_dim=X_train.shape[1],\n",
    "            hidden_dim1=params['hidden_dim1'],\n",
    "            dropout=params['dropout'],\n",
    "            num_classes=num_classes\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Treina por 5 epochs\n",
    "        for epoch in range(5):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * X_batch.size(0)\n",
    "            epoch_loss = running_loss / len(train_loader.dataset)\n",
    "            print(f\"  Epoch {epoch+1} - Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Avalia√ß√£o no conjunto de valida√ß√£o (agora com F1)\n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        val_true_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                val_predictions.extend(predicted.cpu().numpy())\n",
    "                val_true_labels.extend(y_batch.cpu().numpy())\n",
    "        \n",
    "        # Calcula F1 na valida√ß√£o\n",
    "        val_f1 = f1_score(val_true_labels, val_predictions, average='weighted')\n",
    "        val_acc = accuracy_score(val_true_labels, val_predictions)\n",
    "        \n",
    "        print(f\"  Val Accuracy: {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_params = params\n",
    "            best_model = model.state_dict().copy()  # Salva o estado do modelo\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üèÜ MELHORES RESULTADOS NA VALIDA√á√ÉO:\")\n",
    "    print(f\"Melhores par√¢metros: {best_params}\")\n",
    "    print(f\"Melhor F1 na valida√ß√£o: {best_f1:.4f}\")\n",
    "\n",
    "    # Recria o melhor modelo para avalia√ß√£o final\n",
    "    final_model = create_model(\n",
    "        input_dim=X_train.shape[1],\n",
    "        hidden_dim1=best_params['hidden_dim1'],\n",
    "        dropout=best_params['dropout'],\n",
    "        num_classes=num_classes\n",
    "    ).to(device)\n",
    "    final_model.load_state_dict(best_model)\n",
    "\n",
    "    # Avalia√ß√£o final no conjunto de teste\n",
    "    print(\"\\nüìä AVALIA√á√ÉO FINAL NO TESTE:\")\n",
    "    final_model.eval()\n",
    "    test_predictions = []\n",
    "    test_true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = final_model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            test_predictions.extend(predicted.cpu().numpy())\n",
    "            test_true_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    # Calcula m√©tricas finais\n",
    "    test_accuracy = accuracy_score(test_true_labels, test_predictions)\n",
    "    test_f1_weighted = f1_score(test_true_labels, test_predictions, average='weighted')\n",
    "    test_f1_macro = f1_score(test_true_labels, test_predictions, average='macro')\n",
    "    \n",
    "    # Tempo total\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"üìà RESULTADOS FINAIS:\")\n",
    "    print(f\"Acur√°cia no teste: {test_accuracy:.4f}\")\n",
    "    print(f\"F1-Score (weighted): {test_f1_weighted:.4f}\")\n",
    "    print(f\"F1-Score (macro): {test_f1_macro:.4f}\")\n",
    "    print(f\"‚è±Ô∏è  Tempo total: {total_time:.2f} segundos ({total_time/60:.2f} minutos)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nüìã RELAT√ìRIO DETALHADO:\")\n",
    "    print(classification_report(test_true_labels, test_predictions))\n",
    "    \n",
    "    # Retorna as m√©tricas principais\n",
    "    return {\n",
    "        'accuracy': test_accuracy,\n",
    "        'f1_weighted': test_f1_weighted,\n",
    "        'f1_macro': test_f1_macro,\n",
    "        'best_params': best_params,\n",
    "        'total_time_seconds': total_time,\n",
    "        'total_time_minutes': total_time/60\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec6f940-fc60-4ca4-b5c9-5b7cc7bd7834",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2bac7e2-4f1e-46ca-9c03-4c18784f2c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "def trainNNEmb(\n",
    "    bertTrainEmbeddings, robertaTrainEmbeddings, electraTrainEmbeddings,\n",
    "    bertTestEmbeddings, robertaTestEmbeddings, electraTestEmbeddings,\n",
    "    trainLabels, testLabels,\n",
    "    num_classes,\n",
    "    val_size=0.2,\n",
    "    batch_size=32,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "):\n",
    "    # Inicia contagem de tempo\n",
    "    start_time = time.time()\n",
    "    print(\"üïê Iniciando experimento...\")\n",
    "    \n",
    "    # Concatena os logits das tr√™s redes\n",
    "    concatenated_logits = np.concatenate(\n",
    "        [bertTrainEmbeddings, robertaTrainEmbeddings, electraTrainEmbeddings], axis=1\n",
    "    )\n",
    "    concatenated_test_logits = np.concatenate(\n",
    "        [bertTestEmbeddings, robertaTestEmbeddings, electraTestEmbeddings], axis=1\n",
    "    )\n",
    "\n",
    "    train_labels = np.array(trainLabels)\n",
    "    test_labels = np.array(testLabels)\n",
    "\n",
    "    # Split treino/val a partir do conjunto de treino\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        concatenated_logits,\n",
    "        train_labels,\n",
    "        test_size=val_size,\n",
    "        stratify=train_labels,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Cria DataLoaders\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "    val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
    "    test_dataset = TensorDataset(torch.tensor(concatenated_test_logits, dtype=torch.float32), torch.tensor(test_labels, dtype=torch.long))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    param_grid = {\n",
    "        'lr': [1e-3, 5e-4],\n",
    "        'hidden_dim1': [64, 128],\n",
    "        'dropout': [0.3, 0.5]\n",
    "    }\n",
    "\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "    best_f1 = 0.0  # Mudan√ßa: agora otimiza pelo F1 ao inv√©s da loss\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    print(f\"üîç Testando {len(combinations)} combina√ß√µes de hiperpar√¢metros...\")\n",
    "\n",
    "    for i, params in enumerate(combinations, 1):\n",
    "        print(f\"\\n[{i}/{len(combinations)}] Testando: {params}\")\n",
    "        \n",
    "        model = create_model(\n",
    "            input_dim=X_train.shape[1],\n",
    "            hidden_dim1=params['hidden_dim1'],\n",
    "            dropout=params['dropout'],\n",
    "            num_classes=num_classes\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Treina por 5 epochs\n",
    "        for epoch in range(5):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * X_batch.size(0)\n",
    "            epoch_loss = running_loss / len(train_loader.dataset)\n",
    "            print(f\"  Epoch {epoch+1} - Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Avalia√ß√£o no conjunto de valida√ß√£o (agora com F1)\n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        val_true_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                val_predictions.extend(predicted.cpu().numpy())\n",
    "                val_true_labels.extend(y_batch.cpu().numpy())\n",
    "        \n",
    "        # Calcula F1 na valida√ß√£o\n",
    "        val_f1 = f1_score(val_true_labels, val_predictions, average='weighted')\n",
    "        val_acc = accuracy_score(val_true_labels, val_predictions)\n",
    "        \n",
    "        print(f\"  Val Accuracy: {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_params = params\n",
    "            best_model = model.state_dict().copy()  # Salva o estado do modelo\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üèÜ MELHORES RESULTADOS NA VALIDA√á√ÉO:\")\n",
    "    print(f\"Melhores par√¢metros: {best_params}\")\n",
    "    print(f\"Melhor F1 na valida√ß√£o: {best_f1:.4f}\")\n",
    "\n",
    "    # Recria o melhor modelo para avalia√ß√£o final\n",
    "    final_model = create_model(\n",
    "        input_dim=X_train.shape[1],\n",
    "        hidden_dim1=best_params['hidden_dim1'],\n",
    "        dropout=best_params['dropout'],\n",
    "        num_classes=num_classes\n",
    "    ).to(device)\n",
    "    final_model.load_state_dict(best_model)\n",
    "\n",
    "    # Avalia√ß√£o final no conjunto de teste\n",
    "    print(\"\\nüìä AVALIA√á√ÉO FINAL NO TESTE:\")\n",
    "    final_model.eval()\n",
    "    test_predictions = []\n",
    "    test_true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = final_model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            test_predictions.extend(predicted.cpu().numpy())\n",
    "            test_true_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    # Calcula m√©tricas finais\n",
    "    test_accuracy = accuracy_score(test_true_labels, test_predictions)\n",
    "    test_f1_weighted = f1_score(test_true_labels, test_predictions, average='weighted')\n",
    "    test_f1_macro = f1_score(test_true_labels, test_predictions, average='macro')\n",
    "    \n",
    "    # Tempo total\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"üìà RESULTADOS FINAIS:\")\n",
    "    print(f\"Acur√°cia no teste: {test_accuracy:.4f}\")\n",
    "    print(f\"F1-Score (weighted): {test_f1_weighted:.4f}\")\n",
    "    print(f\"F1-Score (macro): {test_f1_macro:.4f}\")\n",
    "    print(f\"‚è±Ô∏è  Tempo total: {total_time:.2f} segundos ({total_time/60:.2f} minutos)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nüìã RELAT√ìRIO DETALHADO:\")\n",
    "    print(classification_report(test_true_labels, test_predictions))\n",
    "    \n",
    "    # Retorna as m√©tricas principais\n",
    "    return {\n",
    "        'accuracy': test_accuracy,\n",
    "        'f1_weighted': test_f1_weighted,\n",
    "        'f1_macro': test_f1_macro,\n",
    "        'best_params': best_params,\n",
    "        'total_time_seconds': total_time,\n",
    "        'total_time_minutes': total_time/60\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31928fa9-c57f-42c2-a7cc-e6a22c26a0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def trainNNEmbL2WithVal(\n",
    "    bertTrainEmbeddings, robertaTrainEmbeddings, electraTrainEmbeddings,\n",
    "    bertValEmbeddings, robertaValEmbeddings, electraValEmbeddings,\n",
    "    bertTestEmbeddings, robertaTestEmbeddings, electraTestEmbeddings,\n",
    "    trainLabels, valLabels, testLabels,\n",
    "    num_classes,\n",
    "    batch_size=32,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "):\n",
    "    # Inicia contagem de tempo\n",
    "    start_time = time.time()\n",
    "    print(\" Iniciando experimento com L2 normalization...\")\n",
    "    \n",
    "    # L2 normalize embeddings\n",
    "    bertTrainL2 = normalize(bertTrainEmbeddings, norm='l2', axis=1)\n",
    "    robertaTrainL2 = normalize(robertaTrainEmbeddings, norm='l2', axis=1)\n",
    "    electraTrainL2 = normalize(electraTrainEmbeddings, norm='l2', axis=1)\n",
    "\n",
    "    bertValL2 = normalize(bertValEmbeddings, norm='l2', axis=1)\n",
    "    robertaValL2 = normalize(robertaValEmbeddings, norm='l2', axis=1)\n",
    "    electraValL2 = normalize(electraValEmbeddings, norm='l2', axis=1)\n",
    "\n",
    "    bertTestL2 = normalize(bertTestEmbeddings, norm='l2', axis=1)\n",
    "    robertaTestL2 = normalize(robertaTestEmbeddings, norm='l2', axis=1)\n",
    "    electraTestL2 = normalize(electraTestEmbeddings, norm='l2', axis=1)\n",
    "    \n",
    "    # Concatena os embeddings normalizados\n",
    "    X_train = np.concatenate([bertTrainL2, robertaTrainL2, electraTrainL2], axis=1)\n",
    "    X_val = np.concatenate([bertValL2, robertaValL2, electraValL2], axis=1)\n",
    "    X_test = np.concatenate([bertTestL2, robertaTestL2, electraTestL2], axis=1)\n",
    "\n",
    "    y_train = np.array(trainLabels)\n",
    "    y_val = np.array(valLabels)\n",
    "    y_test = np.array(testLabels)\n",
    "\n",
    "    # Cria DataLoaders\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "    val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
    "    test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    param_grid = {\n",
    "        'lr': [1e-3, 5e-4],\n",
    "        'hidden_dim1': [64, 128],\n",
    "        'dropout': [0.3, 0.5]\n",
    "    }\n",
    "\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "    best_f1 = 0.0\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    print(f\" Testando {len(combinations)} combina√ß√µes de hiperpar√¢metros...\")\n",
    "\n",
    "    for i, params in enumerate(combinations, 1):\n",
    "        print(f\"\\n[{i}/{len(combinations)}] Testando: {params}\")\n",
    "        \n",
    "        model = create_model(  # Usando create_model_2\n",
    "            input_dim=X_train.shape[1],\n",
    "            hidden_dim1=params['hidden_dim1'],\n",
    "            dropout=params['dropout'],\n",
    "            num_classes=num_classes\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Treina por 5 epochs\n",
    "        for epoch in range(5):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * X_batch.size(0)\n",
    "            epoch_loss = running_loss / len(train_loader.dataset)\n",
    "            print(f\"  Epoch {epoch+1} - Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Avalia√ß√£o no conjunto de valida√ß√£o\n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        val_true_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                val_predictions.extend(predicted.cpu().numpy())\n",
    "                val_true_labels.extend(y_batch.cpu().numpy())\n",
    "        \n",
    "        # Calcula F1 na valida√ß√£o\n",
    "        val_f1 = f1_score(val_true_labels, val_predictions, average='weighted')\n",
    "        val_acc = accuracy_score(val_true_labels, val_predictions)\n",
    "        \n",
    "        print(f\"  Val Accuracy: {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_params = params\n",
    "            best_model = model.state_dict().copy()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\" MELHORES RESULTADOS NA VALIDA√á√ÉO:\")\n",
    "    print(f\"Melhores par√¢metros: {best_params}\")\n",
    "    print(f\"Melhor F1 na valida√ß√£o: {best_f1:.4f}\")\n",
    "\n",
    "    # Recria o melhor modelo para avalia√ß√£o final\n",
    "    final_model = create_model(\n",
    "        input_dim=X_train.shape[1],\n",
    "        hidden_dim1=best_params['hidden_dim1'],\n",
    "        dropout=best_params['dropout'],\n",
    "        num_classes=num_classes\n",
    "    ).to(device)\n",
    "    final_model.load_state_dict(best_model)\n",
    "\n",
    "    # Avalia√ß√£o final no conjunto de teste\n",
    "    print(\"\\n AVALIA√á√ÉO FINAL NO TESTE:\")\n",
    "    final_model.eval()\n",
    "    test_predictions = []\n",
    "    test_true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = final_model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            test_predictions.extend(predicted.cpu().numpy())\n",
    "            test_true_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    # Calcula m√©tricas finais\n",
    "    test_accuracy = accuracy_score(test_true_labels, test_predictions)\n",
    "    test_f1_weighted = f1_score(test_true_labels, test_predictions, average='weighted')\n",
    "    test_f1_macro = f1_score(test_true_labels, test_predictions, average='macro')\n",
    "    \n",
    "    # Tempo total\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\" RESULTADOS FINAIS (L2 Normalized):\")\n",
    "    print(f\"Acur√°cia no teste: {test_accuracy:.4f}\")\n",
    "    print(f\"F1-Score (weighted): {test_f1_weighted:.4f}\")\n",
    "    print(f\"F1-Score (macro): {test_f1_macro:.4f}\")\n",
    "    print(f\"‚è±Ô∏è  Tempo total: {total_time:.2f} segundos ({total_time/60:.2f} minutos)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\n RELAT√ìRIO DETALHADO:\")\n",
    "    print(classification_report(test_true_labels, test_predictions))\n",
    "    \n",
    "    return {\n",
    "        'accuracy': test_accuracy,\n",
    "        'f1_weighted': test_f1_weighted,\n",
    "        'f1_macro': test_f1_macro,\n",
    "        'best_params': best_params,\n",
    "        'total_time_seconds': total_time,\n",
    "        'total_time_minutes': total_time/60\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca1a22be-8670-4cfb-be0c-7a6f524895a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def trainNNEmbL2(\n",
    "    bertTrainEmbeddings, robertaTrainEmbeddings, electraTrainEmbeddings,\n",
    "    bertTestEmbeddings, robertaTestEmbeddings, electraTestEmbeddings,\n",
    "    trainLabels, testLabels,\n",
    "    num_classes,\n",
    "    val_size=0.2,\n",
    "    batch_size=32,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "):\n",
    "    # Inicia contagem de tempo\n",
    "    start_time = time.time()\n",
    "    print(\"üïê Iniciando experimento...\")\n",
    "    bertTrainL2 = normalize(bertTrainEmbeddings, norm='l2', axis=1)\n",
    "    robertaTrainL2 = normalize(robertaTrainEmbeddings, norm='l2', axis=1)\n",
    "    electraTrainL2 = normalize(electraTrainEmbeddings, norm='l2', axis=1)\n",
    "\n",
    "    bertTestL2 = normalize(bertTestEmbeddings, norm='l2', axis=1)\n",
    "    robertaTestL2 = normalize(robertaTestEmbeddings, norm='l2', axis=1)\n",
    "    electraTestL2 = normalize(electraTestEmbeddings, norm='l2', axis=1)\n",
    "    # Concatena os logits das tr√™s redes\n",
    "    concatenated_logits = np.concatenate(\n",
    "        [bertTrainL2, robertaTrainL2, electraTrainL2], axis=1\n",
    "    )\n",
    "    concatenated_test_logits = np.concatenate(\n",
    "        [bertTestL2, robertaTestL2, electraTestL2], axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "    train_labels = np.array(trainLabels)\n",
    "    test_labels = np.array(testLabels)\n",
    "\n",
    "    # Split treino/val a partir do conjunto de treino\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        concatenated_logits,\n",
    "        train_labels,\n",
    "        test_size=val_size,\n",
    "        stratify=train_labels,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Cria DataLoaders\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "    val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
    "    test_dataset = TensorDataset(torch.tensor(concatenated_test_logits, dtype=torch.float32), torch.tensor(test_labels, dtype=torch.long))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    param_grid = {\n",
    "        'lr': [1e-3, 5e-4],\n",
    "        'hidden_dim1': [64, 128],\n",
    "        'dropout': [0.3, 0.5]\n",
    "    }\n",
    "\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "    best_f1 = 0.0  # Mudan√ßa: agora otimiza pelo F1 ao inv√©s da loss\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    print(f\"üîç Testando {len(combinations)} combina√ß√µes de hiperpar√¢metros...\")\n",
    "\n",
    "    for i, params in enumerate(combinations, 1):\n",
    "        print(f\"\\n[{i}/{len(combinations)}] Testando: {params}\")\n",
    "        \n",
    "        model = create_model(\n",
    "            input_dim=X_train.shape[1],\n",
    "            hidden_dim1=params['hidden_dim1'],\n",
    "            dropout=params['dropout'],\n",
    "            num_classes=num_classes\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Treina por 5 epochs\n",
    "        for epoch in range(5):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * X_batch.size(0)\n",
    "            epoch_loss = running_loss / len(train_loader.dataset)\n",
    "            print(f\"  Epoch {epoch+1} - Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Avalia√ß√£o no conjunto de valida√ß√£o (agora com F1)\n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        val_true_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                val_predictions.extend(predicted.cpu().numpy())\n",
    "                val_true_labels.extend(y_batch.cpu().numpy())\n",
    "        \n",
    "        # Calcula F1 na valida√ß√£o\n",
    "        val_f1 = f1_score(val_true_labels, val_predictions, average='weighted')\n",
    "        val_acc = accuracy_score(val_true_labels, val_predictions)\n",
    "        \n",
    "        print(f\"  Val Accuracy: {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_params = params\n",
    "            best_model = model.state_dict().copy()  # Salva o estado do modelo\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üèÜ MELHORES RESULTADOS NA VALIDA√á√ÉO:\")\n",
    "    print(f\"Melhores par√¢metros: {best_params}\")\n",
    "    print(f\"Melhor F1 na valida√ß√£o: {best_f1:.4f}\")\n",
    "\n",
    "    # Recria o melhor modelo para avalia√ß√£o final\n",
    "    final_model = create_model(\n",
    "        input_dim=X_train.shape[1],\n",
    "        hidden_dim1=best_params['hidden_dim1'],\n",
    "        dropout=best_params['dropout'],\n",
    "        num_classes=num_classes\n",
    "    ).to(device)\n",
    "    final_model.load_state_dict(best_model)\n",
    "\n",
    "    # Avalia√ß√£o final no conjunto de teste\n",
    "    print(\"\\nüìä AVALIA√á√ÉO FINAL NO TESTE:\")\n",
    "    final_model.eval()\n",
    "    test_predictions = []\n",
    "    test_true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = final_model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            test_predictions.extend(predicted.cpu().numpy())\n",
    "            test_true_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    # Calcula m√©tricas finais\n",
    "    test_accuracy = accuracy_score(test_true_labels, test_predictions)\n",
    "    test_f1_weighted = f1_score(test_true_labels, test_predictions, average='weighted')\n",
    "    test_f1_macro = f1_score(test_true_labels, test_predictions, average='macro')\n",
    "    \n",
    "    # Tempo total\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"üìà RESULTADOS FINAIS:\")\n",
    "    print(f\"Acur√°cia no teste: {test_accuracy:.4f}\")\n",
    "    print(f\"F1-Score (weighted): {test_f1_weighted:.4f}\")\n",
    "    print(f\"F1-Score (macro): {test_f1_macro:.4f}\")\n",
    "    print(f\"‚è±Ô∏è  Tempo total: {total_time:.2f} segundos ({total_time/60:.2f} minutos)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nüìã RELAT√ìRIO DETALHADO:\")\n",
    "    print(classification_report(test_true_labels, test_predictions))\n",
    "    \n",
    "    # Retorna as m√©tricas principais\n",
    "    return {\n",
    "        'accuracy': test_accuracy,\n",
    "        'f1_weighted': test_f1_weighted,\n",
    "        'f1_macro': test_f1_macro,\n",
    "        'best_params': best_params,\n",
    "        'total_time_seconds': total_time,\n",
    "        'total_time_minutes': total_time/60\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17a0890c-b05d-4af5-9018-9e678f9d3a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def create_model_2(input_dim, hidden_dim1, dropout, num_classes=2):\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.norm = nn.LayerNorm(input_dim) \n",
    "            self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.fc2 = nn.Linear(hidden_dim1, 32)\n",
    "            self.fc3 = nn.Linear(32, num_classes)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.norm(x)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.dropout(x)\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "\n",
    "    return Net()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0096c96c-2887-48ed-bf3a-0f9c1dd29807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "def trainNNEmbOutra(\n",
    "    bertTrainEmbeddings, robertaTrainEmbeddings, electraTrainEmbeddings,\n",
    "    bertTestEmbeddings, robertaTestEmbeddings, electraTestEmbeddings,\n",
    "    trainLabels, testLabels,\n",
    "    num_classes,\n",
    "    val_size=0.2,\n",
    "    batch_size=32,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "):\n",
    "    # Inicia contagem de tempo\n",
    "    start_time = time.time()\n",
    "    print(\"üïê Iniciando experimento...\")\n",
    "    \n",
    "    # Concatena os logits das tr√™s redes\n",
    "    concatenated_logits = np.concatenate(\n",
    "        [bertTrainEmbeddings, robertaTrainEmbeddings, electraTrainEmbeddings], axis=1\n",
    "    )\n",
    "    concatenated_test_logits = np.concatenate(\n",
    "        [bertTestEmbeddings, robertaTestEmbeddings, electraTestEmbeddings], axis=1\n",
    "    )\n",
    "\n",
    "    train_labels = np.array(trainLabels)\n",
    "    test_labels = np.array(testLabels)\n",
    "\n",
    "    # Split treino/val a partir do conjunto de treino\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        concatenated_logits,\n",
    "        train_labels,\n",
    "        test_size=val_size,\n",
    "        stratify=train_labels,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Cria DataLoaders\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "    val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
    "    test_dataset = TensorDataset(torch.tensor(concatenated_test_logits, dtype=torch.float32), torch.tensor(test_labels, dtype=torch.long))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    param_grid = {\n",
    "        'lr': [1e-3, 5e-4],\n",
    "        'hidden_dim1': [64, 128],\n",
    "        'dropout': [0.3, 0.5]\n",
    "    }\n",
    "\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "    best_f1 = 0.0  # Mudan√ßa: agora otimiza pelo F1 ao inv√©s da loss\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    print(f\"üîç Testando {len(combinations)} combina√ß√µes de hiperpar√¢metros...\")\n",
    "\n",
    "    for i, params in enumerate(combinations, 1):\n",
    "        print(f\"\\n[{i}/{len(combinations)}] Testando: {params}\")\n",
    "        \n",
    "        model = create_model_2(\n",
    "            input_dim=X_train.shape[1],\n",
    "            hidden_dim1=params['hidden_dim1'],\n",
    "            dropout=params['dropout'],\n",
    "            num_classes=num_classes\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Treina por 5 epochs\n",
    "        for epoch in range(5):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * X_batch.size(0)\n",
    "            epoch_loss = running_loss / len(train_loader.dataset)\n",
    "            print(f\"  Epoch {epoch+1} - Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Avalia√ß√£o no conjunto de valida√ß√£o (agora com F1)\n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        val_true_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                val_predictions.extend(predicted.cpu().numpy())\n",
    "                val_true_labels.extend(y_batch.cpu().numpy())\n",
    "        \n",
    "        # Calcula F1 na valida√ß√£o\n",
    "        val_f1 = f1_score(val_true_labels, val_predictions, average='weighted')\n",
    "        val_acc = accuracy_score(val_true_labels, val_predictions)\n",
    "        \n",
    "        print(f\"  Val Accuracy: {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_params = params\n",
    "            best_model = model.state_dict().copy()  # Salva o estado do modelo\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üèÜ MELHORES RESULTADOS NA VALIDA√á√ÉO:\")\n",
    "    print(f\"Melhores par√¢metros: {best_params}\")\n",
    "    print(f\"Melhor F1 na valida√ß√£o: {best_f1:.4f}\")\n",
    "\n",
    "    # Recria o melhor modelo para avalia√ß√£o final\n",
    "    final_model = create_model_2(\n",
    "        input_dim=X_train.shape[1],\n",
    "        hidden_dim1=best_params['hidden_dim1'],\n",
    "        dropout=best_params['dropout'],\n",
    "        num_classes=num_classes\n",
    "    ).to(device)\n",
    "    final_model.load_state_dict(best_model)\n",
    "\n",
    "    # Avalia√ß√£o final no conjunto de teste\n",
    "    print(\"\\nüìä AVALIA√á√ÉO FINAL NO TESTE:\")\n",
    "    final_model.eval()\n",
    "    test_predictions = []\n",
    "    test_true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = final_model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            test_predictions.extend(predicted.cpu().numpy())\n",
    "            test_true_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    # Calcula m√©tricas finais\n",
    "    test_accuracy = accuracy_score(test_true_labels, test_predictions)\n",
    "    test_f1_weighted = f1_score(test_true_labels, test_predictions, average='weighted')\n",
    "    test_f1_macro = f1_score(test_true_labels, test_predictions, average='macro')\n",
    "    \n",
    "    # Tempo total\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"üìà RESULTADOS FINAIS:\")\n",
    "    print(f\"Acur√°cia no teste: {test_accuracy:.4f}\")\n",
    "    print(f\"F1-Score (weighted): {test_f1_weighted:.4f}\")\n",
    "    print(f\"F1-Score (macro): {test_f1_macro:.4f}\")\n",
    "    print(f\"‚è±Ô∏è  Tempo total: {total_time:.2f} segundos ({total_time/60:.2f} minutos)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nüìã RELAT√ìRIO DETALHADO:\")\n",
    "    print(classification_report(test_true_labels, test_predictions))\n",
    "    \n",
    "    # Retorna as m√©tricas principais\n",
    "    return {\n",
    "        'accuracy': test_accuracy,\n",
    "        'f1_weighted': test_f1_weighted,\n",
    "        'f1_macro': test_f1_macro,\n",
    "        'best_params': best_params,\n",
    "        'total_time_seconds': total_time,\n",
    "        'total_time_minutes': total_time/60\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4070d447-1b28-42cd-89e5-e72250288624",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_logits_file = np.load('logits_google-bert/bert-base-uncased_clincoos_train_bert-base-uncased.npz')\n",
    "roberta_logits_file = np.load('logits_roberta-base_clincoos_train_roberta-base.npz')\n",
    "electra_logits_file = np.load('logits_google/electra-base-discriminator_clincoos_train_electra-base-discriminator.npz')\n",
    "\n",
    "\n",
    "bert_logits_test_file = np.load('logits_google-bert/bert-base-uncased_clincoos_test_bert-base-uncased.npz')\n",
    "roberta_logits_test_file = np.load('logits_roberta-base_clincoos_test_roberta-base.npz')\n",
    "electra_logits_test_file = np.load('logits_google/electra-base-discriminator_clincoos_test_electra-base-discriminator.npz')\n",
    "\n",
    "bertTrainLogits = bert_logits_file\n",
    "robertaTrainLogits = roberta_logits_file\n",
    "electraTrainLogits = electra_logits_file\n",
    "\n",
    "bertTestLogits = bert_logits_test_file\n",
    "robertaTestLogits = roberta_logits_test_file\n",
    "electraTestLogits = electra_logits_test_file\n",
    "\n",
    "trainLabels = bert_logits_file['labels']\n",
    "testLabels = bert_logits_test_file['labels']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd5ca1ef-2c92-488a-87b1-cb55d6e8fca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTADOS INDIVIDUAIS:\n",
      "BERT     - Acc: 0.8016 | F1: 0.7739\n",
      "RoBERTa  - Acc: 0.8535 | F1: 0.8390\n",
      "ELECTRA  - Acc: 0.6953 | F1: 0.6546\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# BERT\n",
    "bert_preds = np.argmax(bertTestLogits['logits'], axis=1)\n",
    "bert_acc = accuracy_score(testLabels, bert_preds)\n",
    "bert_f1 = f1_score(testLabels, bert_preds, average='weighted')\n",
    "\n",
    "# RoBERTa  \n",
    "roberta_preds = np.argmax(robertaTestLogits['logits'], axis=1)\n",
    "roberta_acc = accuracy_score(testLabels, roberta_preds)\n",
    "roberta_f1 = f1_score(testLabels, roberta_preds, average='weighted')\n",
    "\n",
    "# ELECTRA\n",
    "electra_preds = np.argmax(electraTestLogits['logits'], axis=1)\n",
    "electra_acc = accuracy_score(testLabels, electra_preds)\n",
    "electra_f1 = f1_score(testLabels, electra_preds, average='weighted')\n",
    "\n",
    "# Resultados\n",
    "print(\"RESULTADOS INDIVIDUAIS:\")\n",
    "print(f\"BERT     - Acc: {bert_acc:.4f} | F1: {bert_f1:.4f}\")\n",
    "print(f\"RoBERTa  - Acc: {roberta_acc:.4f} | F1: {roberta_f1:.4f}\")\n",
    "print(f\"ELECTRA  - Acc: {electra_acc:.4f} | F1: {electra_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12267608-78ad-4e18-8b1e-febb13e0d908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score Results:\n",
      "BERT:           0.7739\n",
      "RoBERTa:        0.8390\n",
      "ELECTRA:        0.6546\n",
      "Voto Majorit√°rio: 0.8170\n",
      "M√©dia Logits:     0.8377\n",
      "Or√°culo:          0.8710\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bert': 0.7738696822268317,\n",
       " 'roberta': 0.8390475782409581,\n",
       " 'electra': 0.6546132670301089,\n",
       " 'majority': 0.8170297821998733,\n",
       " 'avg_logits': 0.8376793880847972,\n",
       " 'oracle': 0.8710315862364507}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "def avaliar_ensemble_logits(logits_bert, logits_roberta, logits_electra, true_labels):\n",
    "    \"\"\"\n",
    "    Avalia ensemble de logits com diferentes estrat√©gias\n",
    "    \"\"\"\n",
    "    # Predi√ß√µes individuais\n",
    "    pred_bert = np.argmax(logits_bert, axis=1)\n",
    "    pred_roberta = np.argmax(logits_roberta, axis=1)\n",
    "    pred_electra = np.argmax(logits_electra, axis=1)\n",
    "    \n",
    "    # 1. F1 individual\n",
    "    f1_bert = f1_score(true_labels, pred_bert, average='weighted')\n",
    "    f1_roberta = f1_score(true_labels, pred_roberta, average='weighted')\n",
    "    f1_electra = f1_score(true_labels, pred_electra, average='weighted')\n",
    "    \n",
    "    # 2. Voto majorit√°rio\n",
    "    votes = np.column_stack([pred_bert, pred_roberta, pred_electra])\n",
    "    pred_majority = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=1, arr=votes)\n",
    "    f1_majority = f1_score(true_labels, pred_majority, average='weighted')\n",
    "    \n",
    "    # 3. M√©dia dos logits\n",
    "    logits_avg = (logits_bert + logits_roberta + logits_electra) / 3\n",
    "    pred_avg = np.argmax(logits_avg, axis=1)\n",
    "    f1_avg = f1_score(true_labels, pred_avg, average='weighted')\n",
    "    \n",
    "    # 4. Or√°culo (melhor predi√ß√£o para cada amostra)\n",
    "    all_preds = np.column_stack([pred_bert, pred_roberta, pred_electra])\n",
    "    pred_oracle = []\n",
    "    for i in range(len(true_labels)):\n",
    "        # Para cada amostra, pega a predi√ß√£o que est√° certa (se houver)\n",
    "        correct_preds = all_preds[i][all_preds[i] == true_labels[i]]\n",
    "        if len(correct_preds) > 0:\n",
    "            pred_oracle.append(correct_preds[0])\n",
    "        else:\n",
    "            # Se nenhuma est√° certa, usa voto majorit√°rio\n",
    "            pred_oracle.append(pred_majority[i])\n",
    "    \n",
    "    f1_oracle = f1_score(true_labels, pred_oracle, average='weighted')\n",
    "    \n",
    "    print(\"F1-Score Results:\")\n",
    "    print(f\"BERT:           {f1_bert:.4f}\")\n",
    "    print(f\"RoBERTa:        {f1_roberta:.4f}\")\n",
    "    print(f\"ELECTRA:        {f1_electra:.4f}\")\n",
    "    print(f\"Voto Majorit√°rio: {f1_majority:.4f}\")\n",
    "    print(f\"M√©dia Logits:     {f1_avg:.4f}\")\n",
    "    print(f\"Or√°culo:          {f1_oracle:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'bert': f1_bert,\n",
    "        'roberta': f1_roberta, \n",
    "        'electra': f1_electra,\n",
    "        'majority': f1_majority,\n",
    "        'avg_logits': f1_avg,\n",
    "        'oracle': f1_oracle\n",
    "    }\n",
    "\n",
    "# Exemplo de uso:\n",
    "avaliar_ensemble_logits(bertTestLogits['logits'], robertaTestLogits['logits'], electraTestLogits['logits'], bertTestLogits['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "528c2a2e-4a1e-4f5e-96f7-f0e723c75b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïê Iniciando experimento NN com Logits...\n",
      "üìä Shape dos logits concatenados: (10625, 453)\n",
      "üìä N√∫mero de classes: 151\n",
      "üîç Testando 8 combina√ß√µes de hiperpar√¢metros...\n",
      "\n",
      "[1/8] Testando: {'lr': 0.001, 'hidden_dim1': 64, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 2.2558\n",
      "  Epoch 2 - Loss: 0.3376\n",
      "  Epoch 3 - Loss: 0.1637\n",
      "  Epoch 4 - Loss: 0.1138\n",
      "  Epoch 5 - Loss: 0.0933\n",
      "  Val Accuracy: 0.9972 | Val F1: 0.9972\n",
      "\n",
      "[2/8] Testando: {'lr': 0.001, 'hidden_dim1': 64, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 2.7929\n",
      "  Epoch 2 - Loss: 0.7431\n",
      "  Epoch 3 - Loss: 0.4437\n",
      "  Epoch 4 - Loss: 0.3308\n",
      "  Epoch 5 - Loss: 0.2750\n",
      "  Val Accuracy: 0.9962 | Val F1: 0.9962\n",
      "\n",
      "[3/8] Testando: {'lr': 0.001, 'hidden_dim1': 128, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 1.6815\n",
      "  Epoch 2 - Loss: 0.1465\n",
      "  Epoch 3 - Loss: 0.0687\n",
      "  Epoch 4 - Loss: 0.0568\n",
      "  Epoch 5 - Loss: 0.0481\n",
      "  Val Accuracy: 0.9981 | Val F1: 0.9981\n",
      "\n",
      "[4/8] Testando: {'lr': 0.001, 'hidden_dim1': 128, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 2.2948\n",
      "  Epoch 2 - Loss: 0.3191\n",
      "  Epoch 3 - Loss: 0.1621\n",
      "  Epoch 4 - Loss: 0.1125\n",
      "  Epoch 5 - Loss: 0.0891\n",
      "  Val Accuracy: 0.9962 | Val F1: 0.9962\n",
      "\n",
      "[5/8] Testando: {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 3.2387\n",
      "  Epoch 2 - Loss: 0.7440\n",
      "  Epoch 3 - Loss: 0.3227\n",
      "  Epoch 4 - Loss: 0.1893\n",
      "  Epoch 5 - Loss: 0.1456\n",
      "  Val Accuracy: 0.9962 | Val F1: 0.9961\n",
      "\n",
      "[6/8] Testando: {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 3.7943\n",
      "  Epoch 2 - Loss: 1.4846\n",
      "  Epoch 3 - Loss: 0.7960\n",
      "  Epoch 4 - Loss: 0.5355\n",
      "  Epoch 5 - Loss: 0.4208\n",
      "  Val Accuracy: 0.9953 | Val F1: 0.9945\n",
      "\n",
      "[7/8] Testando: {'lr': 0.0005, 'hidden_dim1': 128, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 2.7139\n",
      "  Epoch 2 - Loss: 0.4169\n",
      "  Epoch 3 - Loss: 0.1557\n",
      "  Epoch 4 - Loss: 0.0942\n",
      "  Epoch 5 - Loss: 0.0660\n",
      "  Val Accuracy: 0.9976 | Val F1: 0.9977\n",
      "\n",
      "[8/8] Testando: {'lr': 0.0005, 'hidden_dim1': 128, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 3.1034\n",
      "  Epoch 2 - Loss: 0.7038\n",
      "  Epoch 3 - Loss: 0.3035\n",
      "  Epoch 4 - Loss: 0.1909\n",
      "  Epoch 5 - Loss: 0.1367\n",
      "  Val Accuracy: 0.9976 | Val F1: 0.9977\n",
      "\n",
      "============================================================\n",
      "üèÜ MELHORES RESULTADOS NA VALIDA√á√ÉO:\n",
      "Melhores par√¢metros: {'lr': 0.001, 'hidden_dim1': 128, 'dropout': 0.3}\n",
      "Melhor F1 na valida√ß√£o: 0.9981\n",
      "\n",
      "üìä AVALIA√á√ÉO FINAL NO TESTE:\n",
      "============================================================\n",
      "üìà RESULTADOS FINAIS:\n",
      "Acur√°cia no teste: 0.8542\n",
      "F1-Score (weighted): 0.8424\n",
      "F1-Score (macro): 0.8930\n",
      "‚è±Ô∏è  Tempo total: 6.27 segundos (0.10 minutos)\n",
      "============================================================\n",
      "\n",
      "üìã RELAT√ìRIO DETALHADO:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.83      0.86        30\n",
      "           1       0.65      1.00      0.79        30\n",
      "           2       0.80      0.93      0.86        30\n",
      "           3       0.74      0.97      0.84        30\n",
      "           4       0.91      1.00      0.95        30\n",
      "           5       0.86      1.00      0.92        30\n",
      "           6       0.89      0.83      0.86        30\n",
      "           7       0.85      0.97      0.91        30\n",
      "           8       0.94      1.00      0.97        30\n",
      "           9       0.75      0.90      0.82        30\n",
      "          10       0.58      1.00      0.73        30\n",
      "          11       0.63      0.97      0.76        30\n",
      "          12       0.53      0.93      0.67        30\n",
      "          13       0.91      1.00      0.95        30\n",
      "          14       0.75      1.00      0.86        30\n",
      "          15       0.88      1.00      0.94        30\n",
      "          16       0.55      0.87      0.68        30\n",
      "          17       0.75      1.00      0.86        30\n",
      "          18       0.77      1.00      0.87        30\n",
      "          19       0.93      0.90      0.92        30\n",
      "          20       0.72      0.97      0.83        30\n",
      "          21       0.81      1.00      0.90        30\n",
      "          22       0.97      0.97      0.97        30\n",
      "          23       0.78      0.93      0.85        30\n",
      "          24       1.00      1.00      1.00        30\n",
      "          25       0.76      0.93      0.84        30\n",
      "          26       0.74      0.93      0.82        30\n",
      "          27       0.83      0.80      0.81        30\n",
      "          28       0.90      0.87      0.88        30\n",
      "          29       0.77      1.00      0.87        30\n",
      "          30       1.00      1.00      1.00        30\n",
      "          31       1.00      0.93      0.97        30\n",
      "          32       0.88      0.97      0.92        30\n",
      "          33       0.97      1.00      0.98        30\n",
      "          34       0.88      1.00      0.94        30\n",
      "          35       0.96      0.87      0.91        30\n",
      "          36       0.97      1.00      0.98        30\n",
      "          37       0.88      1.00      0.94        30\n",
      "          38       0.91      1.00      0.95        30\n",
      "          39       0.97      1.00      0.98        30\n",
      "          40       0.75      1.00      0.86        30\n",
      "          41       0.83      0.80      0.81        30\n",
      "          42       0.98      0.44      0.61      1000\n",
      "          43       0.64      0.97      0.77        30\n",
      "          44       0.96      0.87      0.91        30\n",
      "          45       0.91      1.00      0.95        30\n",
      "          46       0.81      0.83      0.82        30\n",
      "          47       0.68      1.00      0.81        30\n",
      "          48       0.67      0.97      0.79        30\n",
      "          49       0.86      0.80      0.83        30\n",
      "          50       0.82      0.93      0.88        30\n",
      "          51       0.86      1.00      0.92        30\n",
      "          52       0.77      1.00      0.87        30\n",
      "          53       0.97      0.93      0.95        30\n",
      "          54       0.97      0.97      0.97        30\n",
      "          55       0.88      0.97      0.92        30\n",
      "          56       0.76      0.83      0.79        30\n",
      "          57       0.92      0.77      0.84        30\n",
      "          58       0.70      0.93      0.80        30\n",
      "          59       0.94      1.00      0.97        30\n",
      "          60       0.86      0.83      0.85        30\n",
      "          61       0.86      1.00      0.92        30\n",
      "          62       0.85      0.93      0.89        30\n",
      "          63       0.93      0.87      0.90        30\n",
      "          64       0.88      0.97      0.92        30\n",
      "          65       0.91      1.00      0.95        30\n",
      "          66       1.00      0.97      0.98        30\n",
      "          67       0.88      1.00      0.94        30\n",
      "          68       0.78      0.70      0.74        30\n",
      "          69       0.88      0.97      0.92        30\n",
      "          70       0.91      0.67      0.77        30\n",
      "          71       0.82      0.93      0.88        30\n",
      "          72       0.49      1.00      0.66        30\n",
      "          73       0.91      0.97      0.94        30\n",
      "          74       0.94      1.00      0.97        30\n",
      "          75       0.93      0.90      0.92        30\n",
      "          76       1.00      0.93      0.97        30\n",
      "          77       0.91      0.97      0.94        30\n",
      "          78       0.89      0.83      0.86        30\n",
      "          79       0.86      1.00      0.92        30\n",
      "          80       0.86      1.00      0.92        30\n",
      "          81       0.83      1.00      0.91        30\n",
      "          82       0.94      0.97      0.95        30\n",
      "          83       0.91      0.97      0.94        30\n",
      "          84       0.86      1.00      0.92        30\n",
      "          85       0.94      1.00      0.97        30\n",
      "          86       0.85      0.97      0.91        30\n",
      "          87       0.93      0.43      0.59        30\n",
      "          88       0.81      1.00      0.90        30\n",
      "          89       0.96      0.83      0.89        30\n",
      "          90       0.90      0.87      0.88        30\n",
      "          91       0.84      0.90      0.87        30\n",
      "          92       0.88      1.00      0.94        30\n",
      "          93       0.72      0.93      0.81        30\n",
      "          94       1.00      1.00      1.00        30\n",
      "          95       0.94      0.97      0.95        30\n",
      "          96       0.94      1.00      0.97        30\n",
      "          97       0.90      0.90      0.90        30\n",
      "          98       1.00      1.00      1.00        30\n",
      "          99       0.77      1.00      0.87        30\n",
      "         100       0.97      1.00      0.98        30\n",
      "         101       0.87      0.90      0.89        30\n",
      "         102       0.78      0.93      0.85        30\n",
      "         103       0.91      1.00      0.95        30\n",
      "         104       0.84      0.90      0.87        30\n",
      "         105       0.79      1.00      0.88        30\n",
      "         106       0.79      0.87      0.83        30\n",
      "         107       0.96      0.87      0.91        30\n",
      "         108       0.86      1.00      0.92        30\n",
      "         109       0.90      0.93      0.92        30\n",
      "         110       0.86      1.00      0.92        30\n",
      "         111       0.96      0.87      0.91        30\n",
      "         112       0.68      0.87      0.76        30\n",
      "         113       0.94      1.00      0.97        30\n",
      "         114       0.94      0.97      0.95        30\n",
      "         115       0.91      0.97      0.94        30\n",
      "         116       0.61      1.00      0.76        30\n",
      "         117       0.91      0.97      0.94        30\n",
      "         118       0.97      1.00      0.98        30\n",
      "         119       0.94      0.97      0.95        30\n",
      "         120       0.80      0.93      0.86        30\n",
      "         121       0.94      1.00      0.97        30\n",
      "         122       0.97      1.00      0.98        30\n",
      "         123       0.90      0.93      0.92        30\n",
      "         124       0.97      0.93      0.95        30\n",
      "         125       0.94      0.97      0.95        30\n",
      "         126       0.97      0.93      0.95        30\n",
      "         127       0.97      1.00      0.98        30\n",
      "         128       0.91      1.00      0.95        30\n",
      "         129       0.94      1.00      0.97        30\n",
      "         130       0.71      1.00      0.83        30\n",
      "         131       0.72      0.93      0.81        30\n",
      "         132       0.90      0.93      0.92        30\n",
      "         133       1.00      0.93      0.97        30\n",
      "         134       0.73      1.00      0.85        30\n",
      "         135       0.94      1.00      0.97        30\n",
      "         136       0.86      1.00      0.92        30\n",
      "         137       0.75      1.00      0.86        30\n",
      "         138       0.78      0.93      0.85        30\n",
      "         139       0.97      0.97      0.97        30\n",
      "         140       0.88      0.97      0.92        30\n",
      "         141       0.91      1.00      0.95        30\n",
      "         142       0.83      1.00      0.91        30\n",
      "         143       0.82      0.90      0.86        30\n",
      "         144       0.91      0.97      0.94        30\n",
      "         145       0.90      0.93      0.92        30\n",
      "         146       0.97      1.00      0.98        30\n",
      "         147       0.77      1.00      0.87        30\n",
      "         148       0.85      0.97      0.91        30\n",
      "         149       0.77      1.00      0.87        30\n",
      "         150       0.96      0.83      0.89        30\n",
      "\n",
      "    accuracy                           0.85      5500\n",
      "   macro avg       0.86      0.94      0.89      5500\n",
      "weighted avg       0.88      0.85      0.84      5500\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8541818181818182,\n",
       " 'f1_weighted': 0.8423885366078767,\n",
       " 'f1_macro': 0.89301984050757,\n",
       " 'best_params': {'lr': 0.001, 'hidden_dim1': 128, 'dropout': 0.3},\n",
       " 'total_time_seconds': 6.270123481750488,\n",
       " 'total_time_minutes': 0.10450205802917481}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertTrainLogits = bert_logits_file\n",
    "robertaTrainLogits = roberta_logits_file\n",
    "electraTrainLogits = electra_logits_file\n",
    "\n",
    "bertTestLogits = bert_logits_test_file\n",
    "robertaTestLogits = roberta_logits_test_file\n",
    "electraTestLogits = electra_logits_test_file\n",
    "\n",
    "trainLabels = bert_logits_file['labels']\n",
    "testLabels = bert_logits_test_file['labels']\n",
    "\n",
    "trainNNLogits(\n",
    "    bertTrainLogits, robertaTrainLogits, electraTrainLogits,\n",
    "    bertTestLogits, robertaTestLogits, electraTestLogits,\n",
    "    trainLabels, testLabels,\n",
    "    num_classes=151,\n",
    "    val_size=0.2,\n",
    "    batch_size=32,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b9acce-6ece-4ea0-8aee-d957d3b8a5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïê Iniciando experimento XGBoost...\n",
      "üìä Shape dos dados concatenados: (10625, 453)\n",
      "üìä N√∫mero de classes: 151\n",
      "üîç Testando 72 combina√ß√µes de hiperpar√¢metros com CV=3...\n",
      "‚öôÔ∏è  Rodando GridSearch para XGBoost...\n",
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n"
     ]
    }
   ],
   "source": [
    "trainXGBoostOnly( bertTrainLogits, robertaTrainLogits, electraTrainLogits,\n",
    "    bertTestLogits, robertaTestLogits, electraTestLogits,\n",
    "    trainLabels, testLabels,\n",
    "    num_classes=151,\n",
    "    val_size=0.2,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ed51c9b-973d-4391-8585-dedeac90eabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMOTION - Shapes:\n",
      "Roberta - Train: (10625, 768), Test: (5500, 768)\n",
      "BERT - Train: (10625, 768), Test: (5500, 768)\n",
      "Electra - Train: (10625, 768), Test: (5500, 768)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# EMOTION - ROBERTA\n",
    "train_data = np.load('embeddings_roberta-base_clincoos_train_roberta-base.npz')\n",
    "test_data = np.load('embeddings_roberta-base_clincoos_test_roberta-base.npz')\n",
    "X_train_emotion_roberta = train_data['embeddings']\n",
    "y_train_emotion_roberta = train_data['labels']\n",
    "X_test_emotion_roberta = test_data['embeddings'] \n",
    "y_test_emotion_roberta = test_data['labels']\n",
    "\n",
    "# EMOTION - BERT\n",
    "train_data = np.load('embeddings_google-bert_bert-base-uncased_clincoos_train_bert-base-uncased.npz')\n",
    "test_data = np.load('embeddings_google-bert_bert-base-uncased_clincoos_test_bert-base-uncased.npz')\n",
    "X_train_emotion_bert = train_data['embeddings']\n",
    "y_train_emotion_bert = train_data['labels']\n",
    "X_test_emotion_bert = test_data['embeddings'] \n",
    "y_test_emotion_bert = test_data['labels']\n",
    "\n",
    "# EMOTION - ELECTRA\n",
    "train_data = np.load('embeddings_google_electra-base-discriminator_clincoos_train_electra-base-discriminator.npz')\n",
    "test_data = np.load('embeddings_google_electra-base-discriminator_clincoos_test_electra-base-discriminator.npz')\n",
    "X_train_emotion_electra = train_data['embeddings']\n",
    "y_train_emotion_electra = train_data['labels']\n",
    "X_test_emotion_electra = test_data['embeddings'] \n",
    "y_test_emotion_electra = test_data['labels']\n",
    "\n",
    "print(\"EMOTION - Shapes:\")\n",
    "print(f\"Roberta - Train: {X_train_emotion_roberta.shape}, Test: {X_test_emotion_roberta.shape}\")\n",
    "print(f\"BERT - Train: {X_train_emotion_bert.shape}, Test: {X_test_emotion_bert.shape}\")\n",
    "print(f\"Electra - Train: {X_train_emotion_electra.shape}, Test: {X_test_emotion_electra.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa92baaf-d7dc-4a01-be3d-72bc9a3af021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLINCOOS - Shapes:\n",
      "Roberta - Train: (10625, 768), Val: (3100, 768), Test: (5500, 768)\n",
      "BERT - Train: (10625, 768), Val: (3100, 768), Test: (5500, 768)\n",
      "Electra - Train: (10625, 768), Val: (3100, 768), Test: (5500, 768)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# CLINCOOS - ROBERTA\n",
    "train_data = np.load('embeddings_roberta-base_clincoos_train_roberta-base.npz')\n",
    "val_data = np.load('embeddings_roberta-base_clincoos_val_roberta-base.npz')\n",
    "test_data = np.load('embeddings_roberta-base_clincoos_test_roberta-base.npz')\n",
    "\n",
    "X_train_clincoos_roberta = train_data['embeddings']\n",
    "y_train_clincoos_roberta = train_data['labels']\n",
    "X_val_clincoos_roberta = val_data['embeddings']\n",
    "y_val_clincoos_roberta = val_data['labels']\n",
    "X_test_clincoos_roberta = test_data['embeddings'] \n",
    "y_test_clincoos_roberta = test_data['labels']\n",
    "\n",
    "# CLINCOOS - BERT\n",
    "train_data = np.load('embeddings_google-bert_bert-base-uncased_clincoos_train_bert-base-uncased.npz')\n",
    "val_data = np.load('embeddings_google-bert_bert-base-uncased_clincoos_val_bert-base-uncased.npz')\n",
    "test_data = np.load('embeddings_google-bert_bert-base-uncased_clincoos_test_bert-base-uncased.npz')\n",
    "\n",
    "X_train_clincoos_bert = train_data['embeddings']\n",
    "y_train_clincoos_bert = train_data['labels']\n",
    "X_val_clincoos_bert = val_data['embeddings']\n",
    "y_val_clincoos_bert = val_data['labels']\n",
    "X_test_clincoos_bert = test_data['embeddings'] \n",
    "y_test_clincoos_bert = test_data['labels']\n",
    "\n",
    "# CLINCOOS - ELECTRA\n",
    "train_data = np.load('embeddings_google_electra-base-discriminator_clincoos_train_electra-base-discriminator.npz')\n",
    "val_data = np.load('embeddings_google_electra-base-discriminator_clincoos_val_electra-base-discriminator.npz')\n",
    "test_data = np.load('embeddings_google_electra-base-discriminator_clincoos_test_electra-base-discriminator.npz')\n",
    "\n",
    "X_train_clincoos_electra = train_data['embeddings']\n",
    "y_train_clincoos_electra = train_data['labels']\n",
    "X_val_clincoos_electra = val_data['embeddings']\n",
    "y_val_clincoos_electra = val_data['labels']\n",
    "X_test_clincoos_electra = test_data['embeddings'] \n",
    "y_test_clincoos_electra = test_data['labels']\n",
    "\n",
    "print(\"CLINCOOS - Shapes:\")\n",
    "print(f\"Roberta - Train: {X_train_clincoos_roberta.shape}, Val: {X_val_clincoos_roberta.shape}, Test: {X_test_clincoos_roberta.shape}\")\n",
    "print(f\"BERT - Train: {X_train_clincoos_bert.shape}, Val: {X_val_clincoos_bert.shape}, Test: {X_test_clincoos_bert.shape}\")\n",
    "print(f\"Electra - Train: {X_train_clincoos_electra.shape}, Val: {X_val_clincoos_electra.shape}, Test: {X_test_clincoos_electra.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c570f6a-d2a8-4f69-a988-d44a75518226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93cd23e0-59ba-4a01-8f08-2046a129e8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iniciando experimento...\n",
      " Testando 8 combina√ß√µes de hiperpar√¢metros...\n",
      "\n",
      "[1/8] Testando: {'lr': 0.001, 'hidden_dim1': 64, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 1.0296\n",
      "  Epoch 2 - Loss: 0.1131\n",
      "  Epoch 3 - Loss: 0.0662\n",
      "  Epoch 4 - Loss: 0.0522\n",
      "  Epoch 5 - Loss: 0.0478\n",
      "  Val Accuracy: 0.9513 | Val F1: 0.9505\n",
      "\n",
      "[2/8] Testando: {'lr': 0.001, 'hidden_dim1': 64, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 1.5458\n",
      "  Epoch 2 - Loss: 0.2918\n",
      "  Epoch 3 - Loss: 0.1929\n",
      "  Epoch 4 - Loss: 0.1672\n",
      "  Epoch 5 - Loss: 0.1609\n",
      "  Val Accuracy: 0.9513 | Val F1: 0.9500\n",
      "\n",
      "[3/8] Testando: {'lr': 0.001, 'hidden_dim1': 128, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 0.7579\n",
      "  Epoch 2 - Loss: 0.0535\n",
      "  Epoch 3 - Loss: 0.0391\n",
      "  Epoch 4 - Loss: 0.0303\n",
      "  Epoch 5 - Loss: 0.0344\n",
      "  Val Accuracy: 0.9516 | Val F1: 0.9505\n",
      "\n",
      "[4/8] Testando: {'lr': 0.001, 'hidden_dim1': 128, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 0.9741\n",
      "  Epoch 2 - Loss: 0.0933\n",
      "  Epoch 3 - Loss: 0.0742\n",
      "  Epoch 4 - Loss: 0.0587\n",
      "  Epoch 5 - Loss: 0.0535\n",
      "  Val Accuracy: 0.9503 | Val F1: 0.9487\n",
      "\n",
      "[5/8] Testando: {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 1.6014\n",
      "  Epoch 2 - Loss: 0.1816\n",
      "  Epoch 3 - Loss: 0.0879\n",
      "  Epoch 4 - Loss: 0.0612\n",
      "  Epoch 5 - Loss: 0.0447\n",
      "  Val Accuracy: 0.9513 | Val F1: 0.9503\n",
      "\n",
      "[6/8] Testando: {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 2.1245\n",
      "  Epoch 2 - Loss: 0.4589\n",
      "  Epoch 3 - Loss: 0.2774\n",
      "  Epoch 4 - Loss: 0.2067\n",
      "  Epoch 5 - Loss: 0.1623\n",
      "  Val Accuracy: 0.9516 | Val F1: 0.9503\n",
      "\n",
      "[7/8] Testando: {'lr': 0.0005, 'hidden_dim1': 128, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 1.1651\n",
      "  Epoch 2 - Loss: 0.0720\n",
      "  Epoch 3 - Loss: 0.0407\n",
      "  Epoch 4 - Loss: 0.0295\n",
      "  Epoch 5 - Loss: 0.0282\n",
      "  Val Accuracy: 0.9519 | Val F1: 0.9510\n",
      "\n",
      "[8/8] Testando: {'lr': 0.0005, 'hidden_dim1': 128, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 1.4971\n",
      "  Epoch 2 - Loss: 0.1425\n",
      "  Epoch 3 - Loss: 0.0764\n",
      "  Epoch 4 - Loss: 0.0577\n",
      "  Epoch 5 - Loss: 0.0471\n",
      "  Val Accuracy: 0.9523 | Val F1: 0.9510\n",
      "\n",
      "============================================================\n",
      " MELHORES RESULTADOS NA VALIDA√á√ÉO:\n",
      "Melhores par√¢metros: {'lr': 0.0005, 'hidden_dim1': 128, 'dropout': 0.3}\n",
      "Melhor F1 na valida√ß√£o: 0.9510\n",
      "\n",
      " AVALIA√á√ÉO FINAL NO TESTE:\n",
      "============================================================\n",
      " RESULTADOS FINAIS:\n",
      "Acur√°cia no teste: 0.8751\n",
      "F1-Score (weighted): 0.8675\n",
      "F1-Score (macro): 0.9084\n",
      "‚è±Ô∏è  Tempo total: 7.86 segundos (0.13 minutos)\n",
      "============================================================\n",
      "\n",
      " RELAT√ìRIO DETALHADO:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.93      0.89        30\n",
      "           1       0.86      1.00      0.92        30\n",
      "           2       0.83      0.97      0.89        30\n",
      "           3       0.79      1.00      0.88        30\n",
      "           4       0.91      1.00      0.95        30\n",
      "           5       0.91      1.00      0.95        30\n",
      "           6       1.00      0.53      0.70        30\n",
      "           7       0.88      0.93      0.90        30\n",
      "           8       1.00      1.00      1.00        30\n",
      "           9       0.96      0.90      0.93        30\n",
      "          10       0.64      1.00      0.78        30\n",
      "          11       0.94      0.97      0.95        30\n",
      "          12       0.89      0.83      0.86        30\n",
      "          13       0.88      1.00      0.94        30\n",
      "          14       0.83      1.00      0.91        30\n",
      "          15       0.97      1.00      0.98        30\n",
      "          16       0.59      0.87      0.70        30\n",
      "          17       0.67      1.00      0.80        30\n",
      "          18       0.86      1.00      0.92        30\n",
      "          19       0.96      0.87      0.91        30\n",
      "          20       0.88      0.97      0.92        30\n",
      "          21       0.88      1.00      0.94        30\n",
      "          22       1.00      0.97      0.98        30\n",
      "          23       0.85      0.93      0.89        30\n",
      "          24       0.57      1.00      0.72        30\n",
      "          25       0.67      0.97      0.79        30\n",
      "          26       0.75      0.90      0.82        30\n",
      "          27       0.75      0.70      0.72        30\n",
      "          28       0.90      0.90      0.90        30\n",
      "          29       0.88      1.00      0.94        30\n",
      "          30       1.00      1.00      1.00        30\n",
      "          31       0.94      0.97      0.95        30\n",
      "          32       0.91      0.97      0.94        30\n",
      "          33       1.00      1.00      1.00        30\n",
      "          34       0.94      1.00      0.97        30\n",
      "          35       0.94      0.97      0.95        30\n",
      "          36       0.97      1.00      0.98        30\n",
      "          37       0.91      1.00      0.95        30\n",
      "          38       0.91      1.00      0.95        30\n",
      "          39       0.88      1.00      0.94        30\n",
      "          40       0.62      1.00      0.77        30\n",
      "          41       0.90      0.90      0.90        30\n",
      "          42       0.97      0.52      0.68      1000\n",
      "          43       0.85      0.93      0.89        30\n",
      "          44       0.97      0.97      0.97        30\n",
      "          45       1.00      1.00      1.00        30\n",
      "          46       0.89      0.83      0.86        30\n",
      "          47       0.97      0.97      0.97        30\n",
      "          48       0.83      0.97      0.89        30\n",
      "          49       1.00      0.90      0.95        30\n",
      "          50       0.68      1.00      0.81        30\n",
      "          51       0.97      1.00      0.98        30\n",
      "          52       0.79      1.00      0.88        30\n",
      "          53       0.87      0.90      0.89        30\n",
      "          54       0.86      1.00      0.92        30\n",
      "          55       0.88      1.00      0.94        30\n",
      "          56       0.80      0.80      0.80        30\n",
      "          57       0.71      0.73      0.72        30\n",
      "          58       0.82      0.90      0.86        30\n",
      "          59       0.94      1.00      0.97        30\n",
      "          60       0.96      0.83      0.89        30\n",
      "          61       0.81      1.00      0.90        30\n",
      "          62       1.00      0.90      0.95        30\n",
      "          63       0.97      0.93      0.95        30\n",
      "          64       0.85      0.97      0.91        30\n",
      "          65       0.94      1.00      0.97        30\n",
      "          66       0.91      0.97      0.94        30\n",
      "          67       0.94      1.00      0.97        30\n",
      "          68       0.85      0.73      0.79        30\n",
      "          69       0.88      1.00      0.94        30\n",
      "          70       0.95      0.70      0.81        30\n",
      "          71       0.74      0.97      0.84        30\n",
      "          72       0.73      1.00      0.85        30\n",
      "          73       0.83      0.97      0.89        30\n",
      "          74       0.97      1.00      0.98        30\n",
      "          75       0.94      0.97      0.95        30\n",
      "          76       0.97      1.00      0.98        30\n",
      "          77       0.97      0.97      0.97        30\n",
      "          78       0.91      0.97      0.94        30\n",
      "          79       0.81      1.00      0.90        30\n",
      "          80       1.00      1.00      1.00        30\n",
      "          81       0.81      1.00      0.90        30\n",
      "          82       0.88      0.97      0.92        30\n",
      "          83       0.97      1.00      0.98        30\n",
      "          84       0.76      0.97      0.85        30\n",
      "          85       0.91      1.00      0.95        30\n",
      "          86       1.00      0.97      0.98        30\n",
      "          87       0.81      1.00      0.90        30\n",
      "          88       1.00      1.00      1.00        30\n",
      "          89       1.00      0.80      0.89        30\n",
      "          90       0.93      0.83      0.88        30\n",
      "          91       0.91      0.97      0.94        30\n",
      "          92       0.86      1.00      0.92        30\n",
      "          93       0.90      0.93      0.92        30\n",
      "          94       1.00      0.93      0.97        30\n",
      "          95       0.76      0.97      0.85        30\n",
      "          96       0.70      1.00      0.82        30\n",
      "          97       0.94      0.97      0.95        30\n",
      "          98       0.91      1.00      0.95        30\n",
      "          99       0.94      1.00      0.97        30\n",
      "         100       0.79      1.00      0.88        30\n",
      "         101       0.97      0.93      0.95        30\n",
      "         102       0.96      0.90      0.93        30\n",
      "         103       0.91      1.00      0.95        30\n",
      "         104       0.66      0.97      0.78        30\n",
      "         105       0.73      1.00      0.85        30\n",
      "         106       0.84      0.90      0.87        30\n",
      "         107       0.96      0.90      0.93        30\n",
      "         108       0.86      1.00      0.92        30\n",
      "         109       0.91      0.97      0.94        30\n",
      "         110       0.94      0.97      0.95        30\n",
      "         111       0.96      0.90      0.93        30\n",
      "         112       0.87      0.90      0.89        30\n",
      "         113       1.00      1.00      1.00        30\n",
      "         114       0.94      0.97      0.95        30\n",
      "         115       0.97      0.97      0.97        30\n",
      "         116       0.94      1.00      0.97        30\n",
      "         117       0.94      0.97      0.95        30\n",
      "         118       0.97      1.00      0.98        30\n",
      "         119       0.77      1.00      0.87        30\n",
      "         120       0.80      0.93      0.86        30\n",
      "         121       0.91      1.00      0.95        30\n",
      "         122       0.97      1.00      0.98        30\n",
      "         123       0.82      0.93      0.88        30\n",
      "         124       0.97      0.97      0.97        30\n",
      "         125       0.97      0.93      0.95        30\n",
      "         126       0.78      0.97      0.87        30\n",
      "         127       1.00      1.00      1.00        30\n",
      "         128       1.00      0.93      0.97        30\n",
      "         129       0.83      1.00      0.91        30\n",
      "         130       0.70      1.00      0.82        30\n",
      "         131       0.71      0.90      0.79        30\n",
      "         132       0.87      0.90      0.89        30\n",
      "         133       0.82      0.93      0.88        30\n",
      "         134       0.79      1.00      0.88        30\n",
      "         135       0.94      1.00      0.97        30\n",
      "         136       0.81      1.00      0.90        30\n",
      "         137       0.94      1.00      0.97        30\n",
      "         138       0.68      0.90      0.77        30\n",
      "         139       0.93      0.93      0.93        30\n",
      "         140       1.00      1.00      1.00        30\n",
      "         141       0.97      1.00      0.98        30\n",
      "         142       0.59      1.00      0.74        30\n",
      "         143       0.78      0.93      0.85        30\n",
      "         144       0.97      1.00      0.98        30\n",
      "         145       0.97      0.97      0.97        30\n",
      "         146       0.97      1.00      0.98        30\n",
      "         147       0.83      1.00      0.91        30\n",
      "         148       0.90      0.90      0.90        30\n",
      "         149       0.77      1.00      0.87        30\n",
      "         150       0.93      0.87      0.90        30\n",
      "\n",
      "    accuracy                           0.88      5500\n",
      "   macro avg       0.88      0.95      0.91      5500\n",
      "weighted avg       0.90      0.88      0.87      5500\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8750909090909091,\n",
       " 'f1_weighted': 0.867529381590174,\n",
       " 'f1_macro': 0.9083993703394577,\n",
       " 'best_params': {'lr': 0.0005, 'hidden_dim1': 128, 'dropout': 0.3},\n",
       " 'total_time_seconds': 7.859890699386597,\n",
       " 'total_time_minutes': 0.13099817832310995}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainNNEmbWithVal(\n",
    "    X_train_clincoos_bert, X_train_clincoos_roberta, X_train_clincoos_electra,\n",
    "    X_val_clincoos_bert, X_val_clincoos_roberta, X_val_clincoos_electra,\n",
    "    X_test_clincoos_bert, X_test_clincoos_roberta, X_test_clincoos_electra,\n",
    "    y_train_clincoos_roberta, y_val_clincoos_roberta, y_test_clincoos_roberta,\n",
    "    num_classes=151  # ou quantas classes tem clincoos\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df29b475-5483-46f9-8829-0cd6b1b21ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iniciando experimento com L2 normalization...\n",
      " Testando 8 combina√ß√µes de hiperpar√¢metros...\n",
      "\n",
      "[1/8] Testando: {'lr': 0.001, 'hidden_dim1': 64, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 2.7473\n",
      "  Epoch 2 - Loss: 0.3667\n",
      "  Epoch 3 - Loss: 0.1512\n",
      "  Epoch 4 - Loss: 0.0979\n",
      "  Epoch 5 - Loss: 0.0749\n",
      "  Val Accuracy: 0.9461 | Val F1: 0.9443\n",
      "\n",
      "[2/8] Testando: {'lr': 0.001, 'hidden_dim1': 64, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 3.1484\n",
      "  Epoch 2 - Loss: 0.8146\n",
      "  Epoch 3 - Loss: 0.4551\n",
      "  Epoch 4 - Loss: 0.3140\n",
      "  Epoch 5 - Loss: 0.2556\n",
      "  Val Accuracy: 0.9423 | Val F1: 0.9403\n",
      "\n",
      "[3/8] Testando: {'lr': 0.001, 'hidden_dim1': 128, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 2.1384\n",
      "  Epoch 2 - Loss: 0.1739\n",
      "  Epoch 3 - Loss: 0.0632\n",
      "  Epoch 4 - Loss: 0.0404\n",
      "  Epoch 5 - Loss: 0.0349\n",
      "  Val Accuracy: 0.9487 | Val F1: 0.9480\n",
      "\n",
      "[4/8] Testando: {'lr': 0.001, 'hidden_dim1': 128, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 2.4619\n",
      "  Epoch 2 - Loss: 0.3684\n",
      "  Epoch 3 - Loss: 0.1401\n",
      "  Epoch 4 - Loss: 0.0889\n",
      "  Epoch 5 - Loss: 0.0677\n",
      "  Val Accuracy: 0.9468 | Val F1: 0.9451\n",
      "\n",
      "[5/8] Testando: {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 3.8529\n",
      "  Epoch 2 - Loss: 1.2121\n",
      "  Epoch 3 - Loss: 0.5234\n",
      "  Epoch 4 - Loss: 0.3003\n",
      "  Epoch 5 - Loss: 0.1930\n",
      "  Val Accuracy: 0.9303 | Val F1: 0.9262\n",
      "\n",
      "[6/8] Testando: {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 4.1280\n",
      "  Epoch 2 - Loss: 1.8568\n",
      "  Epoch 3 - Loss: 0.9893\n",
      "  Epoch 4 - Loss: 0.6855\n",
      "  Epoch 5 - Loss: 0.5067\n",
      "  Val Accuracy: 0.9132 | Val F1: 0.9062\n",
      "\n",
      "[7/8] Testando: {'lr': 0.0005, 'hidden_dim1': 128, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 3.3379\n",
      "  Epoch 2 - Loss: 0.6225\n",
      "  Epoch 3 - Loss: 0.2184\n",
      "  Epoch 4 - Loss: 0.1004\n",
      "  Epoch 5 - Loss: 0.0636\n",
      "  Val Accuracy: 0.9465 | Val F1: 0.9453\n",
      "\n",
      "[8/8] Testando: {'lr': 0.0005, 'hidden_dim1': 128, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 3.6431\n",
      "  Epoch 2 - Loss: 0.9635\n",
      "  Epoch 3 - Loss: 0.4314\n",
      "  Epoch 4 - Loss: 0.2261\n",
      "  Epoch 5 - Loss: 0.1411\n",
      "  Val Accuracy: 0.9432 | Val F1: 0.9413\n",
      "\n",
      "============================================================\n",
      " MELHORES RESULTADOS NA VALIDA√á√ÉO:\n",
      "Melhores par√¢metros: {'lr': 0.001, 'hidden_dim1': 128, 'dropout': 0.3}\n",
      "Melhor F1 na valida√ß√£o: 0.9480\n",
      "\n",
      " AVALIA√á√ÉO FINAL NO TESTE:\n",
      "============================================================\n",
      " RESULTADOS FINAIS (L2 Normalized):\n",
      "Acur√°cia no teste: 0.8829\n",
      "F1-Score (weighted): 0.8781\n",
      "F1-Score (macro): 0.9133\n",
      "‚è±Ô∏è  Tempo total: 8.05 segundos (0.13 minutos)\n",
      "============================================================\n",
      "\n",
      " RELAT√ìRIO DETALHADO:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.87      0.88        30\n",
      "           1       0.83      1.00      0.91        30\n",
      "           2       0.87      0.90      0.89        30\n",
      "           3       0.83      1.00      0.91        30\n",
      "           4       0.97      0.97      0.97        30\n",
      "           5       0.86      1.00      0.92        30\n",
      "           6       1.00      0.80      0.89        30\n",
      "           7       0.85      0.93      0.89        30\n",
      "           8       1.00      1.00      1.00        30\n",
      "           9       0.82      0.90      0.86        30\n",
      "          10       0.52      1.00      0.68        30\n",
      "          11       0.94      0.97      0.95        30\n",
      "          12       1.00      0.83      0.91        30\n",
      "          13       0.94      0.97      0.95        30\n",
      "          14       0.81      1.00      0.90        30\n",
      "          15       0.97      1.00      0.98        30\n",
      "          16       0.51      0.87      0.64        30\n",
      "          17       0.81      1.00      0.90        30\n",
      "          18       0.86      1.00      0.92        30\n",
      "          19       0.90      0.90      0.90        30\n",
      "          20       0.81      0.97      0.88        30\n",
      "          21       0.94      1.00      0.97        30\n",
      "          22       0.88      0.97      0.92        30\n",
      "          23       0.85      0.93      0.89        30\n",
      "          24       0.97      1.00      0.98        30\n",
      "          25       0.76      0.97      0.85        30\n",
      "          26       0.73      0.90      0.81        30\n",
      "          27       0.85      0.73      0.79        30\n",
      "          28       0.90      0.93      0.92        30\n",
      "          29       0.86      1.00      0.92        30\n",
      "          30       1.00      1.00      1.00        30\n",
      "          31       0.93      0.90      0.92        30\n",
      "          32       0.91      0.97      0.94        30\n",
      "          33       1.00      1.00      1.00        30\n",
      "          34       0.94      1.00      0.97        30\n",
      "          35       0.97      0.93      0.95        30\n",
      "          36       0.97      1.00      0.98        30\n",
      "          37       0.91      1.00      0.95        30\n",
      "          38       0.91      1.00      0.95        30\n",
      "          39       0.97      1.00      0.98        30\n",
      "          40       0.75      1.00      0.86        30\n",
      "          41       0.81      0.97      0.88        30\n",
      "          42       0.97      0.56      0.71      1000\n",
      "          43       0.62      0.93      0.75        30\n",
      "          44       1.00      0.93      0.97        30\n",
      "          45       0.97      1.00      0.98        30\n",
      "          46       0.86      0.83      0.85        30\n",
      "          47       0.97      1.00      0.98        30\n",
      "          48       0.97      1.00      0.98        30\n",
      "          49       0.69      0.90      0.78        30\n",
      "          50       0.79      1.00      0.88        30\n",
      "          51       0.94      1.00      0.97        30\n",
      "          52       0.75      1.00      0.86        30\n",
      "          53       0.61      0.93      0.74        30\n",
      "          54       0.88      0.97      0.92        30\n",
      "          55       0.94      1.00      0.97        30\n",
      "          56       0.85      0.73      0.79        30\n",
      "          57       0.92      0.73      0.81        30\n",
      "          58       0.78      0.93      0.85        30\n",
      "          59       0.94      1.00      0.97        30\n",
      "          60       0.89      0.80      0.84        30\n",
      "          61       0.82      0.93      0.88        30\n",
      "          62       1.00      0.90      0.95        30\n",
      "          63       0.97      0.97      0.97        30\n",
      "          64       0.76      0.97      0.85        30\n",
      "          65       0.97      1.00      0.98        30\n",
      "          66       1.00      0.97      0.98        30\n",
      "          67       0.91      1.00      0.95        30\n",
      "          68       0.88      0.73      0.80        30\n",
      "          69       1.00      0.97      0.98        30\n",
      "          70       0.96      0.77      0.85        30\n",
      "          71       0.79      0.90      0.84        30\n",
      "          72       0.62      1.00      0.77        30\n",
      "          73       0.81      1.00      0.90        30\n",
      "          74       0.81      1.00      0.90        30\n",
      "          75       0.78      0.93      0.85        30\n",
      "          76       1.00      1.00      1.00        30\n",
      "          77       1.00      0.97      0.98        30\n",
      "          78       0.78      0.93      0.85        30\n",
      "          79       0.94      1.00      0.97        30\n",
      "          80       1.00      1.00      1.00        30\n",
      "          81       0.97      1.00      0.98        30\n",
      "          82       0.91      0.97      0.94        30\n",
      "          83       0.94      0.97      0.95        30\n",
      "          84       0.83      0.97      0.89        30\n",
      "          85       1.00      1.00      1.00        30\n",
      "          86       0.85      0.97      0.91        30\n",
      "          87       0.93      0.87      0.90        30\n",
      "          88       0.94      1.00      0.97        30\n",
      "          89       0.87      0.87      0.87        30\n",
      "          90       0.83      0.83      0.83        30\n",
      "          91       0.63      0.97      0.76        30\n",
      "          92       1.00      1.00      1.00        30\n",
      "          93       0.93      0.93      0.93        30\n",
      "          94       0.97      1.00      0.98        30\n",
      "          95       0.94      0.97      0.95        30\n",
      "          96       0.81      1.00      0.90        30\n",
      "          97       0.93      0.90      0.92        30\n",
      "          98       0.97      1.00      0.98        30\n",
      "          99       0.91      1.00      0.95        30\n",
      "         100       1.00      1.00      1.00        30\n",
      "         101       0.96      0.90      0.93        30\n",
      "         102       1.00      0.90      0.95        30\n",
      "         103       0.91      0.97      0.94        30\n",
      "         104       0.72      0.93      0.81        30\n",
      "         105       0.83      0.97      0.89        30\n",
      "         106       0.88      0.97      0.92        30\n",
      "         107       0.96      0.90      0.93        30\n",
      "         108       0.77      1.00      0.87        30\n",
      "         109       0.94      0.97      0.95        30\n",
      "         110       0.88      1.00      0.94        30\n",
      "         111       0.93      0.87      0.90        30\n",
      "         112       0.90      0.90      0.90        30\n",
      "         113       0.88      1.00      0.94        30\n",
      "         114       0.97      0.93      0.95        30\n",
      "         115       1.00      0.97      0.98        30\n",
      "         116       0.94      1.00      0.97        30\n",
      "         117       0.97      0.97      0.97        30\n",
      "         118       1.00      1.00      1.00        30\n",
      "         119       0.86      1.00      0.92        30\n",
      "         120       0.88      0.93      0.90        30\n",
      "         121       0.91      1.00      0.95        30\n",
      "         122       0.97      1.00      0.98        30\n",
      "         123       0.82      0.93      0.88        30\n",
      "         124       0.85      0.97      0.91        30\n",
      "         125       0.94      0.97      0.95        30\n",
      "         126       0.76      0.97      0.85        30\n",
      "         127       1.00      1.00      1.00        30\n",
      "         128       0.97      0.97      0.97        30\n",
      "         129       0.94      1.00      0.97        30\n",
      "         130       0.71      1.00      0.83        30\n",
      "         131       0.82      0.90      0.86        30\n",
      "         132       0.96      0.87      0.91        30\n",
      "         133       0.91      0.97      0.94        30\n",
      "         134       0.88      1.00      0.94        30\n",
      "         135       0.88      1.00      0.94        30\n",
      "         136       0.83      1.00      0.91        30\n",
      "         137       1.00      0.97      0.98        30\n",
      "         138       0.80      0.93      0.86        30\n",
      "         139       0.94      1.00      0.97        30\n",
      "         140       0.86      1.00      0.92        30\n",
      "         141       0.97      1.00      0.98        30\n",
      "         142       0.62      1.00      0.77        30\n",
      "         143       0.82      0.93      0.88        30\n",
      "         144       0.97      1.00      0.98        30\n",
      "         145       1.00      0.97      0.98        30\n",
      "         146       1.00      1.00      1.00        30\n",
      "         147       0.94      1.00      0.97        30\n",
      "         148       0.96      0.90      0.93        30\n",
      "         149       0.77      1.00      0.87        30\n",
      "         150       0.90      0.87      0.88        30\n",
      "\n",
      "    accuracy                           0.88      5500\n",
      "   macro avg       0.89      0.95      0.91      5500\n",
      "weighted avg       0.90      0.88      0.88      5500\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8829090909090909,\n",
       " 'f1_weighted': 0.8780685521374432,\n",
       " 'f1_macro': 0.9133131844504395,\n",
       " 'best_params': {'lr': 0.001, 'hidden_dim1': 128, 'dropout': 0.3},\n",
       " 'total_time_seconds': 8.05136251449585,\n",
       " 'total_time_minutes': 0.1341893752415975}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainNNEmbL2WithVal(\n",
    "    X_train_clincoos_bert, X_train_clincoos_roberta, X_train_clincoos_electra,\n",
    "    X_val_clincoos_bert, X_val_clincoos_roberta, X_val_clincoos_electra,\n",
    "    X_test_clincoos_bert, X_test_clincoos_roberta, X_test_clincoos_electra,\n",
    "    y_train_clincoos_roberta, y_val_clincoos_roberta, y_test_clincoos_roberta,\n",
    "    num_classes=151\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2880e2fe-16db-43ee-9c05-c3bef212188c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iniciando experimento...\n",
      " Testando 8 combina√ß√µes de hiperpar√¢metros...\n",
      "\n",
      "[1/8] Testando: {'lr': 0.001, 'hidden_dim1': 64, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 0.9216\n",
      "  Epoch 2 - Loss: 0.0856\n",
      "  Epoch 3 - Loss: 0.0648\n",
      "  Epoch 4 - Loss: 0.0480\n",
      "  Epoch 5 - Loss: 0.0550\n",
      "  Val Accuracy: 0.9490 | Val F1: 0.9479\n",
      "\n",
      "[2/8] Testando: {'lr': 0.001, 'hidden_dim1': 64, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 1.3596\n",
      "  Epoch 2 - Loss: 0.2962\n",
      "  Epoch 3 - Loss: 0.2197\n",
      "  Epoch 4 - Loss: 0.1920\n",
      "  Epoch 5 - Loss: 0.1569\n",
      "  Val Accuracy: 0.9481 | Val F1: 0.9465\n",
      "\n",
      "[3/8] Testando: {'lr': 0.001, 'hidden_dim1': 128, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 0.6831\n",
      "  Epoch 2 - Loss: 0.0530\n",
      "  Epoch 3 - Loss: 0.0442\n",
      "  Epoch 4 - Loss: 0.0377\n",
      "  Epoch 5 - Loss: 0.0374\n",
      "  Val Accuracy: 0.9429 | Val F1: 0.9393\n",
      "\n",
      "[4/8] Testando: {'lr': 0.001, 'hidden_dim1': 128, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 0.9889\n",
      "  Epoch 2 - Loss: 0.1050\n",
      "  Epoch 3 - Loss: 0.0671\n",
      "  Epoch 4 - Loss: 0.0746\n",
      "  Epoch 5 - Loss: 0.0578\n",
      "  Val Accuracy: 0.9526 | Val F1: 0.9513\n",
      "\n",
      "[5/8] Testando: {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 1.3834\n",
      "  Epoch 2 - Loss: 0.1386\n",
      "  Epoch 3 - Loss: 0.0758\n",
      "  Epoch 4 - Loss: 0.0574\n",
      "  Epoch 5 - Loss: 0.0465\n",
      "  Val Accuracy: 0.9526 | Val F1: 0.9514\n",
      "\n",
      "[6/8] Testando: {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 1.9608\n",
      "  Epoch 2 - Loss: 0.4109\n",
      "  Epoch 3 - Loss: 0.2410\n",
      "  Epoch 4 - Loss: 0.1887\n",
      "  Epoch 5 - Loss: 0.1548\n",
      "  Val Accuracy: 0.9494 | Val F1: 0.9479\n",
      "\n",
      "[7/8] Testando: {'lr': 0.0005, 'hidden_dim1': 128, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 1.0528\n",
      "  Epoch 2 - Loss: 0.0554\n",
      "  Epoch 3 - Loss: 0.0381\n",
      "  Epoch 4 - Loss: 0.0291\n",
      "  Epoch 5 - Loss: 0.0277\n",
      "  Val Accuracy: 0.9513 | Val F1: 0.9503\n",
      "\n",
      "[8/8] Testando: {'lr': 0.0005, 'hidden_dim1': 128, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 1.3276\n",
      "  Epoch 2 - Loss: 0.1328\n",
      "  Epoch 3 - Loss: 0.0791\n",
      "  Epoch 4 - Loss: 0.0571\n",
      "  Epoch 5 - Loss: 0.0478\n",
      "  Val Accuracy: 0.9526 | Val F1: 0.9514\n",
      "\n",
      "============================================================\n",
      " MELHORES RESULTADOS NA VALIDA√á√ÉO:\n",
      "Melhores par√¢metros: {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.3}\n",
      "Melhor F1 na valida√ß√£o: 0.9514\n",
      "\n",
      " AVALIA√á√ÉO FINAL NO TESTE:\n",
      "============================================================\n",
      " RESULTADOS FINAIS:\n",
      "Acur√°cia no teste: 0.8736\n",
      "F1-Score (weighted): 0.8667\n",
      "F1-Score (macro): 0.9076\n",
      "‚è±Ô∏è  Tempo total: 8.41 segundos (0.14 minutos)\n",
      "============================================================\n",
      "\n",
      " RELAT√ìRIO DETALHADO:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90        30\n",
      "           1       0.62      1.00      0.77        30\n",
      "           2       0.90      0.90      0.90        30\n",
      "           3       0.73      1.00      0.85        30\n",
      "           4       0.91      1.00      0.95        30\n",
      "           5       0.83      1.00      0.91        30\n",
      "           6       0.90      0.93      0.92        30\n",
      "           7       0.83      0.97      0.89        30\n",
      "           8       1.00      1.00      1.00        30\n",
      "           9       0.93      0.90      0.92        30\n",
      "          10       0.57      1.00      0.72        30\n",
      "          11       0.94      0.97      0.95        30\n",
      "          12       0.96      0.80      0.87        30\n",
      "          13       0.94      1.00      0.97        30\n",
      "          14       0.54      1.00      0.70        30\n",
      "          15       0.97      1.00      0.98        30\n",
      "          16       0.49      0.87      0.63        30\n",
      "          17       0.79      1.00      0.88        30\n",
      "          18       0.81      1.00      0.90        30\n",
      "          19       0.68      0.87      0.76        30\n",
      "          20       0.78      0.97      0.87        30\n",
      "          21       0.88      1.00      0.94        30\n",
      "          22       1.00      0.97      0.98        30\n",
      "          23       0.85      0.93      0.89        30\n",
      "          24       0.86      1.00      0.92        30\n",
      "          25       0.62      0.97      0.75        30\n",
      "          26       0.69      0.90      0.78        30\n",
      "          27       0.82      0.77      0.79        30\n",
      "          28       0.90      0.90      0.90        30\n",
      "          29       0.88      1.00      0.94        30\n",
      "          30       1.00      1.00      1.00        30\n",
      "          31       0.97      0.93      0.95        30\n",
      "          32       0.88      0.97      0.92        30\n",
      "          33       0.97      1.00      0.98        30\n",
      "          34       0.88      1.00      0.94        30\n",
      "          35       0.97      0.97      0.97        30\n",
      "          36       0.97      1.00      0.98        30\n",
      "          37       0.77      1.00      0.87        30\n",
      "          38       0.94      1.00      0.97        30\n",
      "          39       0.94      1.00      0.97        30\n",
      "          40       0.79      1.00      0.88        30\n",
      "          41       0.84      0.90      0.87        30\n",
      "          42       0.98      0.51      0.68      1000\n",
      "          43       0.70      0.93      0.80        30\n",
      "          44       0.88      1.00      0.94        30\n",
      "          45       0.97      1.00      0.98        30\n",
      "          46       0.89      0.83      0.86        30\n",
      "          47       0.73      1.00      0.85        30\n",
      "          48       0.94      0.97      0.95        30\n",
      "          49       0.84      0.87      0.85        30\n",
      "          50       0.93      0.93      0.93        30\n",
      "          51       0.97      1.00      0.98        30\n",
      "          52       0.81      1.00      0.90        30\n",
      "          53       0.93      0.90      0.92        30\n",
      "          54       0.83      0.97      0.89        30\n",
      "          55       0.77      1.00      0.87        30\n",
      "          56       0.79      0.90      0.84        30\n",
      "          57       0.86      0.83      0.85        30\n",
      "          58       0.65      0.93      0.77        30\n",
      "          59       0.94      1.00      0.97        30\n",
      "          60       0.77      0.80      0.79        30\n",
      "          61       0.86      1.00      0.92        30\n",
      "          62       0.97      0.93      0.95        30\n",
      "          63       0.90      0.93      0.92        30\n",
      "          64       0.90      0.93      0.92        30\n",
      "          65       0.88      1.00      0.94        30\n",
      "          66       1.00      0.97      0.98        30\n",
      "          67       0.91      1.00      0.95        30\n",
      "          68       0.88      0.77      0.82        30\n",
      "          69       0.97      1.00      0.98        30\n",
      "          70       0.96      0.77      0.85        30\n",
      "          71       0.81      1.00      0.90        30\n",
      "          72       0.70      1.00      0.82        30\n",
      "          73       0.85      0.97      0.91        30\n",
      "          74       0.94      1.00      0.97        30\n",
      "          75       0.97      0.93      0.95        30\n",
      "          76       0.97      1.00      0.98        30\n",
      "          77       0.81      0.97      0.88        30\n",
      "          78       0.97      0.93      0.95        30\n",
      "          79       0.91      0.97      0.94        30\n",
      "          80       0.97      1.00      0.98        30\n",
      "          81       0.94      1.00      0.97        30\n",
      "          82       0.88      0.97      0.92        30\n",
      "          83       0.97      0.97      0.97        30\n",
      "          84       0.83      0.97      0.89        30\n",
      "          85       1.00      1.00      1.00        30\n",
      "          86       0.97      0.97      0.97        30\n",
      "          87       0.85      0.97      0.91        30\n",
      "          88       0.97      1.00      0.98        30\n",
      "          89       0.89      0.80      0.84        30\n",
      "          90       0.96      0.73      0.83        30\n",
      "          91       0.76      0.93      0.84        30\n",
      "          92       1.00      1.00      1.00        30\n",
      "          93       0.93      0.93      0.93        30\n",
      "          94       1.00      0.93      0.97        30\n",
      "          95       0.94      0.97      0.95        30\n",
      "          96       0.81      1.00      0.90        30\n",
      "          97       1.00      0.93      0.97        30\n",
      "          98       0.97      1.00      0.98        30\n",
      "          99       0.91      1.00      0.95        30\n",
      "         100       1.00      1.00      1.00        30\n",
      "         101       0.96      0.90      0.93        30\n",
      "         102       0.96      0.90      0.93        30\n",
      "         103       0.97      0.97      0.97        30\n",
      "         104       0.91      0.67      0.77        30\n",
      "         105       0.86      1.00      0.92        30\n",
      "         106       0.88      0.93      0.90        30\n",
      "         107       0.96      0.83      0.89        30\n",
      "         108       0.81      1.00      0.90        30\n",
      "         109       0.88      0.97      0.92        30\n",
      "         110       0.81      1.00      0.90        30\n",
      "         111       0.93      0.87      0.90        30\n",
      "         112       0.53      0.90      0.67        30\n",
      "         113       0.91      1.00      0.95        30\n",
      "         114       0.94      0.97      0.95        30\n",
      "         115       1.00      0.97      0.98        30\n",
      "         116       0.91      1.00      0.95        30\n",
      "         117       0.94      0.97      0.95        30\n",
      "         118       0.97      1.00      0.98        30\n",
      "         119       0.83      0.97      0.89        30\n",
      "         120       0.85      0.93      0.89        30\n",
      "         121       0.86      1.00      0.92        30\n",
      "         122       0.79      1.00      0.88        30\n",
      "         123       0.93      0.93      0.93        30\n",
      "         124       0.97      0.97      0.97        30\n",
      "         125       0.97      0.93      0.95        30\n",
      "         126       0.78      0.97      0.87        30\n",
      "         127       0.94      1.00      0.97        30\n",
      "         128       0.97      1.00      0.98        30\n",
      "         129       0.81      1.00      0.90        30\n",
      "         130       0.68      1.00      0.81        30\n",
      "         131       0.79      0.90      0.84        30\n",
      "         132       0.93      0.83      0.88        30\n",
      "         133       0.97      0.97      0.97        30\n",
      "         134       0.88      1.00      0.94        30\n",
      "         135       0.94      1.00      0.97        30\n",
      "         136       0.86      1.00      0.92        30\n",
      "         137       0.94      1.00      0.97        30\n",
      "         138       0.76      0.93      0.84        30\n",
      "         139       0.94      0.97      0.95        30\n",
      "         140       0.79      1.00      0.88        30\n",
      "         141       0.94      1.00      0.97        30\n",
      "         142       0.86      1.00      0.92        30\n",
      "         143       0.88      0.93      0.90        30\n",
      "         144       1.00      1.00      1.00        30\n",
      "         145       0.94      0.97      0.95        30\n",
      "         146       0.94      0.97      0.95        30\n",
      "         147       0.88      1.00      0.94        30\n",
      "         148       0.90      0.90      0.90        30\n",
      "         149       0.71      1.00      0.83        30\n",
      "         150       0.96      0.83      0.89        30\n",
      "\n",
      "    accuracy                           0.87      5500\n",
      "   macro avg       0.88      0.95      0.91      5500\n",
      "weighted avg       0.90      0.87      0.87      5500\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8736363636363637,\n",
       " 'f1_weighted': 0.866650421439953,\n",
       " 'f1_macro': 0.9075967020670846,\n",
       " 'best_params': {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.3},\n",
       " 'total_time_seconds': 8.414437532424927,\n",
       " 'total_time_minutes': 0.14024062554041544}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainNNEmbOutraWithVal(\n",
    "    X_train_clincoos_bert, X_train_clincoos_roberta, X_train_clincoos_electra,\n",
    "    X_val_clincoos_bert, X_val_clincoos_roberta, X_val_clincoos_electra,\n",
    "    X_test_clincoos_bert, X_test_clincoos_roberta, X_test_clincoos_electra,\n",
    "    y_train_clincoos_roberta, y_val_clincoos_roberta, y_test_clincoos_roberta,\n",
    "    num_classes=151\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "041e0275-f622-4558-96b9-e212cd904f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïê Iniciando experimento...\n",
      "üîç Testando 8 combina√ß√µes de hiperpar√¢metros...\n",
      "\n",
      "[1/8] Testando: {'lr': 0.001, 'hidden_dim1': 64, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 1.2340\n",
      "  Epoch 2 - Loss: 0.1160\n",
      "  Epoch 3 - Loss: 0.0785\n",
      "  Epoch 4 - Loss: 0.0571\n",
      "  Epoch 5 - Loss: 0.0456\n",
      "  Val Accuracy: 0.9967 | Val F1: 0.9967\n",
      "\n",
      "[2/8] Testando: {'lr': 0.001, 'hidden_dim1': 64, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 1.7476\n",
      "  Epoch 2 - Loss: 0.3574\n",
      "  Epoch 3 - Loss: 0.2360\n",
      "  Epoch 4 - Loss: 0.1930\n",
      "  Epoch 5 - Loss: 0.1561\n",
      "  Val Accuracy: 0.9981 | Val F1: 0.9981\n",
      "\n",
      "[3/8] Testando: {'lr': 0.001, 'hidden_dim1': 128, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 0.9214\n",
      "  Epoch 2 - Loss: 0.0684\n",
      "  Epoch 3 - Loss: 0.0472\n",
      "  Epoch 4 - Loss: 0.0422\n",
      "  Epoch 5 - Loss: 0.0308\n",
      "  Val Accuracy: 0.9986 | Val F1: 0.9986\n",
      "\n",
      "[4/8] Testando: {'lr': 0.001, 'hidden_dim1': 128, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 1.2238\n",
      "  Epoch 2 - Loss: 0.1313\n",
      "  Epoch 3 - Loss: 0.0880\n",
      "  Epoch 4 - Loss: 0.0657\n",
      "  Epoch 5 - Loss: 0.0589\n",
      "  Val Accuracy: 0.9967 | Val F1: 0.9967\n",
      "\n",
      "[5/8] Testando: {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 1.9479\n",
      "  Epoch 2 - Loss: 0.2586\n",
      "  Epoch 3 - Loss: 0.1112\n",
      "  Epoch 4 - Loss: 0.0808\n",
      "  Epoch 5 - Loss: 0.0585\n",
      "  Val Accuracy: 0.9972 | Val F1: 0.9972\n",
      "\n",
      "[6/8] Testando: {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 2.5008\n",
      "  Epoch 2 - Loss: 0.6188\n",
      "  Epoch 3 - Loss: 0.3486\n",
      "  Epoch 4 - Loss: 0.2517\n",
      "  Epoch 5 - Loss: 0.1885\n",
      "  Val Accuracy: 0.9972 | Val F1: 0.9972\n",
      "\n",
      "[7/8] Testando: {'lr': 0.0005, 'hidden_dim1': 128, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 1.4717\n",
      "  Epoch 2 - Loss: 0.1091\n",
      "  Epoch 3 - Loss: 0.0523\n",
      "  Epoch 4 - Loss: 0.0404\n",
      "  Epoch 5 - Loss: 0.0287\n",
      "  Val Accuracy: 0.9972 | Val F1: 0.9972\n",
      "\n",
      "[8/8] Testando: {'lr': 0.0005, 'hidden_dim1': 128, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 1.8486\n",
      "  Epoch 2 - Loss: 0.2610\n",
      "  Epoch 3 - Loss: 0.1101\n",
      "  Epoch 4 - Loss: 0.0776\n",
      "  Epoch 5 - Loss: 0.0558\n",
      "  Val Accuracy: 0.9981 | Val F1: 0.9981\n",
      "\n",
      "============================================================\n",
      "üèÜ MELHORES RESULTADOS NA VALIDA√á√ÉO:\n",
      "Melhores par√¢metros: {'lr': 0.001, 'hidden_dim1': 128, 'dropout': 0.3}\n",
      "Melhor F1 na valida√ß√£o: 0.9986\n",
      "\n",
      "üìä AVALIA√á√ÉO FINAL NO TESTE:\n",
      "============================================================\n",
      "üìà RESULTADOS FINAIS:\n",
      "Acur√°cia no teste: 0.8658\n",
      "F1-Score (weighted): 0.8556\n",
      "F1-Score (macro): 0.9043\n",
      "‚è±Ô∏è  Tempo total: 6.43 segundos (0.11 minutos)\n",
      "============================================================\n",
      "\n",
      "üìã RELAT√ìRIO DETALHADO:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.90      0.87        30\n",
      "           1       0.56      1.00      0.71        30\n",
      "           2       0.88      0.97      0.92        30\n",
      "           3       0.59      1.00      0.74        30\n",
      "           4       0.91      1.00      0.95        30\n",
      "           5       0.88      1.00      0.94        30\n",
      "           6       1.00      0.90      0.95        30\n",
      "           7       0.85      0.97      0.91        30\n",
      "           8       0.97      1.00      0.98        30\n",
      "           9       0.87      0.90      0.89        30\n",
      "          10       0.73      1.00      0.85        30\n",
      "          11       0.94      0.97      0.95        30\n",
      "          12       0.84      0.90      0.87        30\n",
      "          13       1.00      1.00      1.00        30\n",
      "          14       0.62      1.00      0.77        30\n",
      "          15       0.97      1.00      0.98        30\n",
      "          16       0.60      0.87      0.71        30\n",
      "          17       0.86      1.00      0.92        30\n",
      "          18       0.83      1.00      0.91        30\n",
      "          19       0.90      0.90      0.90        30\n",
      "          20       0.88      1.00      0.94        30\n",
      "          21       0.75      1.00      0.86        30\n",
      "          22       1.00      0.97      0.98        30\n",
      "          23       0.85      0.93      0.89        30\n",
      "          24       0.70      1.00      0.82        30\n",
      "          25       0.64      0.93      0.76        30\n",
      "          26       0.68      0.90      0.77        30\n",
      "          27       0.74      0.77      0.75        30\n",
      "          28       0.90      0.87      0.88        30\n",
      "          29       0.86      1.00      0.92        30\n",
      "          30       1.00      1.00      1.00        30\n",
      "          31       0.85      0.97      0.91        30\n",
      "          32       0.88      0.97      0.92        30\n",
      "          33       1.00      1.00      1.00        30\n",
      "          34       0.81      1.00      0.90        30\n",
      "          35       0.70      1.00      0.82        30\n",
      "          36       1.00      1.00      1.00        30\n",
      "          37       0.91      1.00      0.95        30\n",
      "          38       0.91      1.00      0.95        30\n",
      "          39       0.94      1.00      0.97        30\n",
      "          40       0.56      1.00      0.71        30\n",
      "          41       0.85      0.93      0.89        30\n",
      "          42       0.99      0.46      0.63      1000\n",
      "          43       0.82      0.93      0.88        30\n",
      "          44       0.94      0.97      0.95        30\n",
      "          45       1.00      1.00      1.00        30\n",
      "          46       0.84      0.87      0.85        30\n",
      "          47       0.97      0.97      0.97        30\n",
      "          48       0.81      0.97      0.88        30\n",
      "          49       0.96      0.83      0.89        30\n",
      "          50       0.91      0.97      0.94        30\n",
      "          51       0.86      1.00      0.92        30\n",
      "          52       0.83      1.00      0.91        30\n",
      "          53       0.87      0.90      0.89        30\n",
      "          54       0.72      0.97      0.83        30\n",
      "          55       0.71      1.00      0.83        30\n",
      "          56       0.74      0.97      0.84        30\n",
      "          57       0.96      0.73      0.83        30\n",
      "          58       0.87      0.90      0.89        30\n",
      "          59       0.91      1.00      0.95        30\n",
      "          60       0.96      0.80      0.87        30\n",
      "          61       0.86      1.00      0.92        30\n",
      "          62       1.00      0.90      0.95        30\n",
      "          63       0.97      0.93      0.95        30\n",
      "          64       0.94      1.00      0.97        30\n",
      "          65       0.91      1.00      0.95        30\n",
      "          66       1.00      0.97      0.98        30\n",
      "          67       0.86      1.00      0.92        30\n",
      "          68       0.74      0.77      0.75        30\n",
      "          69       0.94      1.00      0.97        30\n",
      "          70       1.00      0.70      0.82        30\n",
      "          71       0.78      0.97      0.87        30\n",
      "          72       0.65      1.00      0.79        30\n",
      "          73       0.66      0.97      0.78        30\n",
      "          74       0.88      1.00      0.94        30\n",
      "          75       0.94      0.97      0.95        30\n",
      "          76       0.97      0.97      0.97        30\n",
      "          77       0.83      0.97      0.89        30\n",
      "          78       0.63      0.90      0.74        30\n",
      "          79       1.00      0.93      0.97        30\n",
      "          80       0.94      1.00      0.97        30\n",
      "          81       0.81      1.00      0.90        30\n",
      "          82       0.88      0.97      0.92        30\n",
      "          83       0.97      1.00      0.98        30\n",
      "          84       0.83      0.97      0.89        30\n",
      "          85       0.97      1.00      0.98        30\n",
      "          86       0.97      0.97      0.97        30\n",
      "          87       0.93      0.93      0.93        30\n",
      "          88       1.00      1.00      1.00        30\n",
      "          89       0.86      0.83      0.85        30\n",
      "          90       0.93      0.83      0.88        30\n",
      "          91       0.74      0.93      0.82        30\n",
      "          92       1.00      1.00      1.00        30\n",
      "          93       0.93      0.93      0.93        30\n",
      "          94       1.00      0.97      0.98        30\n",
      "          95       0.97      0.97      0.97        30\n",
      "          96       0.97      1.00      0.98        30\n",
      "          97       1.00      0.90      0.95        30\n",
      "          98       0.83      1.00      0.91        30\n",
      "          99       0.88      1.00      0.94        30\n",
      "         100       1.00      1.00      1.00        30\n",
      "         101       0.93      0.87      0.90        30\n",
      "         102       0.93      0.90      0.92        30\n",
      "         103       0.83      1.00      0.91        30\n",
      "         104       0.67      0.97      0.79        30\n",
      "         105       0.88      0.93      0.90        30\n",
      "         106       0.84      0.90      0.87        30\n",
      "         107       0.96      0.90      0.93        30\n",
      "         108       0.88      1.00      0.94        30\n",
      "         109       0.93      0.93      0.93        30\n",
      "         110       0.91      1.00      0.95        30\n",
      "         111       0.84      0.90      0.87        30\n",
      "         112       0.84      0.90      0.87        30\n",
      "         113       0.94      1.00      0.97        30\n",
      "         114       0.85      0.97      0.91        30\n",
      "         115       1.00      0.97      0.98        30\n",
      "         116       0.88      1.00      0.94        30\n",
      "         117       0.81      0.97      0.88        30\n",
      "         118       1.00      1.00      1.00        30\n",
      "         119       0.97      0.97      0.97        30\n",
      "         120       0.88      0.93      0.90        30\n",
      "         121       0.94      1.00      0.97        30\n",
      "         122       0.97      1.00      0.98        30\n",
      "         123       0.82      0.93      0.88        30\n",
      "         124       0.97      0.97      0.97        30\n",
      "         125       0.97      0.93      0.95        30\n",
      "         126       0.74      0.97      0.84        30\n",
      "         127       0.94      1.00      0.97        30\n",
      "         128       0.83      1.00      0.91        30\n",
      "         129       0.91      1.00      0.95        30\n",
      "         130       0.71      1.00      0.83        30\n",
      "         131       0.82      0.90      0.86        30\n",
      "         132       0.96      0.90      0.93        30\n",
      "         133       0.97      0.93      0.95        30\n",
      "         134       0.86      1.00      0.92        30\n",
      "         135       0.77      1.00      0.87        30\n",
      "         136       0.83      1.00      0.91        30\n",
      "         137       0.97      1.00      0.98        30\n",
      "         138       0.72      0.93      0.81        30\n",
      "         139       0.94      1.00      0.97        30\n",
      "         140       0.85      0.97      0.91        30\n",
      "         141       0.97      1.00      0.98        30\n",
      "         142       0.81      1.00      0.90        30\n",
      "         143       0.80      0.93      0.86        30\n",
      "         144       0.97      1.00      0.98        30\n",
      "         145       0.74      0.97      0.84        30\n",
      "         146       1.00      1.00      1.00        30\n",
      "         147       0.75      1.00      0.86        30\n",
      "         148       0.75      0.90      0.82        30\n",
      "         149       0.81      1.00      0.90        30\n",
      "         150       0.92      0.77      0.84        30\n",
      "\n",
      "    accuracy                           0.87      5500\n",
      "   macro avg       0.87      0.95      0.90      5500\n",
      "weighted avg       0.89      0.87      0.86      5500\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8658181818181818,\n",
       " 'f1_weighted': 0.8555635697311237,\n",
       " 'f1_macro': 0.9042942330777042,\n",
       " 'best_params': {'lr': 0.001, 'hidden_dim1': 128, 'dropout': 0.3},\n",
       " 'total_time_seconds': 6.432085752487183,\n",
       " 'total_time_minutes': 0.10720142920811972}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainNNEmb(\n",
    "    X_train_emotion_bert, X_train_emotion_roberta, X_train_emotion_electra,\n",
    "    X_test_emotion_bert, X_test_emotion_roberta, X_test_emotion_electra,\n",
    "    y_train_emotion_roberta, y_test_emotion_roberta,\n",
    "    num_classes=151,  # emotion tem 6 classes\n",
    "    val_size=0.2,\n",
    "    batch_size=32,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddad2971-edfc-44d3-9e94-99a7f1c5b4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïê Iniciando experimento...\n",
      "üîç Testando 8 combina√ß√µes de hiperpar√¢metros...\n",
      "\n",
      "[1/8] Testando: {'lr': 0.001, 'hidden_dim1': 64, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 2.6130\n",
      "  Epoch 2 - Loss: 0.3547\n",
      "  Epoch 3 - Loss: 0.2048\n",
      "  Epoch 4 - Loss: 0.1561\n",
      "  Epoch 5 - Loss: 0.1381\n",
      "  Val Accuracy: 0.9744 | Val F1: 0.9744\n",
      "\n",
      "[2/8] Testando: {'lr': 0.001, 'hidden_dim1': 64, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 3.0077\n",
      "  Epoch 2 - Loss: 0.7938\n",
      "  Epoch 3 - Loss: 0.4341\n",
      "  Epoch 4 - Loss: 0.3382\n",
      "  Epoch 5 - Loss: 0.2791\n",
      "  Val Accuracy: 0.9762 | Val F1: 0.9762\n",
      "\n",
      "[3/8] Testando: {'lr': 0.001, 'hidden_dim1': 128, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 2.2085\n",
      "  Epoch 2 - Loss: 0.2008\n",
      "  Epoch 3 - Loss: 0.1283\n",
      "  Epoch 4 - Loss: 0.1084\n",
      "  Epoch 5 - Loss: 0.0957\n",
      "  Val Accuracy: 0.9762 | Val F1: 0.9762\n",
      "\n",
      "[4/8] Testando: {'lr': 0.001, 'hidden_dim1': 128, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 2.5049\n",
      "  Epoch 2 - Loss: 0.3491\n",
      "  Epoch 3 - Loss: 0.1990\n",
      "  Epoch 4 - Loss: 0.1585\n",
      "  Epoch 5 - Loss: 0.1309\n",
      "  Val Accuracy: 0.9750 | Val F1: 0.9750\n",
      "\n",
      "[5/8] Testando: {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 3.6717\n",
      "  Epoch 2 - Loss: 1.3815\n",
      "  Epoch 3 - Loss: 0.4526\n",
      "  Epoch 4 - Loss: 0.2677\n",
      "  Epoch 5 - Loss: 0.2088\n",
      "  Val Accuracy: 0.9769 | Val F1: 0.9769\n",
      "\n",
      "[6/8] Testando: {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 3.7940\n",
      "  Epoch 2 - Loss: 1.8503\n",
      "  Epoch 3 - Loss: 0.8497\n",
      "  Epoch 4 - Loss: 0.5422\n",
      "  Epoch 5 - Loss: 0.4256\n",
      "  Val Accuracy: 0.9744 | Val F1: 0.9743\n",
      "\n",
      "[7/8] Testando: {'lr': 0.0005, 'hidden_dim1': 128, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 3.2738\n",
      "  Epoch 2 - Loss: 0.6679\n",
      "  Epoch 3 - Loss: 0.2229\n",
      "  Epoch 4 - Loss: 0.1519\n",
      "  Epoch 5 - Loss: 0.1268\n",
      "  Val Accuracy: 0.9725 | Val F1: 0.9726\n",
      "\n",
      "[8/8] Testando: {'lr': 0.0005, 'hidden_dim1': 128, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 3.4456\n",
      "  Epoch 2 - Loss: 1.0388\n",
      "  Epoch 3 - Loss: 0.3797\n",
      "  Epoch 4 - Loss: 0.2451\n",
      "  Epoch 5 - Loss: 0.1942\n",
      "  Val Accuracy: 0.9744 | Val F1: 0.9744\n",
      "\n",
      "============================================================\n",
      "üèÜ MELHORES RESULTADOS NA VALIDA√á√ÉO:\n",
      "Melhores par√¢metros: {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.3}\n",
      "Melhor F1 na valida√ß√£o: 0.9769\n",
      "\n",
      "üìä AVALIA√á√ÉO FINAL NO TESTE:\n",
      "============================================================\n",
      "üìà RESULTADOS FINAIS:\n",
      "Acur√°cia no teste: 0.9223\n",
      "F1-Score (weighted): 0.9224\n",
      "F1-Score (macro): 0.9223\n",
      "‚è±Ô∏è  Tempo total: 5.36 segundos (0.09 minutos)\n",
      "============================================================\n",
      "\n",
      "üìã RELAT√ìRIO DETALHADO:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97        40\n",
      "           1       1.00      1.00      1.00        40\n",
      "           2       1.00      1.00      1.00        40\n",
      "           3       0.95      1.00      0.97        39\n",
      "           4       1.00      0.88      0.93        40\n",
      "           5       0.81      0.72      0.76        40\n",
      "           6       0.97      0.93      0.95        40\n",
      "           7       0.92      0.90      0.91        40\n",
      "           8       1.00      0.97      0.99        40\n",
      "           9       0.97      0.97      0.97        40\n",
      "          10       0.97      0.88      0.92        40\n",
      "          11       0.88      0.88      0.88        40\n",
      "          12       0.90      0.93      0.91        40\n",
      "          13       1.00      0.97      0.99        40\n",
      "          14       0.91      0.97      0.94        40\n",
      "          15       0.90      0.95      0.93        40\n",
      "          16       0.90      0.88      0.89        40\n",
      "          17       0.97      0.95      0.96        40\n",
      "          18       0.92      0.85      0.88        40\n",
      "          19       0.97      0.95      0.96        40\n",
      "          20       0.87      0.97      0.92        40\n",
      "          21       0.97      0.97      0.97        40\n",
      "          22       0.90      0.93      0.91        40\n",
      "          23       1.00      0.90      0.95        40\n",
      "          24       0.93      1.00      0.96        40\n",
      "          25       0.87      0.85      0.86        40\n",
      "          26       0.75      1.00      0.86        40\n",
      "          27       0.97      0.72      0.83        40\n",
      "          28       0.95      0.88      0.91        40\n",
      "          29       0.95      0.90      0.92        40\n",
      "          30       0.98      1.00      0.99        40\n",
      "          31       0.97      0.93      0.95        40\n",
      "          32       0.91      0.97      0.94        40\n",
      "          33       0.86      0.95      0.90        39\n",
      "          34       0.93      0.97      0.95        40\n",
      "          35       0.86      0.90      0.88        40\n",
      "          36       0.92      0.85      0.88        40\n",
      "          37       0.87      0.82      0.85        40\n",
      "          38       0.93      1.00      0.96        40\n",
      "          39       0.95      0.97      0.96        40\n",
      "          40       0.78      0.97      0.87        40\n",
      "          41       0.86      0.95      0.90        40\n",
      "          42       1.00      0.97      0.99        40\n",
      "          43       0.86      0.90      0.88        40\n",
      "          44       0.95      1.00      0.98        40\n",
      "          45       0.97      0.88      0.92        40\n",
      "          46       1.00      0.97      0.99        40\n",
      "          47       0.97      0.93      0.95        40\n",
      "          48       0.90      0.69      0.78        39\n",
      "          49       0.97      0.88      0.92        40\n",
      "          50       0.89      0.97      0.93        40\n",
      "          51       1.00      0.97      0.99        40\n",
      "          52       1.00      0.97      0.99        40\n",
      "          53       0.78      0.95      0.85        40\n",
      "          54       0.90      0.90      0.90        40\n",
      "          55       0.93      1.00      0.96        40\n",
      "          56       0.92      0.90      0.91        40\n",
      "          57       0.97      0.95      0.96        40\n",
      "          58       0.95      0.93      0.94        40\n",
      "          59       0.87      0.97      0.92        40\n",
      "          60       0.97      0.97      0.97        40\n",
      "          61       0.95      0.88      0.91        40\n",
      "          62       0.73      0.80      0.76        40\n",
      "          63       0.95      1.00      0.98        40\n",
      "          64       0.97      0.95      0.96        40\n",
      "          65       0.89      0.78      0.83        40\n",
      "          66       0.74      0.90      0.81        39\n",
      "          67       0.76      0.93      0.83        40\n",
      "          68       0.97      0.93      0.95        40\n",
      "          69       0.82      0.90      0.86        40\n",
      "          70       1.00      1.00      1.00        40\n",
      "          71       1.00      1.00      1.00        40\n",
      "          72       1.00      0.75      0.86        40\n",
      "          73       1.00      0.93      0.96        40\n",
      "          74       0.86      0.80      0.83        40\n",
      "          75       1.00      0.90      0.95        40\n",
      "          76       0.97      0.93      0.95        40\n",
      "\n",
      "    accuracy                           0.92      3076\n",
      "   macro avg       0.93      0.92      0.92      3076\n",
      "weighted avg       0.93      0.92      0.92      3076\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9223016905071522,\n",
       " 'f1_weighted': 0.9223716513605886,\n",
       " 'f1_macro': 0.9223016885693458,\n",
       " 'best_params': {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.3},\n",
       " 'total_time_seconds': 5.358517646789551,\n",
       " 'total_time_minutes': 0.08930862744649251}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainNNEmbL2(\n",
    "    X_train_emotion_bert, X_train_emotion_roberta, X_train_emotion_electra,\n",
    "    X_test_emotion_bert, X_test_emotion_roberta, X_test_emotion_electra,\n",
    "    y_train_emotion_roberta, y_test_emotion_roberta,\n",
    "    num_classes=77, \n",
    "    val_size=0.2,\n",
    "    batch_size=32,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7a74ee4-cb03-4c21-9e8b-ccafffe1f06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïê Iniciando experimento...\n",
      "üîç Testando 8 combina√ß√µes de hiperpar√¢metros...\n",
      "\n",
      "[1/8] Testando: {'lr': 0.001, 'hidden_dim1': 64, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 1.0102\n",
      "  Epoch 2 - Loss: 0.1989\n",
      "  Epoch 3 - Loss: 0.1774\n",
      "  Epoch 4 - Loss: 0.1282\n",
      "  Epoch 5 - Loss: 0.1182\n",
      "  Val Accuracy: 0.9794 | Val F1: 0.9793\n",
      "\n",
      "[2/8] Testando: {'lr': 0.001, 'hidden_dim1': 64, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 1.4355\n",
      "  Epoch 2 - Loss: 0.3787\n",
      "  Epoch 3 - Loss: 0.3155\n",
      "  Epoch 4 - Loss: 0.2741\n",
      "  Epoch 5 - Loss: 0.2549\n",
      "  Val Accuracy: 0.9787 | Val F1: 0.9787\n",
      "\n",
      "[3/8] Testando: {'lr': 0.001, 'hidden_dim1': 128, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 0.7013\n",
      "  Epoch 2 - Loss: 0.1355\n",
      "  Epoch 3 - Loss: 0.1243\n",
      "  Epoch 4 - Loss: 0.0992\n",
      "  Epoch 5 - Loss: 0.0979\n",
      "  Val Accuracy: 0.9775 | Val F1: 0.9775\n",
      "\n",
      "[4/8] Testando: {'lr': 0.001, 'hidden_dim1': 128, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 0.9221\n",
      "  Epoch 2 - Loss: 0.2102\n",
      "  Epoch 3 - Loss: 0.1747\n",
      "  Epoch 4 - Loss: 0.1595\n",
      "  Epoch 5 - Loss: 0.1438\n",
      "  Val Accuracy: 0.9725 | Val F1: 0.9724\n",
      "\n",
      "[5/8] Testando: {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 1.5977\n",
      "  Epoch 2 - Loss: 0.2351\n",
      "  Epoch 3 - Loss: 0.1773\n",
      "  Epoch 4 - Loss: 0.1398\n",
      "  Epoch 5 - Loss: 0.1297\n",
      "  Val Accuracy: 0.9769 | Val F1: 0.9767\n",
      "\n",
      "[6/8] Testando: {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 2.1191\n",
      "  Epoch 2 - Loss: 0.5447\n",
      "  Epoch 3 - Loss: 0.3770\n",
      "  Epoch 4 - Loss: 0.3139\n",
      "  Epoch 5 - Loss: 0.2484\n",
      "  Val Accuracy: 0.9781 | Val F1: 0.9781\n",
      "\n",
      "[7/8] Testando: {'lr': 0.0005, 'hidden_dim1': 128, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 1.1200\n",
      "  Epoch 2 - Loss: 0.1427\n",
      "  Epoch 3 - Loss: 0.1108\n",
      "  Epoch 4 - Loss: 0.0986\n",
      "  Epoch 5 - Loss: 0.0819\n",
      "  Val Accuracy: 0.9781 | Val F1: 0.9780\n",
      "\n",
      "[8/8] Testando: {'lr': 0.0005, 'hidden_dim1': 128, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 1.4804\n",
      "  Epoch 2 - Loss: 0.2299\n",
      "  Epoch 3 - Loss: 0.1732\n",
      "  Epoch 4 - Loss: 0.1499\n",
      "  Epoch 5 - Loss: 0.1266\n",
      "  Val Accuracy: 0.9794 | Val F1: 0.9793\n",
      "\n",
      "============================================================\n",
      "üèÜ MELHORES RESULTADOS NA VALIDA√á√ÉO:\n",
      "Melhores par√¢metros: {'lr': 0.0005, 'hidden_dim1': 128, 'dropout': 0.5}\n",
      "Melhor F1 na valida√ß√£o: 0.9793\n",
      "\n",
      "üìä AVALIA√á√ÉO FINAL NO TESTE:\n",
      "============================================================\n",
      "üìà RESULTADOS FINAIS:\n",
      "Acur√°cia no teste: 0.9249\n",
      "F1-Score (weighted): 0.9247\n",
      "F1-Score (macro): 0.9247\n",
      "‚è±Ô∏è  Tempo total: 5.31 segundos (0.09 minutos)\n",
      "============================================================\n",
      "\n",
      "üìã RELAT√ìRIO DETALHADO:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96        40\n",
      "           1       1.00      1.00      1.00        40\n",
      "           2       0.98      1.00      0.99        40\n",
      "           3       0.95      1.00      0.97        39\n",
      "           4       1.00      0.88      0.93        40\n",
      "           5       0.74      0.70      0.72        40\n",
      "           6       0.97      0.88      0.92        40\n",
      "           7       0.82      0.93      0.87        40\n",
      "           8       1.00      0.93      0.96        40\n",
      "           9       1.00      1.00      1.00        40\n",
      "          10       0.97      0.93      0.95        40\n",
      "          11       0.88      0.88      0.88        40\n",
      "          12       0.90      0.93      0.91        40\n",
      "          13       1.00      0.97      0.99        40\n",
      "          14       0.90      0.95      0.93        40\n",
      "          15       0.88      0.95      0.92        40\n",
      "          16       0.92      0.90      0.91        40\n",
      "          17       0.95      0.95      0.95        40\n",
      "          18       1.00      0.80      0.89        40\n",
      "          19       0.97      0.95      0.96        40\n",
      "          20       0.91      0.97      0.94        40\n",
      "          21       0.95      1.00      0.98        40\n",
      "          22       0.90      0.88      0.89        40\n",
      "          23       1.00      0.90      0.95        40\n",
      "          24       0.95      0.97      0.96        40\n",
      "          25       0.89      0.85      0.87        40\n",
      "          26       0.77      1.00      0.87        40\n",
      "          27       1.00      0.72      0.84        40\n",
      "          28       0.92      0.88      0.90        40\n",
      "          29       0.95      0.88      0.91        40\n",
      "          30       0.98      1.00      0.99        40\n",
      "          31       0.97      0.93      0.95        40\n",
      "          32       0.91      0.97      0.94        40\n",
      "          33       0.86      0.95      0.90        39\n",
      "          34       0.95      0.97      0.96        40\n",
      "          35       0.92      0.88      0.90        40\n",
      "          36       0.92      0.85      0.88        40\n",
      "          37       0.80      0.88      0.83        40\n",
      "          38       0.98      1.00      0.99        40\n",
      "          39       0.95      0.97      0.96        40\n",
      "          40       0.89      0.97      0.93        40\n",
      "          41       0.83      0.95      0.88        40\n",
      "          42       1.00      0.97      0.99        40\n",
      "          43       0.88      0.90      0.89        40\n",
      "          44       1.00      1.00      1.00        40\n",
      "          45       0.93      0.95      0.94        40\n",
      "          46       0.95      0.97      0.96        40\n",
      "          47       0.95      0.93      0.94        40\n",
      "          48       0.91      0.82      0.86        39\n",
      "          49       0.97      0.88      0.92        40\n",
      "          50       0.93      0.97      0.95        40\n",
      "          51       1.00      0.97      0.99        40\n",
      "          52       1.00      0.97      0.99        40\n",
      "          53       0.88      0.95      0.92        40\n",
      "          54       0.90      0.95      0.93        40\n",
      "          55       0.95      1.00      0.98        40\n",
      "          56       0.95      0.88      0.91        40\n",
      "          57       0.95      0.95      0.95        40\n",
      "          58       0.86      0.93      0.89        40\n",
      "          59       0.91      0.97      0.94        40\n",
      "          60       0.91      1.00      0.95        40\n",
      "          61       0.85      0.85      0.85        40\n",
      "          62       0.79      0.75      0.77        40\n",
      "          63       0.95      1.00      0.98        40\n",
      "          64       0.93      0.95      0.94        40\n",
      "          65       0.91      0.80      0.85        40\n",
      "          66       0.78      0.92      0.85        39\n",
      "          67       0.84      0.90      0.87        40\n",
      "          68       0.91      0.97      0.94        40\n",
      "          69       0.79      0.93      0.85        40\n",
      "          70       1.00      1.00      1.00        40\n",
      "          71       1.00      1.00      1.00        40\n",
      "          72       1.00      0.82      0.90        40\n",
      "          73       0.97      0.93      0.95        40\n",
      "          74       0.97      0.75      0.85        40\n",
      "          75       1.00      0.90      0.95        40\n",
      "          76       0.97      0.93      0.95        40\n",
      "\n",
      "    accuracy                           0.92      3076\n",
      "   macro avg       0.93      0.92      0.92      3076\n",
      "weighted avg       0.93      0.92      0.92      3076\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9249024707412223,\n",
       " 'f1_weighted': 0.9247079466781846,\n",
       " 'f1_macro': 0.924672404771065,\n",
       " 'best_params': {'lr': 0.0005, 'hidden_dim1': 128, 'dropout': 0.5},\n",
       " 'total_time_seconds': 5.309385299682617,\n",
       " 'total_time_minutes': 0.08848975499471029}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainNNEmbOutra(\n",
    "    X_train_emotion_bert, X_train_emotion_roberta, X_train_emotion_electra,\n",
    "    X_test_emotion_bert, X_test_emotion_roberta, X_test_emotion_electra,\n",
    "    y_train_emotion_roberta, y_test_emotion_roberta,\n",
    "    num_classes=77,  # emotion tem 6 classes\n",
    "    val_size=0.2,\n",
    "    batch_size=32,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aa610c-6613-4503-9efa-6d3340347ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
