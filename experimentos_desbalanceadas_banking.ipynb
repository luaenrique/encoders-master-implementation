{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0778f19a-abc8-4031-9dd2-4a6df32cfa9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lua/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "import itertools\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7a85655-b5eb-40b7-9ca9-3708ead8f88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in ./venv/lib/python3.11/site-packages (3.0.5)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.11/site-packages (from xgboost) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in ./venv/lib/python3.11/site-packages (from xgboost) (2.26.2)\n",
      "Requirement already satisfied: scipy in ./venv/lib/python3.11/site-packages (from xgboost) (1.15.3)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9c37bdd-d3d4-41d1-9335-6e514594ca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def create_model(input_dim, hidden_dim1, dropout, num_classes=2):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_dim, hidden_dim1),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(hidden_dim1, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, num_classes)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e994828-70ab-4314-afea-b15bcc45c469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "def trainXGBoostOnly(\n",
    "    bertTrainLogits, robertaTrainLogits, electraTrainLogits,\n",
    "    bertTestLogits, robertaTestLogits, electraTestLogits,\n",
    "    trainLabels, testLabels,\n",
    "    num_classes,\n",
    "    val_size=0.2,\n",
    "    random_state=42\n",
    "):\n",
    "    # Inicia contagem de tempo\n",
    "    start_time = time.time()\n",
    "    print(\"üïê Iniciando experimento XGBoost...\")\n",
    "    \n",
    "    # Concatena os logits das tr√™s redes\n",
    "    concatenated_logits_train = np.concatenate(\n",
    "        [bertTrainLogits['logits'], robertaTrainLogits['logits'], electraTrainLogits['logits']], axis=1\n",
    "    )\n",
    "    concatenated_logits_test = np.concatenate(\n",
    "        [bertTestLogits['logits'], robertaTestLogits['logits'], electraTestLogits['logits']], axis=1\n",
    "    )\n",
    "    \n",
    "    train_labels = np.array(trainLabels)\n",
    "    test_labels = np.array(testLabels)\n",
    "    \n",
    "    print(f\"üìä Shape dos dados concatenados: {concatenated_logits_train.shape}\")\n",
    "    print(f\"üìä N√∫mero de classes: {num_classes}\")\n",
    "    \n",
    "    # Dividir treino em treino e valida√ß√£o (estratificado)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        concatenated_logits_train,\n",
    "        train_labels,\n",
    "        test_size=val_size,\n",
    "        stratify=train_labels,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Define o classificador XGBoost (ajuste o objective conforme n√∫mero de classes)\n",
    "    if num_classes == 2:\n",
    "        objective = 'binary:logistic'\n",
    "        eval_metric = 'logloss'\n",
    "        scoring_metric = 'f1'\n",
    "    else:\n",
    "        objective = 'multi:softprob'\n",
    "        eval_metric = 'mlogloss'\n",
    "        scoring_metric = 'f1_weighted'  # Corrigido para multiclasse\n",
    "    \n",
    "    xgb_clf = xgb.XGBClassifier(\n",
    "        tree_method='gpu_hist',\n",
    "        gpu_id=0,\n",
    "        objective=objective,\n",
    "        eval_metric=eval_metric,\n",
    "        use_label_encoder=False,\n",
    "        num_class=num_classes if num_classes > 2 else None,\n",
    "        random_state=random_state,\n",
    "        verbosity=0  # Reduzido para menos verbose\n",
    "    )\n",
    "    \n",
    "    # Grade de hiperpar√¢metros para busca\n",
    "    param_grid_xgb = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 4, 6],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0]\n",
    "    }\n",
    "    \n",
    "    total_combinations = np.prod([len(v) for v in param_grid_xgb.values()])\n",
    "    print(f\"üîç Testando {total_combinations} combina√ß√µes de hiperpar√¢metros com CV=3...\")\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=xgb_clf,\n",
    "        param_grid=param_grid_xgb,\n",
    "        scoring=scoring_metric,\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        verbose=1,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    print(\"‚öôÔ∏è  Rodando GridSearch para XGBoost...\")\n",
    "    grid_start_time = time.time()\n",
    "    \n",
    "    # Usa apenas treino para GridSearch (valida√ß√£o fica separada para avalia√ß√£o)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    grid_end_time = time.time()\n",
    "    print(f\"‚úÖ GridSearch conclu√≠do em {grid_end_time - grid_start_time:.2f} segundos\")\n",
    "    \n",
    "    best_xgb_model = grid_search.best_estimator_\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üèÜ MELHORES HIPERPAR√ÇMETROS:\")\n",
    "    print(grid_search.best_params_)\n",
    "    print(f\"üéØ Melhor score no CV: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Avalia√ß√£o no conjunto de valida√ß√£o\n",
    "    print(\"\\nüìä AVALIA√á√ÉO NO CONJUNTO DE VALIDA√á√ÉO:\")\n",
    "    val_pred = best_xgb_model.predict(X_val)\n",
    "    val_accuracy = accuracy_score(y_val, val_pred)\n",
    "    val_f1_weighted = f1_score(y_val, val_pred, average='weighted')\n",
    "    val_f1_macro = f1_score(y_val, val_pred, average='macro')\n",
    "    \n",
    "    print(f\"Valida√ß√£o - Acur√°cia: {val_accuracy:.4f}\")\n",
    "    print(f\"Valida√ß√£o - F1 Weighted: {val_f1_weighted:.4f}\")\n",
    "    print(f\"Valida√ß√£o - F1 Macro: {val_f1_macro:.4f}\")\n",
    "    \n",
    "    # Retreina o modelo com treino + valida√ß√£o para avalia√ß√£o final\n",
    "    print(\"\\nüîÑ Retreinando modelo final com treino + valida√ß√£o...\")\n",
    "    X_trainval = np.concatenate([X_train, X_val], axis=0)\n",
    "    y_trainval = np.concatenate([y_train, y_val], axis=0)\n",
    "    \n",
    "    final_model = xgb.XGBClassifier(**grid_search.best_params_,\n",
    "                                   objective=objective,\n",
    "                                   eval_metric=eval_metric,\n",
    "                                   use_label_encoder=False,\n",
    "                                   num_class=num_classes if num_classes > 2 else None,\n",
    "                                   random_state=random_state,\n",
    "                                   verbosity=0)\n",
    "    \n",
    "    final_model.fit(X_trainval, y_trainval)\n",
    "    \n",
    "    # Avalia√ß√£o final no conjunto de teste\n",
    "    print(\"\\nüìà AVALIA√á√ÉO FINAL NO TESTE:\")\n",
    "    y_pred = final_model.predict(concatenated_logits_test)\n",
    "    \n",
    "    # Calcula m√©tricas finais\n",
    "    test_accuracy = accuracy_score(test_labels, y_pred)\n",
    "    test_f1_weighted = f1_score(test_labels, y_pred, average='weighted')\n",
    "    test_f1_macro = f1_score(test_labels, y_pred, average='macro')\n",
    "    \n",
    "    # Tempo total\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"üìà RESULTADOS FINAIS:\")\n",
    "    print(f\"Acur√°cia no teste: {test_accuracy:.4f}\")\n",
    "    print(f\"F1-Score (weighted): {test_f1_weighted:.4f}\")\n",
    "    print(f\"F1-Score (macro): {test_f1_macro:.4f}\")\n",
    "    print(f\"‚è±Ô∏è  Tempo total: {total_time:.2f} segundos ({total_time/60:.2f} minutos)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nüìã RELAT√ìRIO DETALHADO:\")\n",
    "    print(classification_report(test_labels, y_pred))\n",
    "    \n",
    "    # Informa√ß√µes adicionais do modelo\n",
    "    if hasattr(final_model, 'feature_importances_'):\n",
    "        print(f\"\\nüîç Top 5 features mais importantes:\")\n",
    "        feature_importance = final_model.feature_importances_\n",
    "        top_features = np.argsort(feature_importance)[-5:][::-1]\n",
    "        for i, feat_idx in enumerate(top_features, 1):\n",
    "            print(f\"  {i}. Feature {feat_idx}: {feature_importance[feat_idx]:.4f}\")\n",
    "    \n",
    "    # Retorna as m√©tricas principais\n",
    "    return {\n",
    "        'model': final_model,\n",
    "        'accuracy': test_accuracy,\n",
    "        'f1_weighted': test_f1_weighted,\n",
    "        'f1_macro': test_f1_macro,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_cv_score': grid_search.best_score_,\n",
    "        'total_time_seconds': total_time,\n",
    "        'total_time_minutes': total_time/60,\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'val_f1_weighted': val_f1_weighted,\n",
    "        'val_f1_macro': val_f1_macro\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dc8305e-157c-48ab-bad3-5ea938de0dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "def trainNNLogits(\n",
    "    bertTrainLogits, robertaTrainLogits, electraTrainLogits,\n",
    "    bertTestLogits, robertaTestLogits, electraTestLogits,\n",
    "    trainLabels, testLabels,\n",
    "    num_classes,\n",
    "    val_size=0.2,\n",
    "    batch_size=32,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "):\n",
    "    # Inicia contagem de tempo\n",
    "    start_time = time.time()\n",
    "    print(\"üïê Iniciando experimento NN com Logits...\")\n",
    "    \n",
    "    # Concatena os logits das tr√™s redes\n",
    "    concatenated_logits = np.concatenate(\n",
    "        [bertTrainLogits['logits'], robertaTrainLogits['logits'], electraTrainLogits['logits']], axis=1\n",
    "    )\n",
    "    concatenated_test_logits = np.concatenate(\n",
    "        [bertTestLogits['logits'], robertaTestLogits['logits'], electraTestLogits['logits']], axis=1\n",
    "    )\n",
    "\n",
    "    train_labels = np.array(trainLabels)\n",
    "    test_labels = np.array(testLabels)\n",
    "    \n",
    "    print(f\"üìä Shape dos logits concatenados: {concatenated_logits.shape}\")\n",
    "    print(f\"üìä N√∫mero de classes: {num_classes}\")\n",
    "\n",
    "    # Split treino/val a partir do conjunto de treino\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        concatenated_logits,\n",
    "        train_labels,\n",
    "        test_size=val_size,\n",
    "        stratify=train_labels,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Cria DataLoaders\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "    val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
    "    test_dataset = TensorDataset(torch.tensor(concatenated_test_logits, dtype=torch.float32), torch.tensor(test_labels, dtype=torch.long))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    param_grid = {\n",
    "        'lr': [1e-3, 5e-4],\n",
    "        'hidden_dim1': [64, 128],\n",
    "        'dropout': [0.3, 0.5]\n",
    "    }\n",
    "\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "    best_f1 = 0.0  # Mudan√ßa: agora otimiza pelo F1 ao inv√©s da loss\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    print(f\"üîç Testando {len(combinations)} combina√ß√µes de hiperpar√¢metros...\")\n",
    "\n",
    "    for i, params in enumerate(combinations, 1):\n",
    "        print(f\"\\n[{i}/{len(combinations)}] Testando: {params}\")\n",
    "        \n",
    "        model = create_model(\n",
    "            input_dim=X_train.shape[1],\n",
    "            hidden_dim1=params['hidden_dim1'],\n",
    "            dropout=params['dropout'],\n",
    "            num_classes=num_classes\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Treina por 5 epochs\n",
    "        for epoch in range(5):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * X_batch.size(0)\n",
    "            epoch_loss = running_loss / len(train_loader.dataset)\n",
    "            print(f\"  Epoch {epoch+1} - Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Avalia√ß√£o no conjunto de valida√ß√£o (agora com F1)\n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        val_true_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                val_predictions.extend(predicted.cpu().numpy())\n",
    "                val_true_labels.extend(y_batch.cpu().numpy())\n",
    "        \n",
    "        # Calcula F1 na valida√ß√£o\n",
    "        val_f1 = f1_score(val_true_labels, val_predictions, average='weighted')\n",
    "        val_acc = accuracy_score(val_true_labels, val_predictions)\n",
    "        \n",
    "        print(f\"  Val Accuracy: {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_params = params\n",
    "            best_model = model.state_dict().copy()  # Salva o estado do modelo\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üèÜ MELHORES RESULTADOS NA VALIDA√á√ÉO:\")\n",
    "    print(f\"Melhores par√¢metros: {best_params}\")\n",
    "    print(f\"Melhor F1 na valida√ß√£o: {best_f1:.4f}\")\n",
    "\n",
    "    # Recria o melhor modelo para avalia√ß√£o final\n",
    "    final_model = create_model(\n",
    "        input_dim=X_train.shape[1],\n",
    "        hidden_dim1=best_params['hidden_dim1'],\n",
    "        dropout=best_params['dropout'],\n",
    "        num_classes=num_classes\n",
    "    ).to(device)\n",
    "    final_model.load_state_dict(best_model)\n",
    "\n",
    "    # Avalia√ß√£o final no conjunto de teste\n",
    "    print(\"\\nüìä AVALIA√á√ÉO FINAL NO TESTE:\")\n",
    "    final_model.eval()\n",
    "    test_predictions = []\n",
    "    test_true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = final_model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            test_predictions.extend(predicted.cpu().numpy())\n",
    "            test_true_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    # Calcula m√©tricas finais\n",
    "    test_accuracy = accuracy_score(test_true_labels, test_predictions)\n",
    "    test_f1_weighted = f1_score(test_true_labels, test_predictions, average='weighted')\n",
    "    test_f1_macro = f1_score(test_true_labels, test_predictions, average='macro')\n",
    "    \n",
    "    # Tempo total\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"üìà RESULTADOS FINAIS:\")\n",
    "    print(f\"Acur√°cia no teste: {test_accuracy:.4f}\")\n",
    "    print(f\"F1-Score (weighted): {test_f1_weighted:.4f}\")\n",
    "    print(f\"F1-Score (macro): {test_f1_macro:.4f}\")\n",
    "    print(f\"‚è±Ô∏è  Tempo total: {total_time:.2f} segundos ({total_time/60:.2f} minutos)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nüìã RELAT√ìRIO DETALHADO:\")\n",
    "    print(classification_report(test_true_labels, test_predictions))\n",
    "    \n",
    "    # Retorna as m√©tricas principais\n",
    "    return {\n",
    "        'accuracy': test_accuracy,\n",
    "        'f1_weighted': test_f1_weighted,\n",
    "        'f1_macro': test_f1_macro,\n",
    "        'best_params': best_params,\n",
    "        'total_time_seconds': total_time,\n",
    "        'total_time_minutes': total_time/60\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec6f940-fc60-4ca4-b5c9-5b7cc7bd7834",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2bac7e2-4f1e-46ca-9c03-4c18784f2c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "def trainNNEmb(\n",
    "    bertTrainEmbeddings, robertaTrainEmbeddings, electraTrainEmbeddings,\n",
    "    bertTestEmbeddings, robertaTestEmbeddings, electraTestEmbeddings,\n",
    "    trainLabels, testLabels,\n",
    "    num_classes,\n",
    "    val_size=0.2,\n",
    "    batch_size=32,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "):\n",
    "    # Inicia contagem de tempo\n",
    "    start_time = time.time()\n",
    "    print(\"üïê Iniciando experimento...\")\n",
    "    \n",
    "    # Concatena os logits das tr√™s redes\n",
    "    concatenated_logits = np.concatenate(\n",
    "        [bertTrainEmbeddings, robertaTrainEmbeddings, electraTrainEmbeddings], axis=1\n",
    "    )\n",
    "    concatenated_test_logits = np.concatenate(\n",
    "        [bertTestEmbeddings, robertaTestEmbeddings, electraTestEmbeddings], axis=1\n",
    "    )\n",
    "\n",
    "    train_labels = np.array(trainLabels)\n",
    "    test_labels = np.array(testLabels)\n",
    "\n",
    "    # Split treino/val a partir do conjunto de treino\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        concatenated_logits,\n",
    "        train_labels,\n",
    "        test_size=val_size,\n",
    "        stratify=train_labels,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Cria DataLoaders\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "    val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
    "    test_dataset = TensorDataset(torch.tensor(concatenated_test_logits, dtype=torch.float32), torch.tensor(test_labels, dtype=torch.long))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    param_grid = {\n",
    "        'lr': [1e-3, 5e-4],\n",
    "        'hidden_dim1': [64, 128],\n",
    "        'dropout': [0.3, 0.5]\n",
    "    }\n",
    "\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "    best_f1 = 0.0  # Mudan√ßa: agora otimiza pelo F1 ao inv√©s da loss\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    print(f\"üîç Testando {len(combinations)} combina√ß√µes de hiperpar√¢metros...\")\n",
    "\n",
    "    for i, params in enumerate(combinations, 1):\n",
    "        print(f\"\\n[{i}/{len(combinations)}] Testando: {params}\")\n",
    "        \n",
    "        model = create_model(\n",
    "            input_dim=X_train.shape[1],\n",
    "            hidden_dim1=params['hidden_dim1'],\n",
    "            dropout=params['dropout'],\n",
    "            num_classes=num_classes\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Treina por 5 epochs\n",
    "        for epoch in range(5):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * X_batch.size(0)\n",
    "            epoch_loss = running_loss / len(train_loader.dataset)\n",
    "            print(f\"  Epoch {epoch+1} - Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Avalia√ß√£o no conjunto de valida√ß√£o (agora com F1)\n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        val_true_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                val_predictions.extend(predicted.cpu().numpy())\n",
    "                val_true_labels.extend(y_batch.cpu().numpy())\n",
    "        \n",
    "        # Calcula F1 na valida√ß√£o\n",
    "        val_f1 = f1_score(val_true_labels, val_predictions, average='weighted')\n",
    "        val_acc = accuracy_score(val_true_labels, val_predictions)\n",
    "        \n",
    "        print(f\"  Val Accuracy: {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_params = params\n",
    "            best_model = model.state_dict().copy()  # Salva o estado do modelo\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üèÜ MELHORES RESULTADOS NA VALIDA√á√ÉO:\")\n",
    "    print(f\"Melhores par√¢metros: {best_params}\")\n",
    "    print(f\"Melhor F1 na valida√ß√£o: {best_f1:.4f}\")\n",
    "\n",
    "    # Recria o melhor modelo para avalia√ß√£o final\n",
    "    final_model = create_model(\n",
    "        input_dim=X_train.shape[1],\n",
    "        hidden_dim1=best_params['hidden_dim1'],\n",
    "        dropout=best_params['dropout'],\n",
    "        num_classes=num_classes\n",
    "    ).to(device)\n",
    "    final_model.load_state_dict(best_model)\n",
    "\n",
    "    # Avalia√ß√£o final no conjunto de teste\n",
    "    print(\"\\nüìä AVALIA√á√ÉO FINAL NO TESTE:\")\n",
    "    final_model.eval()\n",
    "    test_predictions = []\n",
    "    test_true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = final_model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            test_predictions.extend(predicted.cpu().numpy())\n",
    "            test_true_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    # Calcula m√©tricas finais\n",
    "    test_accuracy = accuracy_score(test_true_labels, test_predictions)\n",
    "    test_f1_weighted = f1_score(test_true_labels, test_predictions, average='weighted')\n",
    "    test_f1_macro = f1_score(test_true_labels, test_predictions, average='macro')\n",
    "    \n",
    "    # Tempo total\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"üìà RESULTADOS FINAIS:\")\n",
    "    print(f\"Acur√°cia no teste: {test_accuracy:.4f}\")\n",
    "    print(f\"F1-Score (weighted): {test_f1_weighted:.4f}\")\n",
    "    print(f\"F1-Score (macro): {test_f1_macro:.4f}\")\n",
    "    print(f\"‚è±Ô∏è  Tempo total: {total_time:.2f} segundos ({total_time/60:.2f} minutos)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nüìã RELAT√ìRIO DETALHADO:\")\n",
    "    print(classification_report(test_true_labels, test_predictions))\n",
    "    \n",
    "    # Retorna as m√©tricas principais\n",
    "    return {\n",
    "        'accuracy': test_accuracy,\n",
    "        'f1_weighted': test_f1_weighted,\n",
    "        'f1_macro': test_f1_macro,\n",
    "        'best_params': best_params,\n",
    "        'total_time_seconds': total_time,\n",
    "        'total_time_minutes': total_time/60\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca1a22be-8670-4cfb-be0c-7a6f524895a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def trainNNEmbL2(\n",
    "    bertTrainEmbeddings, robertaTrainEmbeddings, electraTrainEmbeddings,\n",
    "    bertTestEmbeddings, robertaTestEmbeddings, electraTestEmbeddings,\n",
    "    trainLabels, testLabels,\n",
    "    num_classes,\n",
    "    val_size=0.2,\n",
    "    batch_size=32,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "):\n",
    "    # Inicia contagem de tempo\n",
    "    start_time = time.time()\n",
    "    print(\"üïê Iniciando experimento...\")\n",
    "    bertTrainL2 = normalize(bertTrainEmbeddings, norm='l2', axis=1)\n",
    "    robertaTrainL2 = normalize(robertaTrainEmbeddings, norm='l2', axis=1)\n",
    "    electraTrainL2 = normalize(electraTrainEmbeddings, norm='l2', axis=1)\n",
    "\n",
    "    bertTestL2 = normalize(bertTestEmbeddings, norm='l2', axis=1)\n",
    "    robertaTestL2 = normalize(robertaTestEmbeddings, norm='l2', axis=1)\n",
    "    electraTestL2 = normalize(electraTestEmbeddings, norm='l2', axis=1)\n",
    "    # Concatena os logits das tr√™s redes\n",
    "    concatenated_logits = np.concatenate(\n",
    "        [bertTrainL2, robertaTrainL2, electraTrainL2], axis=1\n",
    "    )\n",
    "    concatenated_test_logits = np.concatenate(\n",
    "        [bertTestL2, robertaTestL2, electraTestL2], axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "    train_labels = np.array(trainLabels)\n",
    "    test_labels = np.array(testLabels)\n",
    "\n",
    "    # Split treino/val a partir do conjunto de treino\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        concatenated_logits,\n",
    "        train_labels,\n",
    "        test_size=val_size,\n",
    "        stratify=train_labels,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Cria DataLoaders\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "    val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
    "    test_dataset = TensorDataset(torch.tensor(concatenated_test_logits, dtype=torch.float32), torch.tensor(test_labels, dtype=torch.long))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    param_grid = {\n",
    "        'lr': [1e-3, 5e-4],\n",
    "        'hidden_dim1': [64, 128],\n",
    "        'dropout': [0.3, 0.5]\n",
    "    }\n",
    "\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "    best_f1 = 0.0  # Mudan√ßa: agora otimiza pelo F1 ao inv√©s da loss\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    print(f\"üîç Testando {len(combinations)} combina√ß√µes de hiperpar√¢metros...\")\n",
    "\n",
    "    for i, params in enumerate(combinations, 1):\n",
    "        print(f\"\\n[{i}/{len(combinations)}] Testando: {params}\")\n",
    "        \n",
    "        model = create_model(\n",
    "            input_dim=X_train.shape[1],\n",
    "            hidden_dim1=params['hidden_dim1'],\n",
    "            dropout=params['dropout'],\n",
    "            num_classes=num_classes\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Treina por 5 epochs\n",
    "        for epoch in range(5):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * X_batch.size(0)\n",
    "            epoch_loss = running_loss / len(train_loader.dataset)\n",
    "            print(f\"  Epoch {epoch+1} - Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Avalia√ß√£o no conjunto de valida√ß√£o (agora com F1)\n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        val_true_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                val_predictions.extend(predicted.cpu().numpy())\n",
    "                val_true_labels.extend(y_batch.cpu().numpy())\n",
    "        \n",
    "        # Calcula F1 na valida√ß√£o\n",
    "        val_f1 = f1_score(val_true_labels, val_predictions, average='weighted')\n",
    "        val_acc = accuracy_score(val_true_labels, val_predictions)\n",
    "        \n",
    "        print(f\"  Val Accuracy: {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_params = params\n",
    "            best_model = model.state_dict().copy()  # Salva o estado do modelo\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üèÜ MELHORES RESULTADOS NA VALIDA√á√ÉO:\")\n",
    "    print(f\"Melhores par√¢metros: {best_params}\")\n",
    "    print(f\"Melhor F1 na valida√ß√£o: {best_f1:.4f}\")\n",
    "\n",
    "    # Recria o melhor modelo para avalia√ß√£o final\n",
    "    final_model = create_model(\n",
    "        input_dim=X_train.shape[1],\n",
    "        hidden_dim1=best_params['hidden_dim1'],\n",
    "        dropout=best_params['dropout'],\n",
    "        num_classes=num_classes\n",
    "    ).to(device)\n",
    "    final_model.load_state_dict(best_model)\n",
    "\n",
    "    # Avalia√ß√£o final no conjunto de teste\n",
    "    print(\"\\nüìä AVALIA√á√ÉO FINAL NO TESTE:\")\n",
    "    final_model.eval()\n",
    "    test_predictions = []\n",
    "    test_true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = final_model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            test_predictions.extend(predicted.cpu().numpy())\n",
    "            test_true_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    # Calcula m√©tricas finais\n",
    "    test_accuracy = accuracy_score(test_true_labels, test_predictions)\n",
    "    test_f1_weighted = f1_score(test_true_labels, test_predictions, average='weighted')\n",
    "    test_f1_macro = f1_score(test_true_labels, test_predictions, average='macro')\n",
    "    \n",
    "    # Tempo total\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"üìà RESULTADOS FINAIS:\")\n",
    "    print(f\"Acur√°cia no teste: {test_accuracy:.4f}\")\n",
    "    print(f\"F1-Score (weighted): {test_f1_weighted:.4f}\")\n",
    "    print(f\"F1-Score (macro): {test_f1_macro:.4f}\")\n",
    "    print(f\"‚è±Ô∏è  Tempo total: {total_time:.2f} segundos ({total_time/60:.2f} minutos)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nüìã RELAT√ìRIO DETALHADO:\")\n",
    "    print(classification_report(test_true_labels, test_predictions))\n",
    "    \n",
    "    # Retorna as m√©tricas principais\n",
    "    return {\n",
    "        'accuracy': test_accuracy,\n",
    "        'f1_weighted': test_f1_weighted,\n",
    "        'f1_macro': test_f1_macro,\n",
    "        'best_params': best_params,\n",
    "        'total_time_seconds': total_time,\n",
    "        'total_time_minutes': total_time/60\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17a0890c-b05d-4af5-9018-9e678f9d3a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def create_model_2(input_dim, hidden_dim1, dropout, num_classes=2):\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.bn = nn.BatchNorm1d(input_dim)\n",
    "            self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.fc2 = nn.Linear(hidden_dim1, 32)\n",
    "            self.fc3 = nn.Linear(32, num_classes)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.bn(x)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.dropout(x)\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "\n",
    "    return Net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0096c96c-2887-48ed-bf3a-0f9c1dd29807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "def trainNNEmbOutra(\n",
    "    bertTrainEmbeddings, robertaTrainEmbeddings, electraTrainEmbeddings,\n",
    "    bertTestEmbeddings, robertaTestEmbeddings, electraTestEmbeddings,\n",
    "    trainLabels, testLabels,\n",
    "    num_classes,\n",
    "    val_size=0.2,\n",
    "    batch_size=32,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "):\n",
    "    # Inicia contagem de tempo\n",
    "    start_time = time.time()\n",
    "    print(\"üïê Iniciando experimento...\")\n",
    "    \n",
    "    # Concatena os logits das tr√™s redes\n",
    "    concatenated_logits = np.concatenate(\n",
    "        [bertTrainEmbeddings, robertaTrainEmbeddings, electraTrainEmbeddings], axis=1\n",
    "    )\n",
    "    concatenated_test_logits = np.concatenate(\n",
    "        [bertTestEmbeddings, robertaTestEmbeddings, electraTestEmbeddings], axis=1\n",
    "    )\n",
    "\n",
    "    train_labels = np.array(trainLabels)\n",
    "    test_labels = np.array(testLabels)\n",
    "\n",
    "    # Split treino/val a partir do conjunto de treino\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        concatenated_logits,\n",
    "        train_labels,\n",
    "        test_size=val_size,\n",
    "        stratify=train_labels,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Cria DataLoaders\n",
    "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "    val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
    "    test_dataset = TensorDataset(torch.tensor(concatenated_test_logits, dtype=torch.float32), torch.tensor(test_labels, dtype=torch.long))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    param_grid = {\n",
    "        'lr': [1e-3, 5e-4],\n",
    "        'hidden_dim1': [64, 128],\n",
    "        'dropout': [0.3, 0.5]\n",
    "    }\n",
    "\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "    best_f1 = 0.0  # Mudan√ßa: agora otimiza pelo F1 ao inv√©s da loss\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    \n",
    "    print(f\"üîç Testando {len(combinations)} combina√ß√µes de hiperpar√¢metros...\")\n",
    "\n",
    "    for i, params in enumerate(combinations, 1):\n",
    "        print(f\"\\n[{i}/{len(combinations)}] Testando: {params}\")\n",
    "        \n",
    "        model = create_model_2(\n",
    "            input_dim=X_train.shape[1],\n",
    "            hidden_dim1=params['hidden_dim1'],\n",
    "            dropout=params['dropout'],\n",
    "            num_classes=num_classes\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Treina por 5 epochs\n",
    "        for epoch in range(5):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * X_batch.size(0)\n",
    "            epoch_loss = running_loss / len(train_loader.dataset)\n",
    "            print(f\"  Epoch {epoch+1} - Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Avalia√ß√£o no conjunto de valida√ß√£o (agora com F1)\n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        val_true_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                val_predictions.extend(predicted.cpu().numpy())\n",
    "                val_true_labels.extend(y_batch.cpu().numpy())\n",
    "        \n",
    "        # Calcula F1 na valida√ß√£o\n",
    "        val_f1 = f1_score(val_true_labels, val_predictions, average='weighted')\n",
    "        val_acc = accuracy_score(val_true_labels, val_predictions)\n",
    "        \n",
    "        print(f\"  Val Accuracy: {val_acc:.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_params = params\n",
    "            best_model = model.state_dict().copy()  # Salva o estado do modelo\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üèÜ MELHORES RESULTADOS NA VALIDA√á√ÉO:\")\n",
    "    print(f\"Melhores par√¢metros: {best_params}\")\n",
    "    print(f\"Melhor F1 na valida√ß√£o: {best_f1:.4f}\")\n",
    "\n",
    "    # Recria o melhor modelo para avalia√ß√£o final\n",
    "    final_model = create_model_2(\n",
    "        input_dim=X_train.shape[1],\n",
    "        hidden_dim1=best_params['hidden_dim1'],\n",
    "        dropout=best_params['dropout'],\n",
    "        num_classes=num_classes\n",
    "    ).to(device)\n",
    "    final_model.load_state_dict(best_model)\n",
    "\n",
    "    # Avalia√ß√£o final no conjunto de teste\n",
    "    print(\"\\nüìä AVALIA√á√ÉO FINAL NO TESTE:\")\n",
    "    final_model.eval()\n",
    "    test_predictions = []\n",
    "    test_true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = final_model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            test_predictions.extend(predicted.cpu().numpy())\n",
    "            test_true_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    # Calcula m√©tricas finais\n",
    "    test_accuracy = accuracy_score(test_true_labels, test_predictions)\n",
    "    test_f1_weighted = f1_score(test_true_labels, test_predictions, average='weighted')\n",
    "    test_f1_macro = f1_score(test_true_labels, test_predictions, average='macro')\n",
    "    \n",
    "    # Tempo total\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"üìà RESULTADOS FINAIS:\")\n",
    "    print(f\"Acur√°cia no teste: {test_accuracy:.4f}\")\n",
    "    print(f\"F1-Score (weighted): {test_f1_weighted:.4f}\")\n",
    "    print(f\"F1-Score (macro): {test_f1_macro:.4f}\")\n",
    "    print(f\"‚è±Ô∏è  Tempo total: {total_time:.2f} segundos ({total_time/60:.2f} minutos)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nüìã RELAT√ìRIO DETALHADO:\")\n",
    "    print(classification_report(test_true_labels, test_predictions))\n",
    "    \n",
    "    # Retorna as m√©tricas principais\n",
    "    return {\n",
    "        'accuracy': test_accuracy,\n",
    "        'f1_weighted': test_f1_weighted,\n",
    "        'f1_macro': test_f1_macro,\n",
    "        'best_params': best_params,\n",
    "        'total_time_seconds': total_time,\n",
    "        'total_time_minutes': total_time/60\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4070d447-1b28-42cd-89e5-e72250288624",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_logits_file = np.load('logits_google-bert/bert-base-uncased_banking77_train_bert-base-uncased.npz')\n",
    "roberta_logits_file = np.load('logits_roberta-base_banking77_train_roberta-base.npz')\n",
    "electra_logits_file = np.load('logits_google/electra-base-discriminator_banking77_train_electra-base-discriminator.npz')\n",
    "\n",
    "\n",
    "bert_logits_test_file = np.load('logits_google-bert/bert-base-uncased_banking77_test_bert-base-uncased.npz')\n",
    "roberta_logits_test_file = np.load('logits_roberta-base_banking77_test_roberta-base.npz')\n",
    "electra_logits_test_file = np.load('logits_google/electra-base-discriminator_banking77_test_electra-base-discriminator.npz')\n",
    "\n",
    "bertTrainLogits = bert_logits_file\n",
    "robertaTrainLogits = roberta_logits_file\n",
    "electraTrainLogits = electra_logits_file\n",
    "\n",
    "bertTestLogits = bert_logits_test_file\n",
    "robertaTestLogits = roberta_logits_test_file\n",
    "electraTestLogits = electra_logits_test_file\n",
    "\n",
    "trainLabels = bert_logits_file['labels']\n",
    "testLabels = bert_logits_test_file['labels']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd5ca1ef-2c92-488a-87b1-cb55d6e8fca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTADOS INDIVIDUAIS:\n",
      "BERT     - Acc: 0.9099 | F1: 0.9096\n",
      "RoBERTa  - Acc: 0.9249 | F1: 0.9248\n",
      "ELECTRA  - Acc: 0.8713 | F1: 0.8597\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# BERT\n",
    "bert_preds = np.argmax(bertTestLogits['logits'], axis=1)\n",
    "bert_acc = accuracy_score(testLabels, bert_preds)\n",
    "bert_f1 = f1_score(testLabels, bert_preds, average='weighted')\n",
    "\n",
    "# RoBERTa  \n",
    "roberta_preds = np.argmax(robertaTestLogits['logits'], axis=1)\n",
    "roberta_acc = accuracy_score(testLabels, roberta_preds)\n",
    "roberta_f1 = f1_score(testLabels, roberta_preds, average='weighted')\n",
    "\n",
    "# ELECTRA\n",
    "electra_preds = np.argmax(electraTestLogits['logits'], axis=1)\n",
    "electra_acc = accuracy_score(testLabels, electra_preds)\n",
    "electra_f1 = f1_score(testLabels, electra_preds, average='weighted')\n",
    "\n",
    "# Resultados\n",
    "print(\"RESULTADOS INDIVIDUAIS:\")\n",
    "print(f\"BERT     - Acc: {bert_acc:.4f} | F1: {bert_f1:.4f}\")\n",
    "print(f\"RoBERTa  - Acc: {roberta_acc:.4f} | F1: {roberta_f1:.4f}\")\n",
    "print(f\"ELECTRA  - Acc: {electra_acc:.4f} | F1: {electra_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12267608-78ad-4e18-8b1e-febb13e0d908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score Results:\n",
      "BERT:           0.9096\n",
      "RoBERTa:        0.9248\n",
      "ELECTRA:        0.8597\n",
      "Voto Majorit√°rio: 0.9150\n",
      "M√©dia Logits:     0.9243\n",
      "Or√°culo:          0.9432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bert': 0.9095663871205903,\n",
       " 'roberta': 0.924827143295883,\n",
       " 'electra': 0.8597029962650725,\n",
       " 'majority': 0.91497325574669,\n",
       " 'avg_logits': 0.92433654042191,\n",
       " 'oracle': 0.9431606486408773}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "def avaliar_ensemble_logits(logits_bert, logits_roberta, logits_electra, true_labels):\n",
    "    \"\"\"\n",
    "    Avalia ensemble de logits com diferentes estrat√©gias\n",
    "    \"\"\"\n",
    "    # Predi√ß√µes individuais\n",
    "    pred_bert = np.argmax(logits_bert, axis=1)\n",
    "    pred_roberta = np.argmax(logits_roberta, axis=1)\n",
    "    pred_electra = np.argmax(logits_electra, axis=1)\n",
    "    \n",
    "    # 1. F1 individual\n",
    "    f1_bert = f1_score(true_labels, pred_bert, average='weighted')\n",
    "    f1_roberta = f1_score(true_labels, pred_roberta, average='weighted')\n",
    "    f1_electra = f1_score(true_labels, pred_electra, average='weighted')\n",
    "    \n",
    "    # 2. Voto majorit√°rio\n",
    "    votes = np.column_stack([pred_bert, pred_roberta, pred_electra])\n",
    "    pred_majority = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=1, arr=votes)\n",
    "    f1_majority = f1_score(true_labels, pred_majority, average='weighted')\n",
    "    \n",
    "    # 3. M√©dia dos logits\n",
    "    logits_avg = (logits_bert + logits_roberta + logits_electra) / 3\n",
    "    pred_avg = np.argmax(logits_avg, axis=1)\n",
    "    f1_avg = f1_score(true_labels, pred_avg, average='weighted')\n",
    "    \n",
    "    # 4. Or√°culo (melhor predi√ß√£o para cada amostra)\n",
    "    all_preds = np.column_stack([pred_bert, pred_roberta, pred_electra])\n",
    "    pred_oracle = []\n",
    "    for i in range(len(true_labels)):\n",
    "        # Para cada amostra, pega a predi√ß√£o que est√° certa (se houver)\n",
    "        correct_preds = all_preds[i][all_preds[i] == true_labels[i]]\n",
    "        if len(correct_preds) > 0:\n",
    "            pred_oracle.append(correct_preds[0])\n",
    "        else:\n",
    "            # Se nenhuma est√° certa, usa voto majorit√°rio\n",
    "            pred_oracle.append(pred_majority[i])\n",
    "    \n",
    "    f1_oracle = f1_score(true_labels, pred_oracle, average='weighted')\n",
    "    \n",
    "    print(\"F1-Score Results:\")\n",
    "    print(f\"BERT:           {f1_bert:.4f}\")\n",
    "    print(f\"RoBERTa:        {f1_roberta:.4f}\")\n",
    "    print(f\"ELECTRA:        {f1_electra:.4f}\")\n",
    "    print(f\"Voto Majorit√°rio: {f1_majority:.4f}\")\n",
    "    print(f\"M√©dia Logits:     {f1_avg:.4f}\")\n",
    "    print(f\"Or√°culo:          {f1_oracle:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'bert': f1_bert,\n",
    "        'roberta': f1_roberta, \n",
    "        'electra': f1_electra,\n",
    "        'majority': f1_majority,\n",
    "        'avg_logits': f1_avg,\n",
    "        'oracle': f1_oracle\n",
    "    }\n",
    "\n",
    "# Exemplo de uso:\n",
    "avaliar_ensemble_logits(bertTestLogits['logits'], robertaTestLogits['logits'], electraTestLogits['logits'], bertTestLogits['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "528c2a2e-4a1e-4f5e-96f7-f0e723c75b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïê Iniciando experimento NN com Logits...\n",
      "üìä Shape dos logits concatenados: (7994, 231)\n",
      "üìä N√∫mero de classes: 77\n",
      "üîç Testando 8 combina√ß√µes de hiperpar√¢metros...\n",
      "\n",
      "[1/8] Testando: {'lr': 0.001, 'hidden_dim1': 64, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 1.9866\n",
      "  Epoch 2 - Loss: 0.2790\n",
      "  Epoch 3 - Loss: 0.1793\n",
      "  Epoch 4 - Loss: 0.1503\n",
      "  Epoch 5 - Loss: 0.1381\n",
      "  Val Accuracy: 0.9775 | Val F1: 0.9775\n",
      "\n",
      "[2/8] Testando: {'lr': 0.001, 'hidden_dim1': 64, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 2.4583\n",
      "  Epoch 2 - Loss: 0.6010\n",
      "  Epoch 3 - Loss: 0.3742\n",
      "  Epoch 4 - Loss: 0.3151\n",
      "  Epoch 5 - Loss: 0.2655\n",
      "  Val Accuracy: 0.9787 | Val F1: 0.9787\n",
      "\n",
      "[3/8] Testando: {'lr': 0.001, 'hidden_dim1': 128, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 1.5852\n",
      "  Epoch 2 - Loss: 0.1672\n",
      "  Epoch 3 - Loss: 0.1289\n",
      "  Epoch 4 - Loss: 0.1161\n",
      "  Epoch 5 - Loss: 0.1007\n",
      "  Val Accuracy: 0.9750 | Val F1: 0.9748\n",
      "\n",
      "[4/8] Testando: {'lr': 0.001, 'hidden_dim1': 128, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 1.9058\n",
      "  Epoch 2 - Loss: 0.2895\n",
      "  Epoch 3 - Loss: 0.1946\n",
      "  Epoch 4 - Loss: 0.1641\n",
      "  Epoch 5 - Loss: 0.1397\n",
      "  Val Accuracy: 0.9781 | Val F1: 0.9781\n",
      "\n",
      "[5/8] Testando: {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 3.0606\n",
      "  Epoch 2 - Loss: 0.7106\n",
      "  Epoch 3 - Loss: 0.3031\n",
      "  Epoch 4 - Loss: 0.2101\n",
      "  Epoch 5 - Loss: 0.1788\n",
      "  Val Accuracy: 0.9794 | Val F1: 0.9794\n",
      "\n",
      "[6/8] Testando: {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 3.3951\n",
      "  Epoch 2 - Loss: 1.2614\n",
      "  Epoch 3 - Loss: 0.6525\n",
      "  Epoch 4 - Loss: 0.4919\n",
      "  Epoch 5 - Loss: 0.3797\n",
      "  Val Accuracy: 0.9744 | Val F1: 0.9743\n",
      "\n",
      "[7/8] Testando: {'lr': 0.0005, 'hidden_dim1': 128, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 2.5664\n",
      "  Epoch 2 - Loss: 0.3501\n",
      "  Epoch 3 - Loss: 0.1591\n",
      "  Epoch 4 - Loss: 0.1319\n",
      "  Epoch 5 - Loss: 0.1185\n",
      "  Val Accuracy: 0.9787 | Val F1: 0.9787\n",
      "\n",
      "[8/8] Testando: {'lr': 0.0005, 'hidden_dim1': 128, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 2.8092\n",
      "  Epoch 2 - Loss: 0.6283\n",
      "  Epoch 3 - Loss: 0.2992\n",
      "  Epoch 4 - Loss: 0.2112\n",
      "  Epoch 5 - Loss: 0.1761\n",
      "  Val Accuracy: 0.9769 | Val F1: 0.9768\n",
      "\n",
      "============================================================\n",
      "üèÜ MELHORES RESULTADOS NA VALIDA√á√ÉO:\n",
      "Melhores par√¢metros: {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.3}\n",
      "Melhor F1 na valida√ß√£o: 0.9794\n",
      "\n",
      "üìä AVALIA√á√ÉO FINAL NO TESTE:\n",
      "============================================================\n",
      "üìà RESULTADOS FINAIS:\n",
      "Acur√°cia no teste: 0.9207\n",
      "F1-Score (weighted): 0.9210\n",
      "F1-Score (macro): 0.9209\n",
      "‚è±Ô∏è  Tempo total: 4.13 segundos (0.07 minutos)\n",
      "============================================================\n",
      "\n",
      "üìã RELAT√ìRIO DETALHADO:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97        40\n",
      "           1       1.00      1.00      1.00        40\n",
      "           2       1.00      1.00      1.00        40\n",
      "           3       0.95      1.00      0.97        39\n",
      "           4       0.97      0.88      0.92        40\n",
      "           5       0.64      0.80      0.71        40\n",
      "           6       1.00      0.93      0.96        40\n",
      "           7       0.90      0.90      0.90        40\n",
      "           8       1.00      0.95      0.97        40\n",
      "           9       1.00      1.00      1.00        40\n",
      "          10       1.00      0.88      0.93        40\n",
      "          11       0.88      0.88      0.88        40\n",
      "          12       0.90      0.93      0.91        40\n",
      "          13       1.00      0.97      0.99        40\n",
      "          14       0.91      0.97      0.94        40\n",
      "          15       0.90      0.95      0.93        40\n",
      "          16       0.94      0.85      0.89        40\n",
      "          17       0.97      0.95      0.96        40\n",
      "          18       1.00      0.78      0.87        40\n",
      "          19       0.95      0.95      0.95        40\n",
      "          20       0.91      0.97      0.94        40\n",
      "          21       1.00      1.00      1.00        40\n",
      "          22       0.92      0.88      0.90        40\n",
      "          23       1.00      0.88      0.93        40\n",
      "          24       0.95      0.97      0.96        40\n",
      "          25       0.80      0.93      0.86        40\n",
      "          26       0.74      1.00      0.85        40\n",
      "          27       1.00      0.72      0.84        40\n",
      "          28       0.88      0.90      0.89        40\n",
      "          29       0.95      0.90      0.92        40\n",
      "          30       1.00      1.00      1.00        40\n",
      "          31       0.97      0.93      0.95        40\n",
      "          32       0.91      1.00      0.95        40\n",
      "          33       0.88      0.95      0.91        39\n",
      "          34       0.93      0.97      0.95        40\n",
      "          35       0.88      0.90      0.89        40\n",
      "          36       0.92      0.85      0.88        40\n",
      "          37       0.89      0.82      0.86        40\n",
      "          38       0.93      1.00      0.96        40\n",
      "          39       0.95      0.93      0.94        40\n",
      "          40       0.81      0.97      0.89        40\n",
      "          41       0.80      0.97      0.88        40\n",
      "          42       0.93      0.97      0.95        40\n",
      "          43       0.86      0.90      0.88        40\n",
      "          44       1.00      1.00      1.00        40\n",
      "          45       0.93      0.93      0.93        40\n",
      "          46       0.95      0.97      0.96        40\n",
      "          47       0.95      0.90      0.92        40\n",
      "          48       0.86      0.77      0.81        39\n",
      "          49       0.92      0.90      0.91        40\n",
      "          50       0.93      0.97      0.95        40\n",
      "          51       1.00      0.93      0.96        40\n",
      "          52       0.95      0.97      0.96        40\n",
      "          53       0.88      0.88      0.88        40\n",
      "          54       0.86      0.93      0.89        40\n",
      "          55       1.00      1.00      1.00        40\n",
      "          56       0.92      0.88      0.90        40\n",
      "          57       0.86      0.95      0.90        40\n",
      "          58       0.97      0.93      0.95        40\n",
      "          59       0.86      0.95      0.90        40\n",
      "          60       0.93      0.97      0.95        40\n",
      "          61       0.89      0.82      0.86        40\n",
      "          62       0.72      0.78      0.75        40\n",
      "          63       0.93      1.00      0.96        40\n",
      "          64       0.95      0.95      0.95        40\n",
      "          65       0.91      0.78      0.84        40\n",
      "          66       0.86      0.82      0.84        39\n",
      "          67       0.85      0.85      0.85        40\n",
      "          68       0.95      0.95      0.95        40\n",
      "          69       0.84      0.90      0.87        40\n",
      "          70       1.00      1.00      1.00        40\n",
      "          71       1.00      1.00      1.00        40\n",
      "          72       1.00      0.82      0.90        40\n",
      "          73       1.00      0.93      0.96        40\n",
      "          74       0.89      0.82      0.86        40\n",
      "          75       1.00      0.90      0.95        40\n",
      "          76       0.97      0.90      0.94        40\n",
      "\n",
      "    accuracy                           0.92      3076\n",
      "   macro avg       0.93      0.92      0.92      3076\n",
      "weighted avg       0.93      0.92      0.92      3076\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9206762028608583,\n",
       " 'f1_weighted': 0.9209640463056272,\n",
       " 'f1_macro': 0.9209178255704519,\n",
       " 'best_params': {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.3},\n",
       " 'total_time_seconds': 4.127015590667725,\n",
       " 'total_time_minutes': 0.06878359317779541}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bertTrainLogits = bert_logits_file\n",
    "robertaTrainLogits = roberta_logits_file\n",
    "electraTrainLogits = electra_logits_file\n",
    "\n",
    "bertTestLogits = bert_logits_test_file\n",
    "robertaTestLogits = roberta_logits_test_file\n",
    "electraTestLogits = electra_logits_test_file\n",
    "\n",
    "trainLabels = bert_logits_file['labels']\n",
    "testLabels = bert_logits_test_file['labels']\n",
    "\n",
    "trainNNLogits(\n",
    "    bertTrainLogits, robertaTrainLogits, electraTrainLogits,\n",
    "    bertTestLogits, robertaTestLogits, electraTestLogits,\n",
    "    trainLabels, testLabels,\n",
    "    num_classes=77,\n",
    "    val_size=0.2,\n",
    "    batch_size=32,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2b9acce-6ece-4ea0-8aee-d957d3b8a5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïê Iniciando experimento XGBoost...\n",
      "üìä Shape dos dados concatenados: (7994, 231)\n",
      "üìä N√∫mero de classes: 77\n",
      "üîç Testando 72 combina√ß√µes de hiperpar√¢metros com CV=3...\n",
      "‚öôÔ∏è  Rodando GridSearch para XGBoost...\n",
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainXGBoostOnly\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mbertTrainLogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrobertaTrainLogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melectraTrainLogits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbertTestLogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrobertaTestLogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melectraTestLogits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainLabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestLabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m77\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 89\u001b[39m, in \u001b[36mtrainXGBoostOnly\u001b[39m\u001b[34m(bertTrainLogits, robertaTrainLogits, electraTrainLogits, bertTestLogits, robertaTestLogits, electraTestLogits, trainLabels, testLabels, num_classes, val_size, random_state)\u001b[39m\n\u001b[32m     86\u001b[39m grid_start_time = time.time()\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Usa apenas treino para GridSearch (valida√ß√£o fica separada para avalia√ß√£o)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m grid_end_time = time.time()\n\u001b[32m     92\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ GridSearch conclu√≠do em \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid_end_time\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mgrid_start_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m segundos\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/pytorch_311_env/lib/python3.11/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/pytorch_311_env/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1051\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1045\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1046\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1047\u001b[39m     )\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1055\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/pytorch_311_env/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1605\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1603\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1604\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1605\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/pytorch_311_env/lib/python3.11/site-packages/sklearn/model_selection/_search.py:997\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    993\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    994\u001b[39m         )\n\u001b[32m    995\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1017\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1018\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1019\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1020\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/pytorch_311_env/lib/python3.11/site-packages/sklearn/utils/parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/pytorch_311_env/lib/python3.11/site-packages/joblib/parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/pytorch_311_env/lib/python3.11/site-packages/joblib/parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/pytorch_311_env/lib/python3.11/site-packages/joblib/parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainXGBoostOnly( bertTrainLogits, robertaTrainLogits, electraTrainLogits,\n",
    "    bertTestLogits, robertaTestLogits, electraTestLogits,\n",
    "    trainLabels, testLabels,\n",
    "    num_classes=77,\n",
    "    val_size=0.2,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ed51c9b-973d-4391-8585-dedeac90eabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMOTION - Shapes:\n",
      "Roberta - Train: (7994, 768), Test: (3076, 768)\n",
      "BERT - Train: (7994, 768), Test: (3076, 768)\n",
      "Electra - Train: (7994, 768), Test: (3076, 768)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# EMOTION - ROBERTA\n",
    "train_data = np.load('embeddings_roberta-base_banking77_train_roberta-base.npz')\n",
    "test_data = np.load('embeddings_roberta-base_banking77_test_roberta-base.npz')\n",
    "X_train_emotion_roberta = train_data['embeddings']\n",
    "y_train_emotion_roberta = train_data['labels']\n",
    "X_test_emotion_roberta = test_data['embeddings'] \n",
    "y_test_emotion_roberta = test_data['labels']\n",
    "\n",
    "# EMOTION - BERT\n",
    "train_data = np.load('embeddings_google-bert_bert-base-uncased_banking77_train_bert-base-uncased.npz')\n",
    "test_data = np.load('embeddings_google-bert_bert-base-uncased_banking77_test_bert-base-uncased.npz')\n",
    "X_train_emotion_bert = train_data['embeddings']\n",
    "y_train_emotion_bert = train_data['labels']\n",
    "X_test_emotion_bert = test_data['embeddings'] \n",
    "y_test_emotion_bert = test_data['labels']\n",
    "\n",
    "# EMOTION - ELECTRA\n",
    "train_data = np.load('embeddings_google_electra-base-discriminator_banking77_train_electra-base-discriminator.npz')\n",
    "test_data = np.load('embeddings_google_electra-base-discriminator_banking77_test_electra-base-discriminator.npz')\n",
    "X_train_emotion_electra = train_data['embeddings']\n",
    "y_train_emotion_electra = train_data['labels']\n",
    "X_test_emotion_electra = test_data['embeddings'] \n",
    "y_test_emotion_electra = test_data['labels']\n",
    "\n",
    "print(\"EMOTION - Shapes:\")\n",
    "print(f\"Roberta - Train: {X_train_emotion_roberta.shape}, Test: {X_test_emotion_roberta.shape}\")\n",
    "print(f\"BERT - Train: {X_train_emotion_bert.shape}, Test: {X_test_emotion_bert.shape}\")\n",
    "print(f\"Electra - Train: {X_train_emotion_electra.shape}, Test: {X_test_emotion_electra.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041e0275-f622-4558-96b9-e212cd904f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainNNEmb(\n",
    "    X_train_emotion_bert, X_train_emotion_roberta, X_train_emotion_electra,\n",
    "    X_test_emotion_bert, X_test_emotion_roberta, X_test_emotion_electra,\n",
    "    y_train_emotion_roberta, y_test_emotion_roberta,\n",
    "    num_classes=77,  # emotion tem 6 classes\n",
    "    val_size=0.2,\n",
    "    batch_size=32,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddad2971-edfc-44d3-9e94-99a7f1c5b4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïê Iniciando experimento...\n",
      "üîç Testando 8 combina√ß√µes de hiperpar√¢metros...\n",
      "\n",
      "[1/8] Testando: {'lr': 0.001, 'hidden_dim1': 64, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 2.6130\n",
      "  Epoch 2 - Loss: 0.3547\n",
      "  Epoch 3 - Loss: 0.2048\n",
      "  Epoch 4 - Loss: 0.1561\n",
      "  Epoch 5 - Loss: 0.1381\n",
      "  Val Accuracy: 0.9744 | Val F1: 0.9744\n",
      "\n",
      "[2/8] Testando: {'lr': 0.001, 'hidden_dim1': 64, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 3.0077\n",
      "  Epoch 2 - Loss: 0.7938\n",
      "  Epoch 3 - Loss: 0.4341\n",
      "  Epoch 4 - Loss: 0.3382\n",
      "  Epoch 5 - Loss: 0.2791\n",
      "  Val Accuracy: 0.9762 | Val F1: 0.9762\n",
      "\n",
      "[3/8] Testando: {'lr': 0.001, 'hidden_dim1': 128, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 2.2085\n",
      "  Epoch 2 - Loss: 0.2008\n",
      "  Epoch 3 - Loss: 0.1283\n",
      "  Epoch 4 - Loss: 0.1084\n",
      "  Epoch 5 - Loss: 0.0957\n",
      "  Val Accuracy: 0.9762 | Val F1: 0.9762\n",
      "\n",
      "[4/8] Testando: {'lr': 0.001, 'hidden_dim1': 128, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 2.5049\n",
      "  Epoch 2 - Loss: 0.3491\n",
      "  Epoch 3 - Loss: 0.1990\n",
      "  Epoch 4 - Loss: 0.1585\n",
      "  Epoch 5 - Loss: 0.1309\n",
      "  Val Accuracy: 0.9750 | Val F1: 0.9750\n",
      "\n",
      "[5/8] Testando: {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 3.6717\n",
      "  Epoch 2 - Loss: 1.3815\n",
      "  Epoch 3 - Loss: 0.4526\n",
      "  Epoch 4 - Loss: 0.2677\n",
      "  Epoch 5 - Loss: 0.2088\n",
      "  Val Accuracy: 0.9769 | Val F1: 0.9769\n",
      "\n",
      "[6/8] Testando: {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 3.7940\n",
      "  Epoch 2 - Loss: 1.8503\n",
      "  Epoch 3 - Loss: 0.8497\n",
      "  Epoch 4 - Loss: 0.5422\n",
      "  Epoch 5 - Loss: 0.4256\n",
      "  Val Accuracy: 0.9744 | Val F1: 0.9743\n",
      "\n",
      "[7/8] Testando: {'lr': 0.0005, 'hidden_dim1': 128, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 3.2738\n",
      "  Epoch 2 - Loss: 0.6679\n",
      "  Epoch 3 - Loss: 0.2229\n",
      "  Epoch 4 - Loss: 0.1519\n",
      "  Epoch 5 - Loss: 0.1268\n",
      "  Val Accuracy: 0.9725 | Val F1: 0.9726\n",
      "\n",
      "[8/8] Testando: {'lr': 0.0005, 'hidden_dim1': 128, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 3.4456\n",
      "  Epoch 2 - Loss: 1.0388\n",
      "  Epoch 3 - Loss: 0.3797\n",
      "  Epoch 4 - Loss: 0.2451\n",
      "  Epoch 5 - Loss: 0.1942\n",
      "  Val Accuracy: 0.9744 | Val F1: 0.9744\n",
      "\n",
      "============================================================\n",
      "üèÜ MELHORES RESULTADOS NA VALIDA√á√ÉO:\n",
      "Melhores par√¢metros: {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.3}\n",
      "Melhor F1 na valida√ß√£o: 0.9769\n",
      "\n",
      "üìä AVALIA√á√ÉO FINAL NO TESTE:\n",
      "============================================================\n",
      "üìà RESULTADOS FINAIS:\n",
      "Acur√°cia no teste: 0.9223\n",
      "F1-Score (weighted): 0.9224\n",
      "F1-Score (macro): 0.9223\n",
      "‚è±Ô∏è  Tempo total: 5.36 segundos (0.09 minutos)\n",
      "============================================================\n",
      "\n",
      "üìã RELAT√ìRIO DETALHADO:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97        40\n",
      "           1       1.00      1.00      1.00        40\n",
      "           2       1.00      1.00      1.00        40\n",
      "           3       0.95      1.00      0.97        39\n",
      "           4       1.00      0.88      0.93        40\n",
      "           5       0.81      0.72      0.76        40\n",
      "           6       0.97      0.93      0.95        40\n",
      "           7       0.92      0.90      0.91        40\n",
      "           8       1.00      0.97      0.99        40\n",
      "           9       0.97      0.97      0.97        40\n",
      "          10       0.97      0.88      0.92        40\n",
      "          11       0.88      0.88      0.88        40\n",
      "          12       0.90      0.93      0.91        40\n",
      "          13       1.00      0.97      0.99        40\n",
      "          14       0.91      0.97      0.94        40\n",
      "          15       0.90      0.95      0.93        40\n",
      "          16       0.90      0.88      0.89        40\n",
      "          17       0.97      0.95      0.96        40\n",
      "          18       0.92      0.85      0.88        40\n",
      "          19       0.97      0.95      0.96        40\n",
      "          20       0.87      0.97      0.92        40\n",
      "          21       0.97      0.97      0.97        40\n",
      "          22       0.90      0.93      0.91        40\n",
      "          23       1.00      0.90      0.95        40\n",
      "          24       0.93      1.00      0.96        40\n",
      "          25       0.87      0.85      0.86        40\n",
      "          26       0.75      1.00      0.86        40\n",
      "          27       0.97      0.72      0.83        40\n",
      "          28       0.95      0.88      0.91        40\n",
      "          29       0.95      0.90      0.92        40\n",
      "          30       0.98      1.00      0.99        40\n",
      "          31       0.97      0.93      0.95        40\n",
      "          32       0.91      0.97      0.94        40\n",
      "          33       0.86      0.95      0.90        39\n",
      "          34       0.93      0.97      0.95        40\n",
      "          35       0.86      0.90      0.88        40\n",
      "          36       0.92      0.85      0.88        40\n",
      "          37       0.87      0.82      0.85        40\n",
      "          38       0.93      1.00      0.96        40\n",
      "          39       0.95      0.97      0.96        40\n",
      "          40       0.78      0.97      0.87        40\n",
      "          41       0.86      0.95      0.90        40\n",
      "          42       1.00      0.97      0.99        40\n",
      "          43       0.86      0.90      0.88        40\n",
      "          44       0.95      1.00      0.98        40\n",
      "          45       0.97      0.88      0.92        40\n",
      "          46       1.00      0.97      0.99        40\n",
      "          47       0.97      0.93      0.95        40\n",
      "          48       0.90      0.69      0.78        39\n",
      "          49       0.97      0.88      0.92        40\n",
      "          50       0.89      0.97      0.93        40\n",
      "          51       1.00      0.97      0.99        40\n",
      "          52       1.00      0.97      0.99        40\n",
      "          53       0.78      0.95      0.85        40\n",
      "          54       0.90      0.90      0.90        40\n",
      "          55       0.93      1.00      0.96        40\n",
      "          56       0.92      0.90      0.91        40\n",
      "          57       0.97      0.95      0.96        40\n",
      "          58       0.95      0.93      0.94        40\n",
      "          59       0.87      0.97      0.92        40\n",
      "          60       0.97      0.97      0.97        40\n",
      "          61       0.95      0.88      0.91        40\n",
      "          62       0.73      0.80      0.76        40\n",
      "          63       0.95      1.00      0.98        40\n",
      "          64       0.97      0.95      0.96        40\n",
      "          65       0.89      0.78      0.83        40\n",
      "          66       0.74      0.90      0.81        39\n",
      "          67       0.76      0.93      0.83        40\n",
      "          68       0.97      0.93      0.95        40\n",
      "          69       0.82      0.90      0.86        40\n",
      "          70       1.00      1.00      1.00        40\n",
      "          71       1.00      1.00      1.00        40\n",
      "          72       1.00      0.75      0.86        40\n",
      "          73       1.00      0.93      0.96        40\n",
      "          74       0.86      0.80      0.83        40\n",
      "          75       1.00      0.90      0.95        40\n",
      "          76       0.97      0.93      0.95        40\n",
      "\n",
      "    accuracy                           0.92      3076\n",
      "   macro avg       0.93      0.92      0.92      3076\n",
      "weighted avg       0.93      0.92      0.92      3076\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9223016905071522,\n",
       " 'f1_weighted': 0.9223716513605886,\n",
       " 'f1_macro': 0.9223016885693458,\n",
       " 'best_params': {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.3},\n",
       " 'total_time_seconds': 5.358517646789551,\n",
       " 'total_time_minutes': 0.08930862744649251}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainNNEmbL2(\n",
    "    X_train_emotion_bert, X_train_emotion_roberta, X_train_emotion_electra,\n",
    "    X_test_emotion_bert, X_test_emotion_roberta, X_test_emotion_electra,\n",
    "    y_train_emotion_roberta, y_test_emotion_roberta,\n",
    "    num_classes=77, \n",
    "    val_size=0.2,\n",
    "    batch_size=32,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7a74ee4-cb03-4c21-9e8b-ccafffe1f06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïê Iniciando experimento...\n",
      "üîç Testando 8 combina√ß√µes de hiperpar√¢metros...\n",
      "\n",
      "[1/8] Testando: {'lr': 0.001, 'hidden_dim1': 64, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 1.0102\n",
      "  Epoch 2 - Loss: 0.1989\n",
      "  Epoch 3 - Loss: 0.1774\n",
      "  Epoch 4 - Loss: 0.1282\n",
      "  Epoch 5 - Loss: 0.1182\n",
      "  Val Accuracy: 0.9794 | Val F1: 0.9793\n",
      "\n",
      "[2/8] Testando: {'lr': 0.001, 'hidden_dim1': 64, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 1.4355\n",
      "  Epoch 2 - Loss: 0.3787\n",
      "  Epoch 3 - Loss: 0.3155\n",
      "  Epoch 4 - Loss: 0.2741\n",
      "  Epoch 5 - Loss: 0.2549\n",
      "  Val Accuracy: 0.9787 | Val F1: 0.9787\n",
      "\n",
      "[3/8] Testando: {'lr': 0.001, 'hidden_dim1': 128, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 0.7013\n",
      "  Epoch 2 - Loss: 0.1355\n",
      "  Epoch 3 - Loss: 0.1243\n",
      "  Epoch 4 - Loss: 0.0992\n",
      "  Epoch 5 - Loss: 0.0979\n",
      "  Val Accuracy: 0.9775 | Val F1: 0.9775\n",
      "\n",
      "[4/8] Testando: {'lr': 0.001, 'hidden_dim1': 128, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 0.9221\n",
      "  Epoch 2 - Loss: 0.2102\n",
      "  Epoch 3 - Loss: 0.1747\n",
      "  Epoch 4 - Loss: 0.1595\n",
      "  Epoch 5 - Loss: 0.1438\n",
      "  Val Accuracy: 0.9725 | Val F1: 0.9724\n",
      "\n",
      "[5/8] Testando: {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 1.5977\n",
      "  Epoch 2 - Loss: 0.2351\n",
      "  Epoch 3 - Loss: 0.1773\n",
      "  Epoch 4 - Loss: 0.1398\n",
      "  Epoch 5 - Loss: 0.1297\n",
      "  Val Accuracy: 0.9769 | Val F1: 0.9767\n",
      "\n",
      "[6/8] Testando: {'lr': 0.0005, 'hidden_dim1': 64, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 2.1191\n",
      "  Epoch 2 - Loss: 0.5447\n",
      "  Epoch 3 - Loss: 0.3770\n",
      "  Epoch 4 - Loss: 0.3139\n",
      "  Epoch 5 - Loss: 0.2484\n",
      "  Val Accuracy: 0.9781 | Val F1: 0.9781\n",
      "\n",
      "[7/8] Testando: {'lr': 0.0005, 'hidden_dim1': 128, 'dropout': 0.3}\n",
      "  Epoch 1 - Loss: 1.1200\n",
      "  Epoch 2 - Loss: 0.1427\n",
      "  Epoch 3 - Loss: 0.1108\n",
      "  Epoch 4 - Loss: 0.0986\n",
      "  Epoch 5 - Loss: 0.0819\n",
      "  Val Accuracy: 0.9781 | Val F1: 0.9780\n",
      "\n",
      "[8/8] Testando: {'lr': 0.0005, 'hidden_dim1': 128, 'dropout': 0.5}\n",
      "  Epoch 1 - Loss: 1.4804\n",
      "  Epoch 2 - Loss: 0.2299\n",
      "  Epoch 3 - Loss: 0.1732\n",
      "  Epoch 4 - Loss: 0.1499\n",
      "  Epoch 5 - Loss: 0.1266\n",
      "  Val Accuracy: 0.9794 | Val F1: 0.9793\n",
      "\n",
      "============================================================\n",
      "üèÜ MELHORES RESULTADOS NA VALIDA√á√ÉO:\n",
      "Melhores par√¢metros: {'lr': 0.0005, 'hidden_dim1': 128, 'dropout': 0.5}\n",
      "Melhor F1 na valida√ß√£o: 0.9793\n",
      "\n",
      "üìä AVALIA√á√ÉO FINAL NO TESTE:\n",
      "============================================================\n",
      "üìà RESULTADOS FINAIS:\n",
      "Acur√°cia no teste: 0.9249\n",
      "F1-Score (weighted): 0.9247\n",
      "F1-Score (macro): 0.9247\n",
      "‚è±Ô∏è  Tempo total: 5.31 segundos (0.09 minutos)\n",
      "============================================================\n",
      "\n",
      "üìã RELAT√ìRIO DETALHADO:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96        40\n",
      "           1       1.00      1.00      1.00        40\n",
      "           2       0.98      1.00      0.99        40\n",
      "           3       0.95      1.00      0.97        39\n",
      "           4       1.00      0.88      0.93        40\n",
      "           5       0.74      0.70      0.72        40\n",
      "           6       0.97      0.88      0.92        40\n",
      "           7       0.82      0.93      0.87        40\n",
      "           8       1.00      0.93      0.96        40\n",
      "           9       1.00      1.00      1.00        40\n",
      "          10       0.97      0.93      0.95        40\n",
      "          11       0.88      0.88      0.88        40\n",
      "          12       0.90      0.93      0.91        40\n",
      "          13       1.00      0.97      0.99        40\n",
      "          14       0.90      0.95      0.93        40\n",
      "          15       0.88      0.95      0.92        40\n",
      "          16       0.92      0.90      0.91        40\n",
      "          17       0.95      0.95      0.95        40\n",
      "          18       1.00      0.80      0.89        40\n",
      "          19       0.97      0.95      0.96        40\n",
      "          20       0.91      0.97      0.94        40\n",
      "          21       0.95      1.00      0.98        40\n",
      "          22       0.90      0.88      0.89        40\n",
      "          23       1.00      0.90      0.95        40\n",
      "          24       0.95      0.97      0.96        40\n",
      "          25       0.89      0.85      0.87        40\n",
      "          26       0.77      1.00      0.87        40\n",
      "          27       1.00      0.72      0.84        40\n",
      "          28       0.92      0.88      0.90        40\n",
      "          29       0.95      0.88      0.91        40\n",
      "          30       0.98      1.00      0.99        40\n",
      "          31       0.97      0.93      0.95        40\n",
      "          32       0.91      0.97      0.94        40\n",
      "          33       0.86      0.95      0.90        39\n",
      "          34       0.95      0.97      0.96        40\n",
      "          35       0.92      0.88      0.90        40\n",
      "          36       0.92      0.85      0.88        40\n",
      "          37       0.80      0.88      0.83        40\n",
      "          38       0.98      1.00      0.99        40\n",
      "          39       0.95      0.97      0.96        40\n",
      "          40       0.89      0.97      0.93        40\n",
      "          41       0.83      0.95      0.88        40\n",
      "          42       1.00      0.97      0.99        40\n",
      "          43       0.88      0.90      0.89        40\n",
      "          44       1.00      1.00      1.00        40\n",
      "          45       0.93      0.95      0.94        40\n",
      "          46       0.95      0.97      0.96        40\n",
      "          47       0.95      0.93      0.94        40\n",
      "          48       0.91      0.82      0.86        39\n",
      "          49       0.97      0.88      0.92        40\n",
      "          50       0.93      0.97      0.95        40\n",
      "          51       1.00      0.97      0.99        40\n",
      "          52       1.00      0.97      0.99        40\n",
      "          53       0.88      0.95      0.92        40\n",
      "          54       0.90      0.95      0.93        40\n",
      "          55       0.95      1.00      0.98        40\n",
      "          56       0.95      0.88      0.91        40\n",
      "          57       0.95      0.95      0.95        40\n",
      "          58       0.86      0.93      0.89        40\n",
      "          59       0.91      0.97      0.94        40\n",
      "          60       0.91      1.00      0.95        40\n",
      "          61       0.85      0.85      0.85        40\n",
      "          62       0.79      0.75      0.77        40\n",
      "          63       0.95      1.00      0.98        40\n",
      "          64       0.93      0.95      0.94        40\n",
      "          65       0.91      0.80      0.85        40\n",
      "          66       0.78      0.92      0.85        39\n",
      "          67       0.84      0.90      0.87        40\n",
      "          68       0.91      0.97      0.94        40\n",
      "          69       0.79      0.93      0.85        40\n",
      "          70       1.00      1.00      1.00        40\n",
      "          71       1.00      1.00      1.00        40\n",
      "          72       1.00      0.82      0.90        40\n",
      "          73       0.97      0.93      0.95        40\n",
      "          74       0.97      0.75      0.85        40\n",
      "          75       1.00      0.90      0.95        40\n",
      "          76       0.97      0.93      0.95        40\n",
      "\n",
      "    accuracy                           0.92      3076\n",
      "   macro avg       0.93      0.92      0.92      3076\n",
      "weighted avg       0.93      0.92      0.92      3076\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9249024707412223,\n",
       " 'f1_weighted': 0.9247079466781846,\n",
       " 'f1_macro': 0.924672404771065,\n",
       " 'best_params': {'lr': 0.0005, 'hidden_dim1': 128, 'dropout': 0.5},\n",
       " 'total_time_seconds': 5.309385299682617,\n",
       " 'total_time_minutes': 0.08848975499471029}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainNNEmbOutra(\n",
    "    X_train_emotion_bert, X_train_emotion_roberta, X_train_emotion_electra,\n",
    "    X_test_emotion_bert, X_test_emotion_roberta, X_test_emotion_electra,\n",
    "    y_train_emotion_roberta, y_test_emotion_roberta,\n",
    "    num_classes=77,  # emotion tem 6 classes\n",
    "    val_size=0.2,\n",
    "    batch_size=32,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aa610c-6613-4503-9efa-6d3340347ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
