{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a33e44c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_tensor_value(tensor_str):\n",
    "    \"\"\"\n",
    "    Extrai o valor numérico de uma string tensor.\n",
    "    Ex: 'tensor(0)' -> 0, 'tensor(1)' -> 1\n",
    "    \"\"\"\n",
    "    if isinstance(tensor_str, str) and 'tensor(' in tensor_str:\n",
    "        return int(tensor_str.split('(')[1].split(')')[0])\n",
    "    return tensor_str\n",
    "\n",
    "def load_classifier_results(file_paths, classifier_names):\n",
    "    \"\"\"\n",
    "    Carrega os resultados dos classificadores e organiza em um DataFrame.\n",
    "\n",
    "    Args:\n",
    "        file_paths: lista com caminhos dos arquivos CSV\n",
    "        classifier_names: lista com nomes dos classificadores\n",
    "\n",
    "    Returns:\n",
    "        DataFrame com predições de todos os classificadores\n",
    "    \"\"\"\n",
    "    all_predictions = {}\n",
    "\n",
    "    for i, (file_path, name) in enumerate(zip(file_paths, classifier_names)):\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Converte tensor strings para valores numéricos\n",
    "        df['prediction_clean'] = df['prediction'].apply(extract_tensor_value)\n",
    "        df['label_clean'] = df['label'].apply(extract_tensor_value)\n",
    "\n",
    "        if i == 0:\n",
    "            # Primeira iteração: salva texto e label verdadeiro\n",
    "            all_predictions['text'] = df['text']\n",
    "            all_predictions['true_label'] = df['label_clean']\n",
    "\n",
    "        # Adiciona predições do classificador atual\n",
    "        all_predictions[f'{name}_pred'] = df['prediction_clean']\n",
    "\n",
    "    return pd.DataFrame(all_predictions)\n",
    "\n",
    "def calculate_pairwise_agreement(df, classifier_names):\n",
    "    \"\"\"\n",
    "    Calcula concordância par a par entre todos os classificadores.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame com predições\n",
    "        classifier_names: lista com nomes dos classificadores\n",
    "\n",
    "    Returns:\n",
    "        DataFrame com matriz de concordância (Kappa de Cohen)\n",
    "    \"\"\"\n",
    "    n_classifiers = len(classifier_names)\n",
    "    kappa_matrix = np.zeros((n_classifiers, n_classifiers))\n",
    "\n",
    "    # Calcula Kappa para cada par\n",
    "    for i, j in itertools.combinations(range(n_classifiers), 2):\n",
    "        name_i = f'{classifier_names[i]}_pred'\n",
    "        name_j = f'{classifier_names[j]}_pred'\n",
    "\n",
    "        kappa = cohen_kappa_score(df[name_i], df[name_j])\n",
    "        kappa_matrix[i, j] = kappa\n",
    "        kappa_matrix[j, i] = kappa  # Matriz simétrica\n",
    "\n",
    "    # Diagonal = 1 (concordância perfeita consigo mesmo)\n",
    "    np.fill_diagonal(kappa_matrix, 1.0)\n",
    "\n",
    "    return pd.DataFrame(kappa_matrix,\n",
    "                       index=classifier_names,\n",
    "                       columns=classifier_names)\n",
    "\n",
    "def calculate_accuracy_vs_true_labels(df, classifier_names):\n",
    "    \"\"\"\n",
    "    Calcula acurácia de cada classificador vs labels verdadeiros.\n",
    "    \"\"\"\n",
    "    accuracies = {}\n",
    "\n",
    "    for name in classifier_names:\n",
    "        pred_col = f'{name}_pred'\n",
    "        accuracy = (df[pred_col] == df['true_label']).mean()\n",
    "        accuracies[name] = accuracy\n",
    "\n",
    "    return accuracies\n",
    "\n",
    "def plot_agreement_heatmap(kappa_matrix, title=\"Concordância entre Classificadores (Kappa de Cohen)\"):\n",
    "    \"\"\"\n",
    "    Plota heatmap da matriz de concordância.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(kappa_matrix,\n",
    "                annot=True,\n",
    "                cmap='RdYlBu_r',\n",
    "                center=0,\n",
    "                square=True,\n",
    "                fmt='.3f',\n",
    "                cbar_kws={\"shrink\": .8})\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def interpret_kappa(kappa):\n",
    "    \"\"\"\n",
    "    Interpreta valores de Kappa de Cohen.\n",
    "    \"\"\"\n",
    "    if kappa < 0:\n",
    "        return \"Concordância pior que o acaso\"\n",
    "    elif kappa < 0.20:\n",
    "        return \"Concordância fraca\"\n",
    "    elif kappa < 0.40:\n",
    "        return \"Concordância razoável\"\n",
    "    elif kappa < 0.60:\n",
    "        return \"Concordância moderada\"\n",
    "    elif kappa < 0.80:\n",
    "        return \"Concordância substancial\"\n",
    "    else:\n",
    "        return \"Concordância quase perfeita\"\n",
    "\n",
    "# Exemplo de uso\n",
    "def main():\n",
    "    # CONFIGURAÇÃO - Ajuste estes caminhos e nomes conforme seus arquivos\n",
    "    file_paths = [\n",
    "        'bert.csv',\n",
    "        'roberta.csv',\n",
    "        'electra_results.csv'\n",
    "    ]\n",
    "\n",
    "    classifier_names = ['BERT', 'RoBERTa', 'ELECTRA']\n",
    "\n",
    "    print(\"=== ANÁLISE DE CONCORDÂNCIA ENTRE CLASSIFICADORES ===\\n\")\n",
    "\n",
    "    # Carrega dados\n",
    "    print(\"Carregando dados dos classificadores...\")\n",
    "    df = load_classifier_results(file_paths, classifier_names)\n",
    "    print(f\"Total de amostras: {len(df)}\")\n",
    "\n",
    "    # Calcula concordância par a par\n",
    "    print(\"\\nCalculando concordância par a par (Kappa de Cohen)...\")\n",
    "    kappa_matrix = calculate_pairwise_agreement(df, classifier_names)\n",
    "\n",
    "    print(\"\\nMatriz de Concordância (Kappa de Cohen):\")\n",
    "    print(kappa_matrix.round(3))\n",
    "\n",
    "    # Calcula acurácia vs labels verdadeiros\n",
    "    print(\"\\nAcurácia vs Labels Verdadeiros:\")\n",
    "    accuracies = calculate_accuracy_vs_true_labels(df, classifier_names)\n",
    "    for name, acc in accuracies.items():\n",
    "        print(f\"{name}: {acc:.3f}\")\n",
    "\n",
    "    # Análise detalhada par a par\n",
    "    print(\"\\n=== ANÁLISE DETALHADA PAR A PAR ===\")\n",
    "    for i, j in itertools.combinations(range(len(classifier_names)), 2):\n",
    "        name_i = classifier_names[i]\n",
    "        name_j = classifier_names[j]\n",
    "        kappa = kappa_matrix.loc[name_i, name_j]\n",
    "\n",
    "        print(f\"\\n{name_i} vs {name_j}:\")\n",
    "        print(f\"  Kappa de Cohen: {kappa:.3f}\")\n",
    "        print(f\"  Interpretação: {interpret_kappa(kappa)}\")\n",
    "\n",
    "        # Concordância simples (porcentagem de acordo)\n",
    "        pred_i = df[f'{name_i}_pred']\n",
    "        pred_j = df[f'{name_j}_pred']\n",
    "        simple_agreement = (pred_i == pred_j).mean()\n",
    "        print(f\"  Concordância simples: {simple_agreement:.3f} ({simple_agreement*100:.1f}%)\")\n",
    "\n",
    "    # Plota heatmap\n",
    "    plot_agreement_heatmap(kappa_matrix)\n",
    "\n",
    "    # Estatísticas gerais\n",
    "    print(\"\\n=== ESTATÍSTICAS GERAIS ===\")\n",
    "    # Remove diagonal (concordância consigo mesmo)\n",
    "    off_diagonal = kappa_matrix.values[~np.eye(len(classifier_names), dtype=bool)]\n",
    "    print(f\"Kappa médio entre classificadores: {off_diagonal.mean():.3f}\")\n",
    "    print(f\"Kappa mínimo: {off_diagonal.min():.3f}\")\n",
    "    print(f\"Kappa máximo: {off_diagonal.max():.3f}\")\n",
    "    print(f\"Desvio padrão: {off_diagonal.std():.3f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
