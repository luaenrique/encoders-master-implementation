import torch
from transformers import AutoTokenizer, AutoModel
from datasets import load_dataset
import numpy as np
from tqdm import tqdm
import pickle
import os
import time
import sys

def main():
    print("ğŸš€ Iniciando extraÃ§Ã£o de embeddings do Yelp...")
    
    # ConfiguraÃ§Ãµes
    MODEL_NAMES = [
        'bert-base-uncased',
        'google/electra-base-discriminator', 
        'roberta-base'
    ]
    BATCH_SIZE = 16
    SAVE_DIR = "./embeddings_yelp"  # DiretÃ³rio local na VM
    
    # Criar diretÃ³rio se nÃ£o existir
    os.makedirs(SAVE_DIR, exist_ok=True)
    print(f"ğŸ“ Salvando embeddings em: {SAVE_DIR}")
    
    # Verificar GPU disponÃ­vel
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        print(f"ğŸ–¥ï¸  GPU disponÃ­vel: {gpu_name}")
        print(f"ğŸ’¾ VRAM disponÃ­vel: {gpu_memory:.1f} GB")
    else:
        print("âš ï¸  Usando CPU (vai ser mais lento)")
    
    # Carregar dataset Yelp
    print("\nğŸ“¦ Carregando dataset Yelp...")
    try:
        yelp_dataset = load_dataset("Yelp/yelp_review_full")
        print("âœ… Dataset carregado com sucesso!")
    except Exception as e:
        print(f"âŒ Erro ao carregar dataset: {e}")
        print("ğŸ’¡ Tente: pip install datasets")
        sys.exit(1)
    
    # Dados de treino
    train_texts = yelp_dataset['train']['text']
    train_labels = yelp_dataset['train']['label']
    print(f"ğŸ“Š Treino - Textos: {len(train_texts):,}")
    
    # Dados de teste
    test_texts = yelp_dataset['test']['text']
    test_labels = yelp_dataset['test']['label']
    print(f"ğŸ“Š Teste - Textos: {len(test_texts):,}")
    
    # InformaÃ§Ãµes do dataset
    print(f"\nğŸ“ˆ InformaÃ§Ãµes do Yelp:")
    print(f"Classes: {sorted(set(train_labels))} (1-5 estrelas)")
    train_dist = np.bincount(train_labels)
    test_dist = np.bincount(test_labels)
    print(f"DistribuiÃ§Ã£o treino: {train_dist[1:]}")  # Ignorar Ã­ndice 0
    print(f"DistribuiÃ§Ã£o teste: {test_dist[1:]}")
    
    # FunÃ§Ã£o para extrair embeddings de um batch
    def get_batch_embeddings(batch_texts, model, tokenizer):
        inputs = tokenizer(
            batch_texts, 
            return_tensors='pt', 
            truncation=True, 
            padding=True, 
            max_length=512
        )
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model(**inputs)
            last_hidden = outputs.last_hidden_state  # [batch_size, seq_len, hidden]
            embeddings = last_hidden.mean(dim=1)  # mÃ©dia dos tokens
        
        return embeddings.cpu().numpy()
    
    # FunÃ§Ã£o para processar um split (treino ou teste)
    def process_split(texts, labels, split_name, model, tokenizer, model_name):
        print(f"ğŸ”„ Extraindo embeddings {split_name} para {model_name}...")
        all_embeddings = []
        
        progress_bar = tqdm(
            range(0, len(texts), BATCH_SIZE), 
            desc=f"{model_name} [{split_name.upper()}]",
            unit="batch"
        )
        
        for batch_start in progress_bar:
            batch_texts = texts[batch_start:batch_start+BATCH_SIZE]
            batch_embeddings = get_batch_embeddings(batch_texts, model, tokenizer)
            all_embeddings.append(batch_embeddings)
            
            # Mostrar progresso de memÃ³ria GPU se disponÃ­vel
            if torch.cuda.is_available() and (batch_start // BATCH_SIZE) % 100 == 0:
                memory_used = torch.cuda.memory_allocated() / 1e9
                progress_bar.set_postfix({'GPU_mem': f'{memory_used:.1f}GB'})
        
        return np.vstack(all_embeddings)
    
    # Extrair embeddings para cada modelo
    all_embeddings = {'train': {}, 'test': {}}
    processing_times = {}
    
    for model_idx, model_name in enumerate(MODEL_NAMES):
        print(f"\n{'='*70}")
        print(f"ğŸ¤– Processando modelo {model_idx+1}/{len(MODEL_NAMES)}: {model_name}")
        print(f"{'='*70}")
        
        start_time = time.time()
        
        # Limpar cache GPU
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        try:
            # Carregar modelo e tokenizer
            print("ğŸ“¥ Carregando modelo e tokenizer...")
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModel.from_pretrained(model_name)
            model.eval()
            model.to(device)
            
            # Verificar uso de memÃ³ria
            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / 1e9
                print(f"ğŸ’¾ MemÃ³ria GPU apÃ³s carregar modelo: {memory_used:.2f} GB")
            
            model_key = model_name.replace('/', '_').replace('-', '_')
            
            # Processar treino
            print(f"\nğŸ‹ï¸  Processando dados de TREINO ({len(train_texts):,} exemplos)...")
            X_train = process_split(train_texts, train_labels, 'treino', model, tokenizer, model_name)
            all_embeddings['train'][model_key] = X_train
            
            # Salvar treino
            train_save_path = os.path.join(SAVE_DIR, f"{model_key}_embeddings_train.npy")
            np.save(train_save_path, X_train)
            print(f"ğŸ’¾ Treino salvo: {train_save_path}")
            print(f"ğŸ“ Shape treino: {X_train.shape}")
            
            # Processar teste
            print(f"\nğŸ§ª Processando dados de TESTE ({len(test_texts):,} exemplos)...")
            X_test = process_split(test_texts, test_labels, 'teste', model, tokenizer, model_name)
            all_embeddings['test'][model_key] = X_test
            
            # Salvar teste
            test_save_path = os.path.join(SAVE_DIR, f"{model_key}_embeddings_test.npy")
            np.save(test_save_path, X_test)
            print(f"ğŸ’¾ Teste salvo: {test_save_path}")
            print(f"ğŸ“ Shape teste: {X_test.shape}")
            
            processing_time = time.time() - start_time
            processing_times[model_name] = processing_time
            
            print(f"\nâœ… {model_name} concluÃ­do!")
            print(f"   â±ï¸  Tempo: {processing_time/60:.1f} minutos")
            print(f"   ğŸ“Š Treino: {X_train.shape}, Teste: {X_test.shape}")
            
            # Limpar memÃ³ria
            del model, tokenizer, X_train, X_test
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
        except Exception as e:
            print(f"âŒ Erro ao processar {model_name}: {str(e)}")
            import traceback
            traceback.print_exc()
            continue
    
    # Salvar dados completos
    print(f"\n{'='*60}")
    print("ğŸ’¾ Salvando dados completos...")
    
    if all_embeddings['train']:  # Verificar se hÃ¡ dados para salvar
        # Treino
        train_data = {
            'embeddings': all_embeddings['train'],
            'labels': np.array(train_labels),
            'model_names': list(all_embeddings['train'].keys()),
            'dataset_info': {
                'dataset': 'yelp_review_full',
                'split': 'train',
                'num_samples': len(train_labels),
                'embedding_method': 'mean_pooling',
                'max_length': 512
            }
        }
        
        train_pkl_path = os.path.join(SAVE_DIR, 'all_embeddings_yelp_train.pkl')
        with open(train_pkl_path, 'wb') as f:
            pickle.dump(train_data, f)
        print(f"âœ… Dados de treino salvos: {train_pkl_path}")
        
        # Teste
        test_data = {
            'embeddings': all_embeddings['test'],
            'labels': np.array(test_labels),
            'model_names': list(all_embeddings['test'].keys()),
            'dataset_info': {
                'dataset': 'yelp_review_full',
                'split': 'test',
                'num_samples': len(test_labels),
                'embedding_method': 'mean_pooling',
                'max_length': 512
            }
        }
        
        test_pkl_path = os.path.join(SAVE_DIR, 'all_embeddings_yelp_test.pkl')
        with open(test_pkl_path, 'wb') as f:
            pickle.dump(test_data, f)
        print(f"âœ… Dados de teste salvos: {test_pkl_path}")
        
        # RelatÃ³rio final
        print(f"\nğŸ‰ PROCESSAMENTO CONCLUÃDO!")
        print(f"ğŸ“ DiretÃ³rio: {SAVE_DIR}")
        print(f"ğŸ¤– Modelos processados: {len(all_embeddings['train'])}")
        print(f"ğŸ“Š Treino: {len(train_labels):,} amostras")
        print(f"ğŸ“Š Teste: {len(test_labels):,} amostras")
        
        # Listar arquivos criados
        print(f"\nğŸ“„ Arquivos criados:")
        files = sorted(os.listdir(SAVE_DIR))
        for file in files:
            file_path = os.path.join(SAVE_DIR, file)
            size_mb = os.path.getsize(file_path) / (1024*1024)
            print(f"   {file} ({size_mb:.1f} MB)")
        
        # Tempos de processamento
        if processing_times:
            print(f"\nâ±ï¸  Tempos de processamento:")
            total_time = sum(processing_times.values())
            for model_name, time_sec in processing_times.items():
                print(f"   {model_name}: {time_sec/60:.1f} min")
            print(f"   â±ï¸  Tempo total: {total_time/60:.1f} min ({total_time/3600:.1f}h)")
        
        # InstruÃ§Ãµes de uso
        print(f"\nğŸ“– Para carregar os embeddings:")
        print("```python")
        print("import pickle")
        print("import numpy as np")
        print()
        print("# Carregar dados")
        print(f"with open('{train_pkl_path}', 'rb') as f:")
        print("    train_data = pickle.load(f)")
        print(f"with open('{test_pkl_path}', 'rb') as f:")
        print("    test_data = pickle.load(f)")
        print()
        print("# Extrair embeddings")
        print("X_train_bert = train_data['embeddings']['bert_base_uncased']")
        print("X_test_bert = test_data['embeddings']['bert_base_uncased']")
        print("y_train = train_data['labels']")
        print("y_test = test_data['labels']")
        print("```")
        
    else:
        print("âŒ Nenhum modelo foi processado com sucesso!")
        sys.exit(1)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nâš ï¸  Processamento interrompido pelo usuÃ¡rio")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ Erro inesperado: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)